{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed2d692",
   "metadata": {},
   "source": [
    "# üîÅ Enhanced Unified Joint Model (BART) ‚Äî High-Quality Contextual Reasoning\n",
    "\n",
    "This notebook trains a BART model with **contextual, feature-based reasoning** instead of template responses.\n",
    "\n",
    "**Key Improvements:**\n",
    "- Analyzes message features (keywords, urgency, entities, patterns)\n",
    "- Generates detailed, context-aware explanations\n",
    "- Higher MAX_TARGET_LENGTH for richer reasoning\n",
    "- Improved instruction prompting\n",
    "- Better generation parameters (beam search, length penalty)\n",
    "\n",
    "Model: `facebook/bart-base` with joint classification + contextual generation.\n",
    "Optimized for Kaggle; will auto-detect `/kaggle/input` and save artifacts to `/kaggle/working`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d727893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Detect Kaggle Environment and Set Paths\n",
    "import os, json, sys\n",
    "from pathlib import Path\n",
    "\n",
    "IS_KAGGLE = Path('/kaggle').exists()\n",
    "INPUT_DIR = Path('/kaggle/input') if IS_KAGGLE else Path('..')\n",
    "WORK_DIR = Path('/kaggle/working') if IS_KAGGLE else Path('.')\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Running on Kaggle: {IS_KAGGLE}')\n",
    "print(f'INPUT_DIR: {INPUT_DIR}')\n",
    "print(f'WORK_DIR: {WORK_DIR}')\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print('\\nAvailable datasets under /kaggle/input:')\n",
    "    for p in sorted(INPUT_DIR.glob('*')):\n",
    "        print(' -', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28e9c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5) Ensure compatible libraries\n",
    "import subprocess\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n",
    "\n",
    "need_install = False\n",
    "try:\n",
    "    import transformers as _tf_ver\n",
    "    import huggingface_hub as _hf_ver\n",
    "    from packaging import version as _v\n",
    "    if _v.parse(_tf_ver.__version__) < _v.parse(\"4.43.0\") or _v.parse(_hf_ver.__version__) < _v.parse(\"0.23.4\"):\n",
    "        need_install = True\n",
    "except Exception:\n",
    "    need_install = True\n",
    "\n",
    "if need_install:\n",
    "    print(\"Installing compatible versions of transformers/huggingface_hub...\")\n",
    "    pip_install([\n",
    "        \"transformers==4.44.2\",\n",
    "        \"huggingface_hub==0.24.6\",\n",
    "        \"tokenizers==0.19.1\",\n",
    "        \"safetensors\",\n",
    "        \"sentencepiece\"\n",
    "    ])\n",
    "    import importlib\n",
    "    importlib.invalidate_caches()\n",
    "    print(\"Installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Imports and Device\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import transformers as tf\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    BartForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "print('PyTorch:', torch.__version__)\n",
    "print('Transformers:', tf.__version__)\n",
    "device = 'cuda' if torch.cuda.is_available() else ('mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cd3947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Enhanced Config ‚Äî Better reasoning generation\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Dataset path\n",
    "CSV_PATH = INPUT_DIR / 'fraud-data' / 'final_fraud_detection_dataset.csv'\n",
    "if not CSV_PATH.exists():\n",
    "    matches = list(INPUT_DIR.glob('**/final_fraud_detection_dataset.csv'))\n",
    "    if matches:\n",
    "        CSV_PATH = matches[0]\n",
    "print('CSV_PATH:', CSV_PATH)\n",
    "\n",
    "TEXT_COL = 'text'\n",
    "LABEL_COL = 'detailed_category'\n",
    "\n",
    "DEFAULT_LABELS = [\n",
    "    'job_scam',\n",
    "    'legitimate',\n",
    "    'phishing',\n",
    "    'popup_scam',\n",
    "    'refund_scam',\n",
    "    'reward_scam',\n",
    "    'sms_spam',\n",
    "    'ssn_scam',\n",
    "    'tech_support_scam'\n",
    "]\n",
    "LABEL2ID = {l: i for i, l in enumerate(DEFAULT_LABELS)}\n",
    "ID2LABEL = {i: l for l, i in LABEL2ID.items()}\n",
    "\n",
    "BASE_MODEL = 'facebook/bart-base'\n",
    "OUTPUT_DIR = WORK_DIR / 'unified-bart-joint-enhanced'\n",
    "\n",
    "# Enhanced parameters for better reasoning\n",
    "MAX_SOURCE_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 128  # Increased for richer reasoning\n",
    "TRAIN_SIZE = 0.9\n",
    "NUM_EPOCHS = 4  # More epochs for better learning\n",
    "LR = 2e-5  # Slightly lower for stability\n",
    "BATCH_TRAIN = 8\n",
    "BATCH_EVAL = 8\n",
    "GRAD_ACCUM = 2  # Effective batch size = 16\n",
    "\n",
    "# Joint-loss weights - prioritize generation quality\n",
    "CLS_LOSS_WEIGHT = 0.8    # Classification still important\n",
    "GEN_LOSS_WEIGHT = 1.2    # Emphasize quality reasoning\n",
    "\n",
    "# Early stop threshold\n",
    "EARLY_STOP_LOSS = 0.02\n",
    "MIN_EPOCHS = 2\n",
    "\n",
    "# Enhanced instruction prompt\n",
    "INSTRUCTION_PREFIX = (\n",
    "    'Analyze this message and classify it into one of these categories: '\n",
    "    'job_scam, legitimate, phishing, popup_scam, refund_scam, reward_scam, sms_spam, ssn_scam, tech_support_scam. '\n",
    "    'Then explain your reasoning by identifying specific suspicious elements, patterns, or indicators in the message.\\n\\n'\n",
    "    'Message: '\n",
    ")\n",
    "\n",
    "print(f'\\nModel: {BASE_MODEL}')\n",
    "print(f'Output: {OUTPUT_DIR}')\n",
    "print(f'Max target length: {MAX_TARGET_LENGTH}')\n",
    "print(f'Loss weights - CLS: {CLS_LOSS_WEIGHT}, GEN: {GEN_LOSS_WEIGHT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Load and Split Data\n",
    "assert CSV_PATH.exists(), f'CSV not found at {CSV_PATH}. Attach your dataset to the notebook.'\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print('Loaded:', df.shape)\n",
    "\n",
    "# Keep only known labels\n",
    "df = df[df[LABEL_COL].isin(DEFAULT_LABELS)].copy()\n",
    "df = df[[TEXT_COL, LABEL_COL]]\n",
    "print('After filtering:', df.shape)\n",
    "print('Label distribution:')\n",
    "print(df[LABEL_COL].value_counts())\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=SEED, stratify=df[LABEL_COL])\n",
    "print('\\nTrain:', train_df.shape, 'Val:', val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Import Reasoning Generator\n",
    "import re\n",
    "from typing import Dict, List\n",
    "\n",
    "# Inline reasoning generator (same as reasoning_generator.py)\n",
    "class ReasoningGenerator:\n",
    "    \"\"\"Generates contextual reasoning for fraud detection.\"\"\"\n",
    "    \n",
    "    FRAUD_INDICATORS = {\n",
    "        'phishing': {\n",
    "            'keywords': ['verify', 'account', 'suspended', 'confirm', 'click here', 'update', \n",
    "                        'security', 'alert', 'unauthorized', 'immediately', 'login'],\n",
    "            'patterns': [r'click\\s+(here|link)', r'verify\\s+your\\s+(account|identity|information)',\n",
    "                        r'account\\s+(suspended|locked|compromised)', r'urgent\\s+action'],\n",
    "            'entities': ['bank', 'paypal', 'amazon', 'netflix', 'apple', 'microsoft'],\n",
    "            'urgency': ['immediately', 'urgent', 'now', 'expire', 'within 24', 'act now']\n",
    "        },\n",
    "        'job_scam': {\n",
    "            'keywords': ['work from home', 'easy money', 'no experience', 'apply now', \n",
    "                        'guaranteed income', 'flexible hours', 'earn', 'opportunity'],\n",
    "            'patterns': [r'\\$\\d+\\s*(?:per|\\/)\\s*(?:day|week|month|hour)', \n",
    "                        r'work\\s+from\\s+home', r'no\\s+experience\\s+(?:required|needed)',\n",
    "                        r'guaranteed\\s+(?:income|salary|pay)'],\n",
    "            'entities': ['remote', 'telecommute', 'online'],\n",
    "            'urgency': ['apply now', 'limited positions', 'act fast', 'immediate hire']\n",
    "        },\n",
    "        'reward_scam': {\n",
    "            'keywords': ['congratulations', 'winner', 'prize', 'reward', 'gift card', \n",
    "                        'won', 'claim', 'free', 'selected'],\n",
    "            'patterns': [r'(?:won|winner|prize|reward|gift\\s+card).*\\$\\d+',\n",
    "                        r'congratulations.*(?:won|winner|selected)',\n",
    "                        r'claim\\s+your\\s+(?:prize|reward|gift)'],\n",
    "            'entities': ['walmart', 'target', 'amazon', 'visa', 'mastercard'],\n",
    "            'urgency': ['claim now', 'expires', 'limited time', 'act now']\n",
    "        },\n",
    "        'refund_scam': {\n",
    "            'keywords': ['refund', 'overpaid', 'owed', 'payment', 'credit', 'return'],\n",
    "            'patterns': [r'(?:refund|owed).*\\$\\d+', r'overpaid'],\n",
    "            'entities': ['irs', 'tax', 'government'],\n",
    "            'urgency': ['claim now', 'expires']\n",
    "        },\n",
    "        'tech_support_scam': {\n",
    "            'keywords': ['virus', 'infected', 'malware', 'computer', 'tech support', 'call'],\n",
    "            'patterns': [r'(?:virus|malware)\\s+detected', r'computer\\s+(?:infected|compromised)'],\n",
    "            'entities': ['microsoft', 'windows', 'apple'],\n",
    "            'urgency': ['call now', 'immediately']\n",
    "        },\n",
    "        'popup_scam': {\n",
    "            'keywords': ['click', 'download', 'install', 'update required'],\n",
    "            'patterns': [r'click\\s+(?:here|now)', r'download\\s+now'],\n",
    "            'entities': [],\n",
    "            'urgency': ['now', 'immediately']\n",
    "        },\n",
    "        'ssn_scam': {\n",
    "            'keywords': ['social security', 'ssn', 'suspended', 'verify'],\n",
    "            'patterns': [r'social\\s+security\\s+(?:number|suspended)'],\n",
    "            'entities': ['social security', 'ssa'],\n",
    "            'urgency': ['immediately', 'urgent']\n",
    "        },\n",
    "        'sms_spam': {\n",
    "            'keywords': ['text', 'reply', 'stop', 'offer'],\n",
    "            'patterns': [r'text\\s+\\w+\\s+to\\s+\\d+'],\n",
    "            'entities': [],\n",
    "            'urgency': ['now', 'limited time']\n",
    "        },\n",
    "        'legitimate': {\n",
    "            'keywords': ['confirmation', 'receipt', 'order', 'tracking'],\n",
    "            'patterns': [r'tracking\\s+(?:number|#):\\s*\\w+'],\n",
    "            'entities': [],\n",
    "            'urgency': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.compiled_patterns = {}\n",
    "        for category, indicators in self.FRAUD_INDICATORS.items():\n",
    "            self.compiled_patterns[category] = [\n",
    "                re.compile(pattern, re.IGNORECASE) \n",
    "                for pattern in indicators['patterns']\n",
    "            ]\n",
    "    \n",
    "    def analyze_message(self, text: str, category: str) -> Dict:\n",
    "        \"\"\"Analyze message and extract fraud indicators.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        if category not in self.FRAUD_INDICATORS:\n",
    "            category = 'legitimate'\n",
    "        \n",
    "        indicators = self.FRAUD_INDICATORS[category]\n",
    "        found_keywords = [kw for kw in indicators['keywords'] if kw in text_lower]\n",
    "        found_patterns = []\n",
    "        for pattern in self.compiled_patterns[category]:\n",
    "            matches = pattern.findall(text)\n",
    "            found_patterns.extend(matches)\n",
    "        found_urgency = [u for u in indicators['urgency'] if u in text_lower]\n",
    "        found_entities = [e for e in indicators['entities'] if e in text_lower]\n",
    "        \n",
    "        money_pattern = re.compile(r'\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d{2})?')\n",
    "        money_amounts = money_pattern.findall(text)\n",
    "        phone_pattern = re.compile(r'1?\\s*[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}')\n",
    "        phone_numbers = phone_pattern.findall(text)\n",
    "        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+|click\\s+here', re.IGNORECASE)\n",
    "        urls = url_pattern.findall(text)\n",
    "        \n",
    "        return {\n",
    "            'keywords': found_keywords[:5],\n",
    "            'patterns': found_patterns[:3],\n",
    "            'urgency': found_urgency[:3],\n",
    "            'entities': found_entities[:3],\n",
    "            'money': money_amounts[:2],\n",
    "            'phones': phone_numbers[:2],\n",
    "            'urls': bool(urls)\n",
    "        }\n",
    "    \n",
    "    def generate_reason(self, text: str, category: str) -> str:\n",
    "        \"\"\"Generate contextual reasoning.\"\"\"\n",
    "        analysis = self.analyze_message(text, category)\n",
    "        reasons = []\n",
    "        \n",
    "        if category == 'phishing':\n",
    "            reasons.append(\"This appears to be a phishing attempt because\")\n",
    "            if analysis['entities']:\n",
    "                reasons.append(f\"it impersonates {', '.join(analysis['entities'])}\")\n",
    "            if analysis['urgency']:\n",
    "                reasons.append(f\"uses urgent language like '{analysis['urgency'][0]}'\")\n",
    "            if analysis['keywords']:\n",
    "                suspicious = [kw for kw in analysis['keywords'] if kw in ['verify', 'suspended', 'unauthorized']]\n",
    "                if suspicious:\n",
    "                    reasons.append(f\"requests to {', '.join(suspicious[:2])}\")\n",
    "            if analysis['urls']:\n",
    "                reasons.append(\"includes suspicious links\")\n",
    "            reasons.append(\"to steal personal information.\")\n",
    "        \n",
    "        elif category == 'job_scam':\n",
    "            reasons.append(\"This is likely a job scam as\")\n",
    "            if analysis['money']:\n",
    "                reasons.append(f\"it promises unrealistic income ({analysis['money'][0]})\")\n",
    "            if 'no experience' in ' '.join(analysis['keywords']):\n",
    "                reasons.append(\"requires no qualifications\")\n",
    "            if analysis['urgency']:\n",
    "                reasons.append(f\"creates urgency ('{analysis['urgency'][0]}')\")\n",
    "            reasons.append(\"which are common job scam tactics.\")\n",
    "        \n",
    "        elif category == 'reward_scam':\n",
    "            reasons.append(\"This is a reward scam because\")\n",
    "            if analysis['money']:\n",
    "                reasons.append(f\"it claims you've won {analysis['money'][0]}\")\n",
    "            if any(kw in analysis['keywords'] for kw in ['winner', 'selected']):\n",
    "                reasons.append(\"falsely claims you've been selected\")\n",
    "            if analysis['urgency']:\n",
    "                reasons.append(f\"pressures quick action\")\n",
    "            reasons.append(\"to collect personal information.\")\n",
    "        \n",
    "        elif category == 'refund_scam':\n",
    "            reasons.append(\"This appears to be a refund scam as\")\n",
    "            if analysis['money']:\n",
    "                reasons.append(f\"it claims you're owed {analysis['money'][0]}\")\n",
    "            if analysis['entities']:\n",
    "                reasons.append(f\"impersonates {', '.join(analysis['entities'])}\")\n",
    "            reasons.append(\"to trick you into sharing sensitive data.\")\n",
    "        \n",
    "        elif category == 'tech_support_scam':\n",
    "            reasons.append(\"This is a tech support scam because\")\n",
    "            if any(kw in analysis['keywords'] for kw in ['virus', 'infected', 'malware']):\n",
    "                reasons.append(\"falsely claims device infection\")\n",
    "            if analysis['phones']:\n",
    "                reasons.append(f\"provides a number to call\")\n",
    "            if analysis['entities']:\n",
    "                reasons.append(f\"impersonates {', '.join(analysis['entities'])}\")\n",
    "            reasons.append(\"to sell fake tech support.\")\n",
    "        \n",
    "        elif category == 'popup_scam':\n",
    "            reasons.append(\"This is a popup scam as\")\n",
    "            if any(kw in analysis['keywords'] for kw in ['click', 'download']):\n",
    "                reasons.append(\"prompts immediate action\")\n",
    "            if analysis['urgency']:\n",
    "                reasons.append(\"uses urgent calls-to-action\")\n",
    "            reasons.append(\"to install malware.\")\n",
    "        \n",
    "        elif category == 'ssn_scam':\n",
    "            reasons.append(\"This is an SSN scam because\")\n",
    "            if 'social security' in text.lower():\n",
    "                reasons.append(\"involves SSN threats\")\n",
    "            if any(kw in analysis['keywords'] for kw in ['suspended', 'verify']):\n",
    "                reasons.append(\"falsely claims SSN issues\")\n",
    "            if analysis['urgency']:\n",
    "                reasons.append(\"creates fear with urgency\")\n",
    "            reasons.append(\"which is a common SSN fraud tactic.\")\n",
    "        \n",
    "        elif category == 'sms_spam':\n",
    "            reasons.append(\"This is SMS spam as\")\n",
    "            if any(kw in analysis['keywords'] for kw in ['text', 'reply']):\n",
    "                reasons.append(\"uses text marketing tactics\")\n",
    "            if analysis['urls']:\n",
    "                reasons.append(\"includes promotional links\")\n",
    "            reasons.append(\"for unsolicited marketing.\")\n",
    "        \n",
    "        elif category == 'legitimate':\n",
    "            reasons.append(\"This appears legitimate because\")\n",
    "            if any(kw in analysis['keywords'] for kw in ['confirmation', 'tracking', 'receipt']):\n",
    "                reasons.append(\"contains transactional language\")\n",
    "            if not analysis['urgency']:\n",
    "                reasons.append(\"lacks urgent pressure\")\n",
    "            reasons.append(\"without suspicious fraud indicators.\")\n",
    "        \n",
    "        # Fallback\n",
    "        if len(reasons) <= 1:\n",
    "            category_name = category.replace('_', ' ')\n",
    "            reasons = [\n",
    "                f\"This message exhibits characteristics of {category_name}\",\n",
    "                \"based on its language patterns and content structure.\"\n",
    "            ]\n",
    "        \n",
    "        # Join with proper grammar\n",
    "        if len(reasons) == 2:\n",
    "            return f\"{reasons[0]} {reasons[1]}\"\n",
    "        elif len(reasons) > 2:\n",
    "            middle = ', '.join(reasons[1:-1])\n",
    "            return f\"{reasons[0]} {middle}, and {reasons[-1]}\"\n",
    "        return reasons[0]\n",
    "\n",
    "# Initialize generator\n",
    "reasoning_gen = ReasoningGenerator()\n",
    "print('‚úì Reasoning generator initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Generate Enhanced Training Data\n",
    "print('Generating contextual reasoning for training data...')\n",
    "\n",
    "# Build input texts with enhanced instruction\n",
    "train_X = [f'{INSTRUCTION_PREFIX}{txt}' for txt in train_df[TEXT_COL].tolist()]\n",
    "val_X = [f'{INSTRUCTION_PREFIX}{txt}' for txt in val_df[TEXT_COL].tolist()]\n",
    "\n",
    "# Extract labels\n",
    "train_y = [LABEL2ID[lbl] for lbl in train_df[LABEL_COL].tolist()]\n",
    "val_y = [LABEL2ID[lbl] for lbl in val_df[LABEL_COL].tolist()]\n",
    "\n",
    "# Generate contextual reasoning\n",
    "print('Generating training reasoning...')\n",
    "train_reason = [\n",
    "    reasoning_gen.generate_reason(txt, cat) \n",
    "    for txt, cat in zip(train_df[TEXT_COL].tolist(), train_df[LABEL_COL].tolist())\n",
    "]\n",
    "\n",
    "print('Generating validation reasoning...')\n",
    "val_reason = [\n",
    "    reasoning_gen.generate_reason(txt, cat)\n",
    "    for txt, cat in zip(val_df[TEXT_COL].tolist(), val_df[LABEL_COL].tolist())\n",
    "]\n",
    "\n",
    "print(f'\\n‚úì Prepared {len(train_X)} training samples and {len(val_X)} validation samples')\n",
    "print('\\nSample input:')\n",
    "print(train_X[0][:200])\n",
    "print('\\nSample reasoning:')\n",
    "print(train_reason[0])\n",
    "print('\\nAnother sample:')\n",
    "print(f'Category: {train_df[LABEL_COL].iloc[5]}')\n",
    "print(f'Reasoning: {train_reason[5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db5db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Tokenizer and Dataset\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '0'\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, local_files_only=False)\n",
    "except Exception as e:\n",
    "    print(f'First attempt failed: {e}')\n",
    "    print('Trying with trust_remote_code=True...')\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            BASE_MODEL, \n",
    "            use_fast=True, \n",
    "            trust_remote_code=True,\n",
    "            force_download=False\n",
    "        )\n",
    "    except Exception as e2:\n",
    "        print(f'Second attempt failed: {e2}')\n",
    "        from huggingface_hub import snapshot_download\n",
    "        model_path = snapshot_download(repo_id=BASE_MODEL, \n",
    "                                       ignore_patterns=[\"*.msgpack\", \"*.h5\", \"*.bin\"],\n",
    "                                       cache_dir=str(WORK_DIR / 'hf_cache'))\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True, local_files_only=True)\n",
    "\n",
    "print('‚úì Tokenizer loaded')\n",
    "print(f'Vocab size: {tokenizer.vocab_size}')\n",
    "\n",
    "class JointDataset:\n",
    "    def __init__(self, texts, cls_labels, reasons):\n",
    "        self.texts = texts\n",
    "        self.cls_labels = cls_labels\n",
    "        self.reasons = reasons\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.texts[idx]\n",
    "        tgt = self.reasons[idx]\n",
    "        enc = tokenizer(\n",
    "            src,\n",
    "            max_length=MAX_SOURCE_LENGTH,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "        )\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            dec = tokenizer(\n",
    "                tgt,\n",
    "                max_length=MAX_TARGET_LENGTH,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "            )\n",
    "        enc['labels'] = dec['input_ids']\n",
    "        enc['cls_labels'] = int(self.cls_labels[idx])\n",
    "        return enc\n",
    "\n",
    "train_ds = JointDataset(train_X, train_y, train_reason)\n",
    "val_ds = JointDataset(val_X, val_y, val_reason)\n",
    "print(f'‚úì Dataset created: {len(train_ds)} train, {len(val_ds)} val samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afbead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Joint Model Wrapper\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "class BartForJointClassificationAndGeneration(BartForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = int(getattr(config, 'num_labels', 2))\n",
    "        self.dropout = nn.Dropout(getattr(config, 'classifier_dropout', 0.1))\n",
    "        self.classifier = nn.Linear(config.d_model, self.num_labels)\n",
    "        self.cls_loss_fn = nn.CrossEntropyLoss()\n",
    "        self.cls_loss_weight = float(getattr(config, 'cls_loss_weight', 1.0))\n",
    "        self.gen_loss_weight = float(getattr(config, 'gen_loss_weight', 1.0))\n",
    "        self.config.num_labels = self.num_labels\n",
    "        self.config.cls_loss_weight = self.cls_loss_weight\n",
    "        self.config.gen_loss_weight = self.gen_loss_weight\n",
    "\n",
    "    def pooled_encoder(self, encoder_hidden_states, attention_mask):\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        summed = torch.sum(encoder_hidden_states * mask, dim=1)\n",
    "        counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "        return summed / counts\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, cls_labels=None, **kwargs):\n",
    "        kwargs.pop('return_dict', None)\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True,\n",
    "            **kwargs\n",
    "        )\n",
    "        enc_out = self.model.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        pooled = self.pooled_encoder(enc_out.last_hidden_state, attention_mask)\n",
    "        cls_logits = self.classifier(self.dropout(pooled))\n",
    "        cls_loss = None\n",
    "        if cls_labels is not None:\n",
    "            cls_loss = self.cls_loss_fn(cls_logits, cls_labels)\n",
    "        \n",
    "        total_loss = None\n",
    "        if outputs.loss is not None and cls_loss is not None:\n",
    "            total_loss = self.gen_loss_weight * outputs.loss + self.cls_loss_weight * cls_loss\n",
    "        elif outputs.loss is not None:\n",
    "            total_loss = outputs.loss\n",
    "        elif cls_loss is not None:\n",
    "            total_loss = cls_loss\n",
    "        \n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=total_loss,\n",
    "            logits=outputs.logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
    "            decoder_attentions=outputs.decoder_attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=enc_out.last_hidden_state,\n",
    "            encoder_hidden_states=None,\n",
    "            encoder_attentions=None\n",
    "        ), cls_logits\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(BASE_MODEL)\n",
    "cfg.num_labels = len(DEFAULT_LABELS)\n",
    "cfg.cls_loss_weight = CLS_LOSS_WEIGHT\n",
    "cfg.gen_loss_weight = GEN_LOSS_WEIGHT\n",
    "joint_model = BartForJointClassificationAndGeneration.from_pretrained(BASE_MODEL, config=cfg)\n",
    "joint_model.to(device)\n",
    "print('‚úì Joint model initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb46367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Training Setup\n",
    "import inspect\n",
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "class JointCollator:\n",
    "    def __init__(self, base_collator):\n",
    "        self.base_collator = base_collator\n",
    "    def __call__(self, features):\n",
    "        cls = torch.tensor([f.pop('cls_labels') for f in features], dtype=torch.long)\n",
    "        batch = self.base_collator(features)\n",
    "        batch['cls_labels'] = cls\n",
    "        return batch\n",
    "\n",
    "class LossThresholdEarlyStop(TrainerCallback):\n",
    "    def __init__(self, threshold: float, min_epochs: int = 0):\n",
    "        self.threshold = float(threshold)\n",
    "        self.min_epochs = int(min_epochs)\n",
    "    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        metrics = kwargs.get('metrics') or {}\n",
    "        eval_loss = metrics.get('eval_loss')\n",
    "        current_epoch = state.epoch or 0\n",
    "        if eval_loss is not None and current_epoch >= self.min_epochs and eval_loss <= self.threshold:\n",
    "            control.should_training_stop = True\n",
    "            print(f'Early stopping: eval_loss={eval_loss:.4f} <= {self.threshold} at epoch {current_epoch}')\n",
    "        return control\n",
    "\n",
    "args_dict = {\n",
    "    'output_dir': str(OUTPUT_DIR),\n",
    "    'learning_rate': LR,\n",
    "    'per_device_train_batch_size': BATCH_TRAIN,\n",
    "    'per_device_eval_batch_size': BATCH_EVAL,\n",
    "    'num_train_epochs': NUM_EPOCHS,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'logging_steps': 50,\n",
    "    'gradient_accumulation_steps': GRAD_ACCUM,\n",
    "    'report_to': ['none'],\n",
    "    'fp16': torch.cuda.is_available(),\n",
    "    'bf16': False,\n",
    "}\n",
    "\n",
    "sig = inspect.signature(TrainingArguments.__init__)\n",
    "if 'evaluation_strategy' in sig.parameters:\n",
    "    args_dict['evaluation_strategy'] = 'epoch'\n",
    "if 'save_strategy' in sig.parameters:\n",
    "    args_dict['save_strategy'] = 'epoch'\n",
    "if 'predict_with_generate' in sig.parameters:\n",
    "    args_dict['predict_with_generate'] = True\n",
    "if 'generation_max_length' in sig.parameters:\n",
    "    args_dict['generation_max_length'] = MAX_TARGET_LENGTH\n",
    "\n",
    "training_args = TrainingArguments(**args_dict)\n",
    "callbacks = [LossThresholdEarlyStop(EARLY_STOP_LOSS, MIN_EPOCHS)]\n",
    "\n",
    "_base_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=joint_model)\n",
    "data_collator = JointCollator(_base_collator)\n",
    "\n",
    "class JointTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        (outputs, cls_logits) = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        return (loss, {'lm': outputs, 'cls_logits': cls_logits}) if return_outputs else loss\n",
    "\n",
    "trainer = JointTrainer(\n",
    "    model=joint_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "print('‚úì Trainer configured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4890fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Train and Save\n",
    "print('Starting training with enhanced contextual reasoning...')\n",
    "trainer.train()\n",
    "trainer.save_model(str(OUTPUT_DIR))\n",
    "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
    "print(f'\\n‚úì Model saved to: {OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b601dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Enhanced Inference Test\n",
    "joint_model.eval()\n",
    "sample_text = val_df.iloc[0][TEXT_COL] if len(val_df) else train_df.iloc[0][TEXT_COL]\n",
    "true_label = val_df.iloc[0][LABEL_COL] if len(val_df) else train_df.iloc[0][LABEL_COL]\n",
    "src = f'{INSTRUCTION_PREFIX}{sample_text}'\n",
    "\n",
    "inputs = tokenizer([src], return_tensors='pt', truncation=True, padding=True, max_length=MAX_SOURCE_LENGTH).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Classification\n",
    "    enc_out = joint_model.model.encoder(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        return_dict=True\n",
    "    )\n",
    "    pooled = joint_model.pooled_encoder(enc_out.last_hidden_state, inputs['attention_mask'])\n",
    "    cls_logits = joint_model.classifier(pooled)  # No dropout during inference\n",
    "    pred_id = int(cls_logits.argmax(-1).cpu().item())\n",
    "    pred_label = ID2LABEL[pred_id]\n",
    "    \n",
    "    # Enhanced Generation with better parameters\n",
    "    gen_ids = joint_model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_new_tokens=MAX_TARGET_LENGTH,\n",
    "        num_beams=5,  # Higher beam search\n",
    "        length_penalty=1.0,  # Balance length\n",
    "        no_repeat_ngram_size=3,  # Avoid repetition\n",
    "        early_stopping=True,\n",
    "        do_sample=False\n",
    "    )\n",
    "    reason = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ENHANCED INFERENCE TEST')\n",
    "print('='*80)\n",
    "print(f'\\nInput snippet: {sample_text[:200]}...')\n",
    "print(f'\\nTrue label: {true_label}')\n",
    "print(f'Predicted label: {pred_label}')\n",
    "print(f'\\nContextual Reasoning:')\n",
    "print(reason)\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aee7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Test Multiple Examples\n",
    "print('\\n' + '='*80)\n",
    "print('TESTING MULTIPLE EXAMPLES')\n",
    "print('='*80 + '\\n')\n",
    "\n",
    "test_samples = val_df.sample(n=min(5, len(val_df)), random_state=42)\n",
    "\n",
    "for idx, (_, row) in enumerate(test_samples.iterrows(), 1):\n",
    "    text = row[TEXT_COL]\n",
    "    true_label = row[LABEL_COL]\n",
    "    src = f'{INSTRUCTION_PREFIX}{text}'\n",
    "    inputs = tokenizer([src], return_tensors='pt', truncation=True, padding=True, max_length=MAX_SOURCE_LENGTH).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_out = joint_model.model.encoder(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            return_dict=True\n",
    "        )\n",
    "        pooled = joint_model.pooled_encoder(enc_out.last_hidden_state, inputs['attention_mask'])\n",
    "        cls_logits = joint_model.classifier(pooled)\n",
    "        pred_id = int(cls_logits.argmax(-1).cpu().item())\n",
    "        pred_label = ID2LABEL[pred_id]\n",
    "        \n",
    "        gen_ids = joint_model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=MAX_TARGET_LENGTH,\n",
    "            num_beams=5,\n",
    "            length_penalty=1.0,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        reason = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    correct = '‚úì' if pred_label == true_label else '‚úó'\n",
    "    print(f'Example {idx} {correct}')\n",
    "    print(f'Text: {text[:150]}...')\n",
    "    print(f'True: {true_label} | Predicted: {pred_label}')\n",
    "    print(f'Reasoning: {reason}')\n",
    "    print('-'*80 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Zip Artifacts (Optional)\n",
    "import shutil\n",
    "ZIP_PATH = WORK_DIR / 'unified-bart-joint-enhanced.zip'\n",
    "if ZIP_PATH.exists():\n",
    "    ZIP_PATH.unlink()\n",
    "shutil.make_archive(str(ZIP_PATH.with_suffix('')), 'zip', str(OUTPUT_DIR))\n",
    "print(f'‚úì Zipped model to: {ZIP_PATH}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
