{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13082789,"sourceType":"datasetVersion","datasetId":8286057}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß† Phi-3.5-mini Fraud Detection with Real Reasoning\n\nTrain Microsoft's Phi-3.5-mini-instruct for fraud classification + contextual reasoning.\n\n**Key Features:**\n- Multi-task learning (classification + reasoning generation)\n- 4-bit quantization for efficient training\n- LoRA for parameter-efficient fine-tuning\n- Real contextual reasoning (not templates)\n- Optimized for Kaggle free tier (T4 GPU)\n\n**Model:** `microsoft/Phi-3.5-mini-instruct` (3.8B params)\n**Expected Training Time:** ~2.5 hours on T4 GPU\n**VRAM Usage:** ~8GB with 4-bit quantization","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup and Installation","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install -q transformers==4.44.2 \\\n    datasets==2.19.0 \\\n    accelerate==0.30.1 \\\n    peft==0.11.1 \\\n    bitsandbytes==0.43.1 \\\n    trl==0.8.6 \\\n    sentencepiece \\\n    protobuf","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Imports\nimport os\nimport json\nimport warnings\nfrom pathlib import Path\nimport re\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    BitsAndBytesConfig,\n    set_seed\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom datasets import Dataset\n\nwarnings.filterwarnings('ignore')\n\n# Check environment\nIS_KAGGLE = Path('/kaggle').exists()\nINPUT_DIR = Path('/kaggle/input') if IS_KAGGLE else Path('..')\nWORK_DIR = Path('/kaggle/working') if IS_KAGGLE else Path('.')\nWORK_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f'Environment: {\"Kaggle\" if IS_KAGGLE else \"Local\"}')\nprint(f'PyTorch: {torch.__version__}')\nprint(f'CUDA Available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Configuration","metadata":{}},{"cell_type":"code","source":"# Set seed for reproducibility\nSEED = 42\nset_seed(SEED)\n\n# Model configuration\nMODEL_NAME = \"microsoft/Phi-3.5-mini-instruct\"\nOUTPUT_DIR = WORK_DIR / \"phi-3.5-fraud-reasoning\"\n\n# Dataset configuration\nCSV_PATH = INPUT_DIR / 'fraud-dataset' / 'final_fraud_detection_dataset.csv'\nif not CSV_PATH.exists():\n    matches = list(INPUT_DIR.glob('**/final_fraud_detection_dataset.csv'))\n    if matches:\n        CSV_PATH = matches[0]\n    else:\n        print('‚ö†Ô∏è Dataset not found! Please attach your fraud dataset to the Kaggle notebook.')\n\n# Labels\nLABELS = [\n    'job_scam',\n    'legitimate',\n    'phishing',\n    'popup_scam',\n    'refund_scam',\n    'reward_scam',\n    'sms_spam',\n    'ssn_scam',\n    'tech_support_scam'\n]\n\n# Training hyperparameters\nMAX_SEQ_LENGTH = 1024  # Phi-3.5 supports up to 128K, but 1024 is efficient\nTRAIN_SIZE = 0.9\nNUM_EPOCHS = 3\nBATCH_SIZE = 4\nGRAD_ACCUM_STEPS = 4  # Effective batch size = 16\nLEARNING_RATE = 2e-4\nWARMUP_RATIO = 0.1\nMAX_GRAD_NORM = 0.3\nWEIGHT_DECAY = 0.001\n\n# LoRA configuration\nLORA_R = 16\nLORA_ALPHA = 32\nLORA_DROPOUT = 0.05\nLORA_TARGET_MODULES = [\n    \"q_proj\",\n    \"k_proj\",\n    \"v_proj\",\n    \"o_proj\",\n    \"gate_proj\",\n    \"up_proj\",\n    \"down_proj\"\n]\n\nprint(f'Model: {MODEL_NAME}')\nprint(f'Output: {OUTPUT_DIR}')\nprint(f'Max sequence length: {MAX_SEQ_LENGTH}')\nprint(f'Effective batch size: {BATCH_SIZE * GRAD_ACCUM_STEPS}')\nprint(f'Training epochs: {NUM_EPOCHS}')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Load and Prepare Data","metadata":{}},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv(CSV_PATH)\nprint(f'Loaded dataset: {df.shape}')\nprint(f'\\nColumns: {df.columns.tolist()}')\n\n# Filter to known labels\ndf = df[df['detailed_category'].isin(LABELS)].copy()\ndf = df[['text', 'detailed_category']].dropna()\n\nprint(f'\\nFiltered dataset: {df.shape}')\nprint(f'\\nLabel distribution:')\nprint(df['detailed_category'].value_counts())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize label distribution\nplt.figure(figsize=(12, 6))\ndf['detailed_category'].value_counts().plot(kind='bar', color='skyblue')\nplt.title('Fraud Category Distribution', fontsize=14, fontweight='bold')\nplt.xlabel('Category')\nplt.ylabel('Count')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split data\ntrain_df, val_df = train_test_split(\n    df,\n    test_size=1-TRAIN_SIZE,\n    random_state=SEED,\n    stratify=df['detailed_category']\n)\n\nprint(f'Train set: {len(train_df)} samples')\nprint(f'Validation set: {len(val_df)} samples')\n\n# Show sample\nprint(f'\\n--- Sample Message ---')\nsample = train_df.iloc[0]\nprint(f'Category: {sample[\"detailed_category\"]}')\nprint(f'Text: {sample[\"text\"][:200]}...')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Contextual Reasoning Generator\n\nThis generates real, context-aware reasoning based on message analysis (not templates).","metadata":{}},{"cell_type":"code","source":"class ContextualReasoningGenerator:\n    \"\"\"Generate contextual, feature-based reasoning for fraud detection.\"\"\"\n    \n    # Fraud indicator patterns\n    INDICATORS = {\n        'phishing': {\n            'keywords': ['verify', 'account', 'suspended', 'confirm', 'click', 'update', \n                        'security', 'alert', 'unauthorized', 'immediately', 'login', 'password'],\n            'patterns': [r'click\\s+(here|link|below)', r'verify\\s+your\\s+(account|identity|information)',\n                        r'account\\s+(suspended|locked|compromised|closed)', r'urgent\\s+action\\s+required',\n                        r'confirm\\s+your\\s+identity'],\n            'entities': ['bank', 'paypal', 'amazon', 'netflix', 'apple', 'microsoft', 'irs'],\n            'risk': 'CRITICAL'\n        },\n        'job_scam': {\n            'keywords': ['work from home', 'easy money', 'no experience', 'apply now', \n                        'guaranteed income', 'flexible hours', 'earn', 'opportunity', 'hiring'],\n            'patterns': [r'\\$\\d+[k]?\\s*(?:per|/|a)\\s*(?:day|week|month|hour)', \n                        r'work\\s+from\\s+home', r'no\\s+experience\\s+(?:required|needed)',\n                        r'guaranteed\\s+(?:income|salary|pay|earnings)'],\n            'entities': ['remote', 'telecommute', 'online work', 'freelance'],\n            'risk': 'HIGH'\n        },\n        'reward_scam': {\n            'keywords': ['congratulations', 'winner', 'prize', 'reward', 'gift card', \n                        'won', 'claim', 'free', 'selected', 'lucky'],\n            'patterns': [r'(?:won|winner|prize|reward|gift).*\\$\\d+',\n                        r'congratulations.*(?:won|winner|selected)',\n                        r'claim\\s+your\\s+(?:prize|reward|gift)',\n                        r'you(?:\\'ve|\\s+have)\\s+won'],\n            'entities': ['walmart', 'target', 'amazon', 'visa', 'mastercard', 'gift card'],\n            'risk': 'HIGH'\n        },\n        'refund_scam': {\n            'keywords': ['refund', 'overpaid', 'owed', 'payment', 'credit', 'return', 'reimburse'],\n            'patterns': [r'(?:refund|owed|overpaid).*\\$\\d+', r'you\\s+are\\s+owed'],\n            'entities': ['irs', 'tax', 'government', 'revenue'],\n            'risk': 'HIGH'\n        },\n        'tech_support_scam': {\n            'keywords': ['virus', 'infected', 'malware', 'computer', 'tech support', \n                        'call', 'error', 'security threat', 'system'],\n            'patterns': [r'(?:virus|malware)\\s+detected', r'computer\\s+(?:infected|compromised)',\n                        r'call\\s+(?:immediately|now|us)', r'tech\\s+support'],\n            'entities': ['microsoft', 'windows', 'apple', 'mcafee', 'norton'],\n            'risk': 'CRITICAL'\n        },\n        'popup_scam': {\n            'keywords': ['click', 'download', 'install', 'update required', 'warning'],\n            'patterns': [r'click\\s+(?:here|now)', r'download\\s+now', r'update\\s+(?:required|needed)'],\n            'entities': [],\n            'risk': 'MEDIUM'\n        },\n        'ssn_scam': {\n            'keywords': ['social security', 'ssn', 'suspended', 'verify', 'benefits'],\n            'patterns': [r'social\\s+security\\s+(?:number|suspended|benefits)', r'ssn\\s+suspended'],\n            'entities': ['social security', 'ssa', 'social security administration'],\n            'risk': 'CRITICAL'\n        },\n        'sms_spam': {\n            'keywords': ['text', 'reply', 'stop', 'offer', 'deal', 'discount'],\n            'patterns': [r'text\\s+\\w+\\s+to\\s+\\d+', r'reply\\s+(?:yes|stop)'],\n            'entities': [],\n            'risk': 'LOW'\n        },\n        'legitimate': {\n            'keywords': ['confirmation', 'receipt', 'order', 'tracking', 'delivery', 'invoice'],\n            'patterns': [r'tracking\\s+(?:number|#):\\s*\\w+', r'order\\s+#\\d+'],\n            'entities': [],\n            'risk': 'NONE'\n        }\n    }\n    \n    def __init__(self):\n        # Compile regex patterns\n        self.compiled_patterns = {}\n        for category, data in self.INDICATORS.items():\n            self.compiled_patterns[category] = [\n                re.compile(pattern, re.IGNORECASE) for pattern in data['patterns']\n            ]\n    \n    def extract_features(self, text: str, category: str) -> dict:\n        \"\"\"Extract fraud indicators from message.\"\"\"\n        text_lower = text.lower()\n        \n        if category not in self.INDICATORS:\n            category = 'legitimate'\n        \n        indicators = self.INDICATORS[category]\n        \n        # Find keywords\n        found_keywords = [kw for kw in indicators['keywords'] if kw in text_lower]\n        \n        # Find patterns\n        found_patterns = []\n        for pattern in self.compiled_patterns[category]:\n            matches = pattern.findall(text)\n            if matches:\n                found_patterns.extend(matches[:2])  # Limit to 2 per pattern\n        \n        # Find entities\n        found_entities = [e for e in indicators['entities'] if e in text_lower]\n        \n        # Extract common elements\n        urgency_words = ['urgent', 'immediately', 'now', 'expire', 'limited time', 'act now', 'hurry']\n        found_urgency = [w for w in urgency_words if w in text_lower]\n        \n        # Money amounts\n        money_pattern = re.compile(r'\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d{2})?')\n        money = money_pattern.findall(text)\n        \n        # Phone numbers\n        phone_pattern = re.compile(r'1?\\s*[-.]?\\(?\\d{3}\\)?[-.]?\\d{3}[-.]?\\d{4}')\n        phones = phone_pattern.findall(text)\n        \n        # URLs\n        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+|click\\s+here', re.IGNORECASE)\n        has_urls = bool(url_pattern.search(text))\n        \n        return {\n            'keywords': found_keywords[:5],\n            'patterns': found_patterns[:3],\n            'entities': found_entities[:3],\n            'urgency': found_urgency[:3],\n            'money': money[:2],\n            'phones': phones[:1],\n            'urls': has_urls,\n            'risk': indicators['risk']\n        }\n    \n    def generate_reasoning(self, text: str, category: str) -> str:\n        \"\"\"Generate contextual reasoning based on message features.\"\"\"\n        features = self.extract_features(text, category)\n        \n        # Build reasoning components\n        components = []\n        \n        # Category-specific analysis\n        if category == 'phishing':\n            components.append(\"This message exhibits phishing characteristics:\")\n            if features['entities']:\n                components.append(f\"‚Ä¢ Impersonates trusted entities: {', '.join(features['entities'][:2])}\")\n            if features['urgency']:\n                components.append(f\"‚Ä¢ Uses urgency tactics: '{features['urgency'][0]}'\")\n            if features['keywords']:\n                suspicious = [k for k in features['keywords'] if k in ['verify', 'suspended', 'unauthorized', 'confirm']]\n                if suspicious:\n                    components.append(f\"‚Ä¢ Requests sensitive actions: {', '.join(suspicious[:2])}\")\n            if features['urls']:\n                components.append(\"‚Ä¢ Contains suspicious links to harvest credentials\")\n            components.append(\"These tactics aim to steal login credentials or personal information.\")\n        \n        elif category == 'job_scam':\n            components.append(\"This appears to be a job scam:\")\n            if features['money']:\n                components.append(f\"‚Ä¢ Promises unrealistic income: {features['money'][0]}\")\n            if 'no experience' in ' '.join(features['keywords']):\n                components.append(\"‚Ä¢ Claims no qualifications needed\")\n            if 'work from home' in ' '.join(features['keywords']):\n                components.append(\"‚Ä¢ Promotes vague work-from-home opportunity\")\n            if features['urgency']:\n                components.append(f\"‚Ä¢ Creates false urgency: '{features['urgency'][0]}'\")\n            components.append(\"Legitimate jobs require proper application processes and realistic expectations.\")\n        \n        elif category == 'reward_scam':\n            components.append(\"This is a reward scam:\")\n            if features['money']:\n                components.append(f\"‚Ä¢ Claims you've won: {features['money'][0]}\")\n            if any(k in features['keywords'] for k in ['winner', 'selected', 'congratulations']):\n                components.append(\"‚Ä¢ Falsely claims you've been selected without participation\")\n            if features['urgency']:\n                components.append(\"‚Ä¢ Pressures quick action to 'claim' prize\")\n            if features['urls']:\n                components.append(\"‚Ä¢ Directs to fraudulent site for data collection\")\n            components.append(\"Legitimate prizes don't require claiming through unsolicited messages.\")\n        \n        elif category == 'refund_scam':\n            components.append(\"This is a refund scam:\")\n            if features['money']:\n                components.append(f\"‚Ä¢ Claims you're owed: {features['money'][0]}\")\n            if features['entities']:\n                components.append(f\"‚Ä¢ Impersonates: {', '.join(features['entities'])}\")\n            if features['urgency']:\n                components.append(\"‚Ä¢ Creates urgency to process fake refund\")\n            components.append(\"Designed to collect personal or financial information under false pretenses.\")\n        \n        elif category == 'tech_support_scam':\n            components.append(\"This is a tech support scam:\")\n            if any(k in features['keywords'] for k in ['virus', 'infected', 'malware']):\n                components.append(\"‚Ä¢ Falsely claims device infection\")\n            if features['entities']:\n                components.append(f\"‚Ä¢ Impersonates: {', '.join(features['entities'])}\")\n            if features['phones']:\n                components.append(\"‚Ä¢ Provides number for fake tech support\")\n            if features['urgency']:\n                components.append(\"‚Ä¢ Uses fear tactics for immediate action\")\n            components.append(\"Legitimate tech companies don't send unsolicited security alerts.\")\n        \n        elif category == 'popup_scam':\n            components.append(\"This is a popup scam:\")\n            if any(k in features['keywords'] for k in ['click', 'download', 'install']):\n                components.append(\"‚Ä¢ Prompts immediate download/click action\")\n            if features['urgency']:\n                components.append(\"‚Ä¢ Uses urgent calls-to-action\")\n            components.append(\"Designed to install malware or redirect to phishing sites.\")\n        \n        elif category == 'ssn_scam':\n            components.append(\"This is an SSN scam:\")\n            if 'social security' in text.lower():\n                components.append(\"‚Ä¢ Involves Social Security threats\")\n            if any(k in features['keywords'] for k in ['suspended', 'verify']):\n                components.append(\"‚Ä¢ Falsely claims SSN issues requiring verification\")\n            if features['urgency']:\n                components.append(\"‚Ä¢ Creates fear with urgent language\")\n            components.append(\"SSA never contacts citizens via unsolicited messages about suspensions.\")\n        \n        elif category == 'sms_spam':\n            components.append(\"This is SMS spam:\")\n            if any(k in features['keywords'] for k in ['text', 'reply', 'offer']):\n                components.append(\"‚Ä¢ Uses unsolicited marketing tactics\")\n            if features['urls']:\n                components.append(\"‚Ä¢ Includes promotional links\")\n            components.append(\"Violates anti-spam regulations for commercial messaging.\")\n        \n        elif category == 'legitimate':\n            components.append(\"This appears legitimate:\")\n            if any(k in features['keywords'] for k in ['confirmation', 'tracking', 'receipt', 'order']):\n                components.append(\"‚Ä¢ Contains transactional language\")\n            if not features['urgency']:\n                components.append(\"‚Ä¢ No pressure tactics detected\")\n            if not features['urls'] or 'tracking' in features['keywords']:\n                components.append(\"‚Ä¢ No suspicious requests for sensitive information\")\n            components.append(\"Message shows normal business communication patterns.\")\n        \n        # Fallback if no specific indicators\n        if len(components) <= 1:\n            category_display = category.replace('_', ' ').title()\n            components = [\n                f\"This message is classified as {category_display}.\",\n                \"Analysis based on language patterns, content structure, and typical fraud indicators.\",\n                f\"Risk level: {features['risk']}\"\n            ]\n        \n        # Add risk assessment\n        if features['risk'] != 'NONE':\n            components.append(f\"\\n**Risk Level:** {features['risk']}\")\n        \n        return '\\n'.join(components)\n\n# Initialize reasoning generator\nreasoning_gen = ContextualReasoningGenerator()\nprint('‚úì Contextual reasoning generator initialized')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test reasoning generator with samples\nprint('Testing reasoning generator with sample messages:\\n')\nprint('='*80)\n\nfor i in range(3):\n    sample = train_df.iloc[i]\n    text = sample['text']\n    category = sample['detailed_category']\n    \n    reasoning = reasoning_gen.generate_reasoning(text, category)\n    \n    print(f'\\nExample {i+1}:')\n    print(f'Category: {category}')\n    print(f'Text: {text[:150]}...')\n    print(f'\\nReasoning:')\n    print(reasoning)\n    print('='*80)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Create Training Data with Phi-3.5 Format","metadata":{}},{"cell_type":"code","source":"def format_instruction_phi35(text: str, category: str, reasoning: str) -> str:\n    \"\"\"Format data for Phi-3.5 instruction tuning.\"\"\"\n    \n    # Phi-3.5 chat template format\n    prompt = f\"\"\"<|system|>\nYou are an expert fraud detection AI. Analyze messages to identify fraud types and provide detailed reasoning based on specific indicators in the text.<|end|>\n<|user|>\nAnalyze this message for fraud and provide classification with detailed reasoning:\n\nMessage: {text}<|end|>\n<|assistant|>\n**Classification:** {category}\n\n**Reasoning:**\n{reasoning}<|end|>\"\"\"\n    \n    return prompt\n\n# Generate training data\nprint('Generating training data with contextual reasoning...')\n\ntrain_prompts = []\nfor idx, row in train_df.iterrows():\n    text = row['text']\n    category = row['detailed_category']\n    reasoning = reasoning_gen.generate_reasoning(text, category)\n    prompt = format_instruction_phi35(text, category, reasoning)\n    train_prompts.append(prompt)\n\nval_prompts = []\nfor idx, row in val_df.iterrows():\n    text = row['text']\n    category = row['detailed_category']\n    reasoning = reasoning_gen.generate_reasoning(text, category)\n    prompt = format_instruction_phi35(text, category, reasoning)\n    val_prompts.append(prompt)\n\nprint(f'‚úì Generated {len(train_prompts)} training examples')\nprint(f'‚úì Generated {len(val_prompts)} validation examples')\n\n# Show sample\nprint('\\n--- Sample Training Example ---')\nprint(train_prompts[0][:500] + '...')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create HuggingFace datasets\ntrain_dataset = Dataset.from_dict({'text': train_prompts})\nval_dataset = Dataset.from_dict({'text': val_prompts})\n\nprint(f'Train dataset: {len(train_dataset)} samples')\nprint(f'Validation dataset: {len(val_dataset)} samples')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Load Model with 4-bit Quantization","metadata":{}},{"cell_type":"code","source":"# 4-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nprint(f'Loading {MODEL_NAME}...')\nprint('This may take 2-3 minutes on first run...')\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else \"eager\"\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_NAME,\n    trust_remote_code=True,\n    padding_side=\"right\",\n    add_eos_token=True,\n    add_bos_token=True,\n)\ntokenizer.pad_token = tokenizer.eos_token\n\nprint('‚úì Model and tokenizer loaded')\nprint(f'Model size: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters')\nprint(f'Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e9:.2f}B')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Setup LoRA for Efficient Training","metadata":{}},{"cell_type":"code","source":"# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# LoRA configuration\nlora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    target_modules=LORA_TARGET_MODULES,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Apply LoRA\nmodel = get_peft_model(model, lora_config)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()\n\nprint('\\n‚úì LoRA applied successfully')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Training Configuration","metadata":{}},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=str(OUTPUT_DIR),\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n    learning_rate=LEARNING_RATE,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=WARMUP_RATIO,\n    max_grad_norm=MAX_GRAD_NORM,\n    weight_decay=WEIGHT_DECAY,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    evaluation_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    fp16=False,\n    bf16=torch.cuda.is_available(),\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\",\n    seed=SEED,\n)\n\n# Data collator for completion only (trains only on assistant responses)\nresponse_template = \"<|assistant|>\"\ncollator = DataCollatorForCompletionOnlyLM(\n    response_template=response_template,\n    tokenizer=tokenizer,\n    mlm=False\n)\n\n# SFT Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=collator,\n    dataset_text_field=\"text\",\n    max_seq_length=MAX_SEQ_LENGTH,\n    packing=False,\n)\n\nprint('‚úì Trainer configured')\nprint(f'Effective batch size: {BATCH_SIZE * GRAD_ACCUM_STEPS}')\nprint(f'Total training steps: {len(train_dataset) // (BATCH_SIZE * GRAD_ACCUM_STEPS) * NUM_EPOCHS}')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Train the Model\n\n‚è±Ô∏è Expected time: ~2.5 hours on T4 GPU","metadata":{}},{"cell_type":"code","source":"# Clear cache\ntorch.cuda.empty_cache()\n\nprint('Starting training...')\nprint('='*80)\n\n# Train\ntrainer.train()\n\nprint('='*80)\nprint('\\n‚úì Training completed!')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Save Model","metadata":{}},{"cell_type":"code","source":"# Save final model\ntrainer.save_model(str(OUTPUT_DIR))\ntokenizer.save_pretrained(str(OUTPUT_DIR))\n\nprint(f'‚úì Model saved to: {OUTPUT_DIR}')\n\n# Save training config\nconfig_dict = {\n    'model_name': MODEL_NAME,\n    'labels': LABELS,\n    'max_seq_length': MAX_SEQ_LENGTH,\n    'lora_r': LORA_R,\n    'lora_alpha': LORA_ALPHA,\n    'training_samples': len(train_dataset),\n    'validation_samples': len(val_dataset),\n}\n\nwith open(OUTPUT_DIR / 'training_config.json', 'w') as f:\n    json.dump(config_dict, f, indent=2)\n\nprint('‚úì Training config saved')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 11. Evaluation & Testing","metadata":{}},{"cell_type":"code","source":"def test_inference(text: str, model, tokenizer, max_length: int = 512):\n    \"\"\"Test inference on a single message.\"\"\"\n    \n    prompt = f\"\"\"<|system|>\nYou are an expert fraud detection AI. Analyze messages to identify fraud types and provide detailed reasoning based on specific indicators in the text.<|end|>\n<|user|>\nAnalyze this message for fraud and provide classification with detailed reasoning:\n\nMessage: {text}<|end|>\n<|assistant|>\n\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_length,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract assistant response\n    if \"<|assistant|>\" in response:\n        response = response.split(\"<|assistant|>\")[-1].strip()\n    \n    return response\n\nprint('Testing trained model on validation samples...')\nprint('='*80)\n\n# Test on 5 random validation samples\ntest_samples = val_df.sample(n=min(5, len(val_df)), random_state=42)\n\nfor idx, (_, row) in enumerate(test_samples.iterrows(), 1):\n    text = row['text']\n    true_category = row['detailed_category']\n    \n    print(f'\\nüîç Test Example {idx}')\n    print(f'Message: {text[:200]}...')\n    print(f'\\nTrue Category: {true_category}')\n    print('\\nModel Response:')\n    \n    response = test_inference(text, model, tokenizer)\n    print(response)\n    print('='*80)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 12. Comprehensive Evaluation","metadata":{}},{"cell_type":"code","source":"def extract_classification(response: str) -> str:\n    \"\"\"Extract classification from model response.\"\"\"\n    # Look for classification pattern\n    patterns = [\n        r'\\*\\*Classification:\\*\\*\\s*(\\w+)',\n        r'Classification:\\s*(\\w+)',\n        r'Category:\\s*(\\w+)',\n    ]\n    \n    for pattern in patterns:\n        match = re.search(pattern, response, re.IGNORECASE)\n        if match:\n            return match.group(1).lower().replace(' ', '_')\n    \n    # Fallback: check if any label appears in response\n    response_lower = response.lower()\n    for label in LABELS:\n        if label.replace('_', ' ') in response_lower or label in response_lower:\n            return label\n    \n    return 'unknown'\n\nprint('Running comprehensive evaluation on validation set...')\nprint('This may take 10-15 minutes...')\n\npredictions = []\ntrue_labels = []\n\n# Evaluate on subset for speed (use full set for final eval)\neval_samples = val_df.sample(n=min(100, len(val_df)), random_state=42)\n\nfor idx, (_, row) in enumerate(eval_samples.iterrows(), 1):\n    text = row['text']\n    true_category = row['detailed_category']\n    \n    response = test_inference(text, model, tokenizer, max_length=256)\n    pred_category = extract_classification(response)\n    \n    predictions.append(pred_category)\n    true_labels.append(true_category)\n    \n    if idx % 10 == 0:\n        print(f'Processed {idx}/{len(eval_samples)} samples...')\n\nprint('\\n‚úì Evaluation completed')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate accuracy\naccuracy = accuracy_score(true_labels, predictions)\nprint(f'\\nüìä Overall Accuracy: {accuracy:.2%}')\n\n# Classification report\nprint('\\nüìã Classification Report:')\nprint(classification_report(true_labels, predictions, target_names=LABELS, zero_division=0))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Confusion matrix\ncm = confusion_matrix(true_labels, predictions, labels=LABELS)\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(\n    cm,\n    annot=True,\n    fmt='d',\n    cmap='Blues',\n    xticklabels=LABELS,\n    yticklabels=LABELS\n)\nplt.title('Confusion Matrix - Phi-3.5 Fraud Detection', fontsize=14, fontweight='bold')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.savefig(WORK_DIR / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint('‚úì Confusion matrix saved')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 13. Interactive Testing","metadata":{}},{"cell_type":"code","source":"# Test with custom messages\ntest_messages = [\n    \"Congratulations! You've won $1000 in our sweepstakes. Click here to claim now!\",\n    \"Your account has been compromised. Verify immediately at secure-bank-login.com\",\n    \"Work from home and earn $5000/week. No experience needed. Apply now!\",\n    \"Your order #12345 has shipped. Tracking: 1Z999AA10123456784\",\n]\n\nprint('Testing with custom messages:')\nprint('='*80)\n\nfor i, msg in enumerate(test_messages, 1):\n    print(f'\\nüìß Test Message {i}:')\n    print(msg)\n    print('\\nü§ñ Model Analysis:')\n    response = test_inference(msg, model, tokenizer)\n    print(response)\n    print('='*80)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 14. Save Results and Create Archive","metadata":{}},{"cell_type":"code","source":"# Save evaluation results\nresults_df = pd.DataFrame({\n    'true_label': true_labels,\n    'predicted_label': predictions,\n    'correct': [t == p for t, p in zip(true_labels, predictions)]\n})\n\nresults_df.to_csv(WORK_DIR / 'evaluation_results.csv', index=False)\nprint('‚úì Evaluation results saved')\n\n# Save metrics\nmetrics = {\n    'accuracy': float(accuracy),\n    'num_samples': len(eval_samples),\n    'model': MODEL_NAME,\n    'timestamp': pd.Timestamp.now().isoformat()\n}\n\nwith open(WORK_DIR / 'metrics.json', 'w') as f:\n    json.dump(metrics, f, indent=2)\n\nprint('‚úì Metrics saved')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create archive of model\nimport shutil\n\nif IS_KAGGLE:\n    print('Creating model archive...')\n    archive_path = WORK_DIR / 'phi-3.5-fraud-reasoning'\n    shutil.make_archive(str(archive_path), 'zip', str(OUTPUT_DIR))\n    print(f'‚úì Model archived to: {archive_path}.zip')\n    print('\\nüì• Download this file from the Kaggle output section')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 15. Summary and Next Steps","metadata":{}},{"cell_type":"code","source":"print('\\n' + '='*80)\nprint('üéâ TRAINING COMPLETED SUCCESSFULLY!')\nprint('='*80)\n\nprint(f'\\nüìä Final Results:')\nprint(f'  ‚Ä¢ Model: {MODEL_NAME}')\nprint(f'  ‚Ä¢ Training Samples: {len(train_dataset)}')\nprint(f'  ‚Ä¢ Validation Samples: {len(val_dataset)}')\nprint(f'  ‚Ä¢ Overall Accuracy: {accuracy:.2%}')\nprint(f'  ‚Ä¢ Model Size: ~3.8B parameters')\nprint(f'  ‚Ä¢ Training Method: LoRA (4-bit quantization)')\n\nprint(f'\\nüìÅ Saved Artifacts:')\nprint(f'  ‚Ä¢ Model: {OUTPUT_DIR}')\nprint(f'  ‚Ä¢ Results: {WORK_DIR}/evaluation_results.csv')\nprint(f'  ‚Ä¢ Metrics: {WORK_DIR}/metrics.json')\nprint(f'  ‚Ä¢ Confusion Matrix: {WORK_DIR}/confusion_matrix.png')\n\nprint(f'\\nüöÄ Next Steps:')\nprint(f'  1. Download the model archive from Kaggle output')\nprint(f'  2. Load model locally for deployment:')\nprint(f'     from transformers import AutoModelForCausalLM, AutoTokenizer')\nprint(f'     from peft import PeftModel')\nprint(f'     model = AutoModelForCausalLM.from_pretrained(\"{MODEL_NAME}\")')\nprint(f'     model = PeftModel.from_pretrained(model, \"path/to/saved/model\")')\nprint(f'  3. Create REST API or demo interface')\nprint(f'  4. Deploy to production')\n\nprint('\\n' + '='*80)","metadata":{},"outputs":[],"execution_count":null}]}