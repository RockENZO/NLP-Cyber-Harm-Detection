{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c666a435",
   "metadata": {},
   "source": [
    "# üöÄ Fraud Detection Training on Kaggle\n",
    "\n",
    "This notebook is optimized for Kaggle's environment with GPU acceleration.\n",
    "\n",
    "## üìä Dataset\n",
    "- Upload your `final_fraud_detection_dataset.csv`\n",
    "- The notebook will automatically load and preprocess your data\n",
    "\n",
    "## üéØ Models\n",
    "- Traditional ML: TF-IDF + Logistic Regression/SVM\n",
    "- Deep Learning: BERT-based classifier\n",
    "\n",
    "## ‚ö° Kaggle Advantages\n",
    "- Free GPU access (Tesla P100)\n",
    "- Pre-installed ML libraries\n",
    "- Easy dataset upload\n",
    "- Community sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbf1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages if needed\n",
    "!pip install transformers torch --quiet\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Environment ready!\")\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa379cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "try:\n",
    "    df = pd.read_csv('/kaggle/input/fraud-detection-dataset/final_fraud_detection_dataset.csv')\n",
    "    print(f\"‚úÖ Dataset loaded: {len(df)} samples\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"Label distribution: {df['binary_label'].value_counts()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Dataset not found. Please upload your CSV file.\")\n",
    "    # Create sample data for demonstration\n",
    "    print(\"üìù Using sample data instead...\")\n",
    "    # [Sample data creation code here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537538ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Map labels\n",
    "df['label'] = df['binary_label'].map({1: 'fraud', 0: 'normal'})\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "print(f\"Training label distribution: {y_train.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF features: {X_train_tfidf.shape[1]}\")\n",
    "print(\"‚úÖ Text vectorization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143abe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train traditional ML models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_tfidf, y_train_encoded)\n",
    "\n",
    "# SVM\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "svm_model.fit(X_train_tfidf, y_train_encoded)\n",
    "\n",
    "print(\"‚úÖ Models trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329eb2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "models = {'Logistic Regression': lr_model, 'SVM': svm_model}\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    print(f\"\\nüîç {name} Results:\")\n",
    "    print(classification_report(y_test_encoded, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test_encoded, y_pred)\n",
    "    print(f\"Confusion Matrix:\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f8bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Training (GPU accelerated)\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    AdamW, get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FraudDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"üöÄ Initializing BERT...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcfc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare BERT datasets\n",
    "train_dataset = FraudDataset(X_train, y_train_encoded, tokenizer)\n",
    "test_dataset = FraudDataset(X_test, y_test_encoded, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Testing batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467170ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(train_loader) * 3\n",
    ")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    print(f\"\\nüöÄ Epoch {epoch + 1}/3\")\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"‚úÖ BERT training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee94c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BERT model\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label']\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        \n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.numpy())\n",
    "\n",
    "print(\"\\nüéØ BERT Evaluation Results:\")\n",
    "print(classification_report(true_labels, predictions, target_names=le.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models for download\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('/kaggle/working/models', exist_ok=True)\n",
    "\n",
    "# Save traditional ML models\n",
    "joblib.dump(lr_model, '/kaggle/working/models/logistic_regression.pkl')\n",
    "joblib.dump(svm_model, '/kaggle/working/models/svm.pkl')\n",
    "joblib.dump(tfidf, '/kaggle/working/models/tfidf_vectorizer.pkl')\n",
    "joblib.dump(le, '/kaggle/working/models/label_encoder.pkl')\n",
    "\n",
    "# Save BERT model\n",
    "model.save_pretrained('/kaggle/working/models/bert_model')\n",
    "tokenizer.save_pretrained('/kaggle/working/models/bert_tokenizer')\n",
    "\n",
    "print(\"üíæ Models saved to /kaggle/working/models/\")\n",
    "print(\"Download them from the Output tab!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb693577",
   "metadata": {},
   "source": [
    "# üìä Results Summary\n",
    "\n",
    "## üéØ Performance Comparison\n",
    "- Compare all models' F1-scores, precision, and recall\n",
    "- BERT typically performs best but requires more resources\n",
    "\n",
    "## üí° Next Steps\n",
    "1. **Download Models**: Get your trained models from the Output tab\n",
    "2. **Deploy**: Use the saved models in production\n",
    "3. **Experiment**: Try different hyperparameters\n",
    "4. **Share**: Publish your notebook to Kaggle community\n",
    "\n",
    "## ‚ö° Kaggle Tips\n",
    "- Use GPU accelerator for faster training\n",
    "- Save models regularly to avoid losing progress\n",
    "- Monitor memory usage with large datasets\n",
    "- Use the Discussion forum for questions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
