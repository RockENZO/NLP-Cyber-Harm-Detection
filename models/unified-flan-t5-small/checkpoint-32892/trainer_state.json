{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 32892,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004560379423568041,
      "grad_norm": 5191381.0,
      "learning_rate": 8.936170212765958e-06,
      "loss": 31.7092,
      "step": 50
    },
    {
      "epoch": 0.009120758847136081,
      "grad_norm": 2986101.5,
      "learning_rate": 1.8054711246200608e-05,
      "loss": 20.2918,
      "step": 100
    },
    {
      "epoch": 0.013681138270704123,
      "grad_norm": 1009308.5625,
      "learning_rate": 2.7173252279635254e-05,
      "loss": 10.1357,
      "step": 150
    },
    {
      "epoch": 0.018241517694272163,
      "grad_norm": 1024604.0,
      "learning_rate": 3.629179331306991e-05,
      "loss": 4.296,
      "step": 200
    },
    {
      "epoch": 0.022801897117840204,
      "grad_norm": 1150882.625,
      "learning_rate": 4.541033434650456e-05,
      "loss": 2.3434,
      "step": 250
    },
    {
      "epoch": 0.027362276541408246,
      "grad_norm": 917637.0,
      "learning_rate": 5.452887537993921e-05,
      "loss": 1.2138,
      "step": 300
    },
    {
      "epoch": 0.03192265596497629,
      "grad_norm": 352328.40625,
      "learning_rate": 6.364741641337386e-05,
      "loss": 0.4973,
      "step": 350
    },
    {
      "epoch": 0.036483035388544326,
      "grad_norm": 94084.1328125,
      "learning_rate": 7.276595744680851e-05,
      "loss": 0.1717,
      "step": 400
    },
    {
      "epoch": 0.04104341481211237,
      "grad_norm": 41576.3203125,
      "learning_rate": 8.188449848024316e-05,
      "loss": 0.0748,
      "step": 450
    },
    {
      "epoch": 0.04560379423568041,
      "grad_norm": 11227.4306640625,
      "learning_rate": 9.100303951367781e-05,
      "loss": 0.0403,
      "step": 500
    },
    {
      "epoch": 0.05016417365924845,
      "grad_norm": 12313.716796875,
      "learning_rate": 0.00010012158054711244,
      "loss": 0.0249,
      "step": 550
    },
    {
      "epoch": 0.05472455308281649,
      "grad_norm": 20233.6953125,
      "learning_rate": 0.0001092401215805471,
      "loss": 0.0176,
      "step": 600
    },
    {
      "epoch": 0.05928493250638453,
      "grad_norm": 15492.6396484375,
      "learning_rate": 0.00011835866261398174,
      "loss": 0.0116,
      "step": 650
    },
    {
      "epoch": 0.06384531192995258,
      "grad_norm": 7781.10986328125,
      "learning_rate": 0.0001274772036474164,
      "loss": 0.0092,
      "step": 700
    },
    {
      "epoch": 0.0684056913535206,
      "grad_norm": 5014.93408203125,
      "learning_rate": 0.00013659574468085104,
      "loss": 0.0093,
      "step": 750
    },
    {
      "epoch": 0.07296607077708865,
      "grad_norm": 10414.806640625,
      "learning_rate": 0.0001457142857142857,
      "loss": 0.0076,
      "step": 800
    },
    {
      "epoch": 0.0775264502006567,
      "grad_norm": 5240.27099609375,
      "learning_rate": 0.00015483282674772034,
      "loss": 0.0062,
      "step": 850
    },
    {
      "epoch": 0.08208682962422474,
      "grad_norm": 4882.24755859375,
      "learning_rate": 0.000163951367781155,
      "loss": 0.0062,
      "step": 900
    },
    {
      "epoch": 0.08664720904779277,
      "grad_norm": 6396.0517578125,
      "learning_rate": 0.00017306990881458964,
      "loss": 0.0056,
      "step": 950
    },
    {
      "epoch": 0.09120758847136082,
      "grad_norm": 6876.47509765625,
      "learning_rate": 0.0001821884498480243,
      "loss": 0.0052,
      "step": 1000
    },
    {
      "epoch": 0.09576796789492886,
      "grad_norm": 1745.3070068359375,
      "learning_rate": 0.00019130699088145894,
      "loss": 0.0048,
      "step": 1050
    },
    {
      "epoch": 0.1003283473184969,
      "grad_norm": 2925.928466796875,
      "learning_rate": 0.0002004255319148936,
      "loss": 0.005,
      "step": 1100
    },
    {
      "epoch": 0.10488872674206494,
      "grad_norm": 3129.099365234375,
      "learning_rate": 0.00020954407294832824,
      "loss": 0.0045,
      "step": 1150
    },
    {
      "epoch": 0.10944910616563298,
      "grad_norm": 4802.99609375,
      "learning_rate": 0.0002186626139817629,
      "loss": 0.0043,
      "step": 1200
    },
    {
      "epoch": 0.11400948558920102,
      "grad_norm": 3725.22705078125,
      "learning_rate": 0.00022778115501519757,
      "loss": 0.0038,
      "step": 1250
    },
    {
      "epoch": 0.11856986501276906,
      "grad_norm": 2246.4873046875,
      "learning_rate": 0.0002368996960486322,
      "loss": 0.0046,
      "step": 1300
    },
    {
      "epoch": 0.1231302444363371,
      "grad_norm": 1279.67822265625,
      "learning_rate": 0.00024601823708206684,
      "loss": 0.005,
      "step": 1350
    },
    {
      "epoch": 0.12769062385990515,
      "grad_norm": 4941.34375,
      "learning_rate": 0.0002551367781155015,
      "loss": 0.0042,
      "step": 1400
    },
    {
      "epoch": 0.1322510032834732,
      "grad_norm": 11591.8203125,
      "learning_rate": 0.00026425531914893614,
      "loss": 0.0044,
      "step": 1450
    },
    {
      "epoch": 0.1368113827070412,
      "grad_norm": 2826.35400390625,
      "learning_rate": 0.0002733738601823708,
      "loss": 0.0053,
      "step": 1500
    },
    {
      "epoch": 0.14137176213060926,
      "grad_norm": 4483.15283203125,
      "learning_rate": 0.00028249240121580544,
      "loss": 0.0041,
      "step": 1550
    },
    {
      "epoch": 0.1459321415541773,
      "grad_norm": 6124.10595703125,
      "learning_rate": 0.0002916109422492401,
      "loss": 0.0039,
      "step": 1600
    },
    {
      "epoch": 0.15049252097774535,
      "grad_norm": 3820.394287109375,
      "learning_rate": 0.000299961596313246,
      "loss": 0.0039,
      "step": 1650
    },
    {
      "epoch": 0.1550529004013134,
      "grad_norm": 5245.4638671875,
      "learning_rate": 0.00029948155022882197,
      "loss": 0.0036,
      "step": 1700
    },
    {
      "epoch": 0.15961327982488144,
      "grad_norm": 5578.05712890625,
      "learning_rate": 0.00029900150414439785,
      "loss": 0.0043,
      "step": 1750
    },
    {
      "epoch": 0.16417365924844948,
      "grad_norm": 18701.275390625,
      "learning_rate": 0.00029852145805997374,
      "loss": 0.0045,
      "step": 1800
    },
    {
      "epoch": 0.1687340386720175,
      "grad_norm": 2198.85107421875,
      "learning_rate": 0.0002980414119755496,
      "loss": 0.0043,
      "step": 1850
    },
    {
      "epoch": 0.17329441809558555,
      "grad_norm": 4847.12890625,
      "learning_rate": 0.0002975613658911255,
      "loss": 0.0052,
      "step": 1900
    },
    {
      "epoch": 0.1778547975191536,
      "grad_norm": 776.0476684570312,
      "learning_rate": 0.00029708131980670145,
      "loss": 0.0028,
      "step": 1950
    },
    {
      "epoch": 0.18241517694272164,
      "grad_norm": 1443.686767578125,
      "learning_rate": 0.0002966012737222773,
      "loss": 0.003,
      "step": 2000
    },
    {
      "epoch": 0.18697555636628968,
      "grad_norm": 3676.90478515625,
      "learning_rate": 0.0002961212276378532,
      "loss": 0.0039,
      "step": 2050
    },
    {
      "epoch": 0.19153593578985773,
      "grad_norm": 40.87212371826172,
      "learning_rate": 0.0002956411815534291,
      "loss": 0.0034,
      "step": 2100
    },
    {
      "epoch": 0.19609631521342577,
      "grad_norm": 6979.63818359375,
      "learning_rate": 0.000295161135469005,
      "loss": 0.0037,
      "step": 2150
    },
    {
      "epoch": 0.2006566946369938,
      "grad_norm": 3042.655517578125,
      "learning_rate": 0.0002946810893845809,
      "loss": 0.004,
      "step": 2200
    },
    {
      "epoch": 0.20521707406056183,
      "grad_norm": 11051.544921875,
      "learning_rate": 0.00029420104330015677,
      "loss": 0.0031,
      "step": 2250
    },
    {
      "epoch": 0.20977745348412988,
      "grad_norm": 1560.3939208984375,
      "learning_rate": 0.0002937209972157327,
      "loss": 0.0035,
      "step": 2300
    },
    {
      "epoch": 0.21433783290769792,
      "grad_norm": 1301.023193359375,
      "learning_rate": 0.0002932409511313086,
      "loss": 0.0027,
      "step": 2350
    },
    {
      "epoch": 0.21889821233126597,
      "grad_norm": 4049.370361328125,
      "learning_rate": 0.0002927609050468845,
      "loss": 0.0034,
      "step": 2400
    },
    {
      "epoch": 0.223458591754834,
      "grad_norm": 3727.81005859375,
      "learning_rate": 0.00029228085896246036,
      "loss": 0.0039,
      "step": 2450
    },
    {
      "epoch": 0.22801897117840203,
      "grad_norm": 23.087020874023438,
      "learning_rate": 0.00029180081287803625,
      "loss": 0.0025,
      "step": 2500
    },
    {
      "epoch": 0.23257935060197008,
      "grad_norm": 6355.607421875,
      "learning_rate": 0.0002913207667936122,
      "loss": 0.0039,
      "step": 2550
    },
    {
      "epoch": 0.23713973002553812,
      "grad_norm": 5175.6767578125,
      "learning_rate": 0.000290840720709188,
      "loss": 0.0029,
      "step": 2600
    },
    {
      "epoch": 0.24170010944910617,
      "grad_norm": 3428.44677734375,
      "learning_rate": 0.00029036067462476396,
      "loss": 0.0037,
      "step": 2650
    },
    {
      "epoch": 0.2462604888726742,
      "grad_norm": 1918.808837890625,
      "learning_rate": 0.00028988062854033985,
      "loss": 0.0038,
      "step": 2700
    },
    {
      "epoch": 0.2508208682962422,
      "grad_norm": 873.9747314453125,
      "learning_rate": 0.00028940058245591573,
      "loss": 0.0031,
      "step": 2750
    },
    {
      "epoch": 0.2553812477198103,
      "grad_norm": 894.5855712890625,
      "learning_rate": 0.0002889205363714917,
      "loss": 0.0029,
      "step": 2800
    },
    {
      "epoch": 0.2599416271433783,
      "grad_norm": 3663.469970703125,
      "learning_rate": 0.0002884404902870675,
      "loss": 0.004,
      "step": 2850
    },
    {
      "epoch": 0.2645020065669464,
      "grad_norm": 5029.2021484375,
      "learning_rate": 0.00028796044420264345,
      "loss": 0.0029,
      "step": 2900
    },
    {
      "epoch": 0.2690623859905144,
      "grad_norm": 1186.6944580078125,
      "learning_rate": 0.00028748039811821933,
      "loss": 0.0033,
      "step": 2950
    },
    {
      "epoch": 0.2736227654140824,
      "grad_norm": 1336.5592041015625,
      "learning_rate": 0.0002870003520337952,
      "loss": 0.0038,
      "step": 3000
    },
    {
      "epoch": 0.2781831448376505,
      "grad_norm": 162.01199340820312,
      "learning_rate": 0.0002865203059493711,
      "loss": 0.0036,
      "step": 3050
    },
    {
      "epoch": 0.2827435242612185,
      "grad_norm": 2847.91796875,
      "learning_rate": 0.000286040259864947,
      "loss": 0.0035,
      "step": 3100
    },
    {
      "epoch": 0.2873039036847866,
      "grad_norm": 3522.3447265625,
      "learning_rate": 0.00028556021378052293,
      "loss": 0.0036,
      "step": 3150
    },
    {
      "epoch": 0.2918642831083546,
      "grad_norm": 7830.3466796875,
      "learning_rate": 0.0002850801676960988,
      "loss": 0.0026,
      "step": 3200
    },
    {
      "epoch": 0.2964246625319227,
      "grad_norm": 8993.009765625,
      "learning_rate": 0.0002846001216116747,
      "loss": 0.0035,
      "step": 3250
    },
    {
      "epoch": 0.3009850419554907,
      "grad_norm": 4850.8564453125,
      "learning_rate": 0.0002841200755272506,
      "loss": 0.0037,
      "step": 3300
    },
    {
      "epoch": 0.3055454213790587,
      "grad_norm": 6726.22412109375,
      "learning_rate": 0.0002836400294428265,
      "loss": 0.0041,
      "step": 3350
    },
    {
      "epoch": 0.3101058008026268,
      "grad_norm": 4995.31396484375,
      "learning_rate": 0.0002831599833584024,
      "loss": 0.004,
      "step": 3400
    },
    {
      "epoch": 0.3146661802261948,
      "grad_norm": 13345.486328125,
      "learning_rate": 0.0002826799372739783,
      "loss": 0.0032,
      "step": 3450
    },
    {
      "epoch": 0.3192265596497629,
      "grad_norm": 2827.42431640625,
      "learning_rate": 0.0002821998911895542,
      "loss": 0.0025,
      "step": 3500
    },
    {
      "epoch": 0.3237869390733309,
      "grad_norm": 12352.2509765625,
      "learning_rate": 0.00028171984510513007,
      "loss": 0.0036,
      "step": 3550
    },
    {
      "epoch": 0.32834731849689897,
      "grad_norm": 6555.58349609375,
      "learning_rate": 0.00028123979902070596,
      "loss": 0.0036,
      "step": 3600
    },
    {
      "epoch": 0.332907697920467,
      "grad_norm": 5263.6025390625,
      "learning_rate": 0.00028075975293628184,
      "loss": 0.0028,
      "step": 3650
    },
    {
      "epoch": 0.337468077344035,
      "grad_norm": 4955.0693359375,
      "learning_rate": 0.00028027970685185773,
      "loss": 0.0039,
      "step": 3700
    },
    {
      "epoch": 0.3420284567676031,
      "grad_norm": 4084.581787109375,
      "learning_rate": 0.00027979966076743367,
      "loss": 0.0028,
      "step": 3750
    },
    {
      "epoch": 0.3465888361911711,
      "grad_norm": 52.824527740478516,
      "learning_rate": 0.00027931961468300956,
      "loss": 0.0024,
      "step": 3800
    },
    {
      "epoch": 0.35114921561473916,
      "grad_norm": 36.61239242553711,
      "learning_rate": 0.00027883956859858544,
      "loss": 0.004,
      "step": 3850
    },
    {
      "epoch": 0.3557095950383072,
      "grad_norm": 8609.90625,
      "learning_rate": 0.00027835952251416133,
      "loss": 0.0029,
      "step": 3900
    },
    {
      "epoch": 0.36026997446187525,
      "grad_norm": 7232.01708984375,
      "learning_rate": 0.0002778794764297372,
      "loss": 0.0029,
      "step": 3950
    },
    {
      "epoch": 0.36483035388544327,
      "grad_norm": 3892.7958984375,
      "learning_rate": 0.0002773994303453131,
      "loss": 0.0037,
      "step": 4000
    },
    {
      "epoch": 0.3693907333090113,
      "grad_norm": 7876.177734375,
      "learning_rate": 0.00027691938426088904,
      "loss": 0.0032,
      "step": 4050
    },
    {
      "epoch": 0.37395111273257936,
      "grad_norm": 111.283203125,
      "learning_rate": 0.0002764393381764649,
      "loss": 0.004,
      "step": 4100
    },
    {
      "epoch": 0.3785114921561474,
      "grad_norm": 2361.9755859375,
      "learning_rate": 0.0002759592920920408,
      "loss": 0.0044,
      "step": 4150
    },
    {
      "epoch": 0.38307187157971545,
      "grad_norm": 75.35592651367188,
      "learning_rate": 0.0002754792460076167,
      "loss": 0.003,
      "step": 4200
    },
    {
      "epoch": 0.38763225100328347,
      "grad_norm": 8587.8662109375,
      "learning_rate": 0.0002749991999231926,
      "loss": 0.0036,
      "step": 4250
    },
    {
      "epoch": 0.39219263042685154,
      "grad_norm": 9101.6474609375,
      "learning_rate": 0.0002745191538387685,
      "loss": 0.0036,
      "step": 4300
    },
    {
      "epoch": 0.39675300985041956,
      "grad_norm": 8242.1396484375,
      "learning_rate": 0.00027403910775434436,
      "loss": 0.0039,
      "step": 4350
    },
    {
      "epoch": 0.4013133892739876,
      "grad_norm": 7261.2919921875,
      "learning_rate": 0.0002735590616699203,
      "loss": 0.0036,
      "step": 4400
    },
    {
      "epoch": 0.40587376869755565,
      "grad_norm": 11546.7353515625,
      "learning_rate": 0.0002730790155854962,
      "loss": 0.0022,
      "step": 4450
    },
    {
      "epoch": 0.41043414812112367,
      "grad_norm": 13710.5146484375,
      "learning_rate": 0.00027259896950107207,
      "loss": 0.005,
      "step": 4500
    },
    {
      "epoch": 0.41499452754469174,
      "grad_norm": 10107.23828125,
      "learning_rate": 0.000272118923416648,
      "loss": 0.0037,
      "step": 4550
    },
    {
      "epoch": 0.41955490696825976,
      "grad_norm": 9945.3603515625,
      "learning_rate": 0.00027163887733222384,
      "loss": 0.0043,
      "step": 4600
    },
    {
      "epoch": 0.4241152863918278,
      "grad_norm": 223.8675079345703,
      "learning_rate": 0.0002711588312477998,
      "loss": 0.0032,
      "step": 4650
    },
    {
      "epoch": 0.42867566581539585,
      "grad_norm": 7325.44482421875,
      "learning_rate": 0.00027067878516337567,
      "loss": 0.0044,
      "step": 4700
    },
    {
      "epoch": 0.43323604523896386,
      "grad_norm": 18206.541015625,
      "learning_rate": 0.00027019873907895155,
      "loss": 0.0044,
      "step": 4750
    },
    {
      "epoch": 0.43779642466253194,
      "grad_norm": 19797.138671875,
      "learning_rate": 0.00026971869299452744,
      "loss": 0.0036,
      "step": 4800
    },
    {
      "epoch": 0.44235680408609995,
      "grad_norm": 25315.513671875,
      "learning_rate": 0.0002692386469101033,
      "loss": 0.003,
      "step": 4850
    },
    {
      "epoch": 0.446917183509668,
      "grad_norm": 71.27204132080078,
      "learning_rate": 0.00026875860082567927,
      "loss": 0.0051,
      "step": 4900
    },
    {
      "epoch": 0.45147756293323604,
      "grad_norm": 94.12933349609375,
      "learning_rate": 0.00026827855474125515,
      "loss": 0.004,
      "step": 4950
    },
    {
      "epoch": 0.45603794235680406,
      "grad_norm": 20835.52734375,
      "learning_rate": 0.00026779850865683104,
      "loss": 0.0028,
      "step": 5000
    },
    {
      "epoch": 0.46059832178037213,
      "grad_norm": 10.855692863464355,
      "learning_rate": 0.0002673184625724069,
      "loss": 0.0038,
      "step": 5050
    },
    {
      "epoch": 0.46515870120394015,
      "grad_norm": 14723.458984375,
      "learning_rate": 0.0002668384164879828,
      "loss": 0.0033,
      "step": 5100
    },
    {
      "epoch": 0.4697190806275082,
      "grad_norm": 9277.3291015625,
      "learning_rate": 0.00026635837040355875,
      "loss": 0.0042,
      "step": 5150
    },
    {
      "epoch": 0.47427946005107624,
      "grad_norm": 3981.341796875,
      "learning_rate": 0.0002658783243191346,
      "loss": 0.0055,
      "step": 5200
    },
    {
      "epoch": 0.4788398394746443,
      "grad_norm": 9875.7001953125,
      "learning_rate": 0.0002653982782347105,
      "loss": 0.0031,
      "step": 5250
    },
    {
      "epoch": 0.48340021889821233,
      "grad_norm": 0.7528746724128723,
      "learning_rate": 0.0002649182321502864,
      "loss": 0.004,
      "step": 5300
    },
    {
      "epoch": 0.48796059832178035,
      "grad_norm": 13251.9052734375,
      "learning_rate": 0.0002644381860658623,
      "loss": 0.0067,
      "step": 5350
    },
    {
      "epoch": 0.4925209777453484,
      "grad_norm": 3365.882568359375,
      "learning_rate": 0.00026395813998143823,
      "loss": 0.0028,
      "step": 5400
    },
    {
      "epoch": 0.49708135716891644,
      "grad_norm": 3.9904885292053223,
      "learning_rate": 0.00026347809389701407,
      "loss": 0.0033,
      "step": 5450
    },
    {
      "epoch": 0.5016417365924845,
      "grad_norm": 4237.01953125,
      "learning_rate": 0.00026299804781259,
      "loss": 0.0042,
      "step": 5500
    },
    {
      "epoch": 0.5062021160160526,
      "grad_norm": 0.5582995414733887,
      "learning_rate": 0.0002625180017281659,
      "loss": 0.0049,
      "step": 5550
    },
    {
      "epoch": 0.5107624954396206,
      "grad_norm": 8.926702499389648,
      "learning_rate": 0.0002620379556437418,
      "loss": 0.0051,
      "step": 5600
    },
    {
      "epoch": 0.5153228748631886,
      "grad_norm": 0.11075832694768906,
      "learning_rate": 0.00026155790955931766,
      "loss": 0.0052,
      "step": 5650
    },
    {
      "epoch": 0.5198832542867566,
      "grad_norm": 13742.435546875,
      "learning_rate": 0.00026107786347489355,
      "loss": 0.0063,
      "step": 5700
    },
    {
      "epoch": 0.5244436337103247,
      "grad_norm": 2153.0947265625,
      "learning_rate": 0.0002605978173904695,
      "loss": 0.004,
      "step": 5750
    },
    {
      "epoch": 0.5290040131338928,
      "grad_norm": 9385.3115234375,
      "learning_rate": 0.0002601177713060454,
      "loss": 0.0042,
      "step": 5800
    },
    {
      "epoch": 0.5335643925574608,
      "grad_norm": 6672.72509765625,
      "learning_rate": 0.00025963772522162126,
      "loss": 0.0043,
      "step": 5850
    },
    {
      "epoch": 0.5381247719810288,
      "grad_norm": 9525.65234375,
      "learning_rate": 0.00025915767913719715,
      "loss": 0.0053,
      "step": 5900
    },
    {
      "epoch": 0.5426851514045968,
      "grad_norm": 232.60498046875,
      "learning_rate": 0.00025867763305277303,
      "loss": 0.0037,
      "step": 5950
    },
    {
      "epoch": 0.5472455308281649,
      "grad_norm": 17895.58984375,
      "learning_rate": 0.0002581975869683489,
      "loss": 0.0049,
      "step": 6000
    },
    {
      "epoch": 0.551805910251733,
      "grad_norm": 12737.7802734375,
      "learning_rate": 0.00025771754088392486,
      "loss": 0.0059,
      "step": 6050
    },
    {
      "epoch": 0.556366289675301,
      "grad_norm": 18965.5,
      "learning_rate": 0.00025723749479950075,
      "loss": 0.0043,
      "step": 6100
    },
    {
      "epoch": 0.560926669098869,
      "grad_norm": 4996.62646484375,
      "learning_rate": 0.00025675744871507663,
      "loss": 0.0042,
      "step": 6150
    },
    {
      "epoch": 0.565487048522437,
      "grad_norm": 13.310291290283203,
      "learning_rate": 0.0002562774026306525,
      "loss": 0.0027,
      "step": 6200
    },
    {
      "epoch": 0.5700474279460052,
      "grad_norm": 8628.5615234375,
      "learning_rate": 0.0002557973565462284,
      "loss": 0.0035,
      "step": 6250
    },
    {
      "epoch": 0.5746078073695732,
      "grad_norm": 19231.8046875,
      "learning_rate": 0.0002553173104618043,
      "loss": 0.0032,
      "step": 6300
    },
    {
      "epoch": 0.5791681867931412,
      "grad_norm": 18334.12109375,
      "learning_rate": 0.0002548372643773802,
      "loss": 0.0047,
      "step": 6350
    },
    {
      "epoch": 0.5837285662167092,
      "grad_norm": 31634.16796875,
      "learning_rate": 0.0002543572182929561,
      "loss": 0.0048,
      "step": 6400
    },
    {
      "epoch": 0.5882889456402772,
      "grad_norm": 0.27193963527679443,
      "learning_rate": 0.000253877172208532,
      "loss": 0.0037,
      "step": 6450
    },
    {
      "epoch": 0.5928493250638454,
      "grad_norm": 5134.4482421875,
      "learning_rate": 0.0002533971261241079,
      "loss": 0.0049,
      "step": 6500
    },
    {
      "epoch": 0.5974097044874134,
      "grad_norm": 10275.345703125,
      "learning_rate": 0.0002529170800396838,
      "loss": 0.0057,
      "step": 6550
    },
    {
      "epoch": 0.6019700839109814,
      "grad_norm": 13363.2197265625,
      "learning_rate": 0.00025243703395525966,
      "loss": 0.005,
      "step": 6600
    },
    {
      "epoch": 0.6065304633345494,
      "grad_norm": 993.8096923828125,
      "learning_rate": 0.0002519569878708356,
      "loss": 0.0039,
      "step": 6650
    },
    {
      "epoch": 0.6110908427581174,
      "grad_norm": 5708.0478515625,
      "learning_rate": 0.0002514769417864115,
      "loss": 0.0037,
      "step": 6700
    },
    {
      "epoch": 0.6156512221816856,
      "grad_norm": 7.295849800109863,
      "learning_rate": 0.0002509968957019874,
      "loss": 0.0059,
      "step": 6750
    },
    {
      "epoch": 0.6202116016052536,
      "grad_norm": 10413.23046875,
      "learning_rate": 0.00025051684961756326,
      "loss": 0.0036,
      "step": 6800
    },
    {
      "epoch": 0.6247719810288216,
      "grad_norm": 337.8074645996094,
      "learning_rate": 0.00025003680353313914,
      "loss": 0.0042,
      "step": 6850
    },
    {
      "epoch": 0.6293323604523896,
      "grad_norm": 5516.873046875,
      "learning_rate": 0.0002495567574487151,
      "loss": 0.0042,
      "step": 6900
    },
    {
      "epoch": 0.6338927398759577,
      "grad_norm": 11528.994140625,
      "learning_rate": 0.0002490767113642909,
      "loss": 0.0058,
      "step": 6950
    },
    {
      "epoch": 0.6384531192995258,
      "grad_norm": 9465.6904296875,
      "learning_rate": 0.00024859666527986686,
      "loss": 0.0044,
      "step": 7000
    },
    {
      "epoch": 0.6430134987230938,
      "grad_norm": 6091.9970703125,
      "learning_rate": 0.00024811661919544274,
      "loss": 0.0041,
      "step": 7050
    },
    {
      "epoch": 0.6475738781466618,
      "grad_norm": 2880.179931640625,
      "learning_rate": 0.00024763657311101863,
      "loss": 0.0035,
      "step": 7100
    },
    {
      "epoch": 0.6521342575702298,
      "grad_norm": 14479.642578125,
      "learning_rate": 0.0002471565270265945,
      "loss": 0.0038,
      "step": 7150
    },
    {
      "epoch": 0.6566946369937979,
      "grad_norm": 20063.322265625,
      "learning_rate": 0.0002466764809421704,
      "loss": 0.0039,
      "step": 7200
    },
    {
      "epoch": 0.661255016417366,
      "grad_norm": 14122.59375,
      "learning_rate": 0.00024619643485774634,
      "loss": 0.005,
      "step": 7250
    },
    {
      "epoch": 0.665815395840934,
      "grad_norm": 0.047965772449970245,
      "learning_rate": 0.00024571638877332223,
      "loss": 0.0045,
      "step": 7300
    },
    {
      "epoch": 0.670375775264502,
      "grad_norm": 7790.6357421875,
      "learning_rate": 0.0002452363426888981,
      "loss": 0.0038,
      "step": 7350
    },
    {
      "epoch": 0.67493615468807,
      "grad_norm": 8388.1767578125,
      "learning_rate": 0.000244756296604474,
      "loss": 0.004,
      "step": 7400
    },
    {
      "epoch": 0.6794965341116381,
      "grad_norm": 39.791282653808594,
      "learning_rate": 0.0002442762505200499,
      "loss": 0.0033,
      "step": 7450
    },
    {
      "epoch": 0.6840569135352061,
      "grad_norm": 19.19691276550293,
      "learning_rate": 0.0002437962044356258,
      "loss": 0.0043,
      "step": 7500
    },
    {
      "epoch": 0.6886172929587742,
      "grad_norm": 0.6072261333465576,
      "learning_rate": 0.00024331615835120168,
      "loss": 0.0039,
      "step": 7550
    },
    {
      "epoch": 0.6931776723823422,
      "grad_norm": 12369.353515625,
      "learning_rate": 0.0002428361122667776,
      "loss": 0.0047,
      "step": 7600
    },
    {
      "epoch": 0.6977380518059102,
      "grad_norm": 0.123549684882164,
      "learning_rate": 0.00024235606618235346,
      "loss": 0.0043,
      "step": 7650
    },
    {
      "epoch": 0.7022984312294783,
      "grad_norm": 830.545654296875,
      "learning_rate": 0.00024187602009792937,
      "loss": 0.0048,
      "step": 7700
    },
    {
      "epoch": 0.7068588106530463,
      "grad_norm": 7564.1455078125,
      "learning_rate": 0.00024139597401350528,
      "loss": 0.0052,
      "step": 7750
    },
    {
      "epoch": 0.7114191900766144,
      "grad_norm": 7156.69873046875,
      "learning_rate": 0.00024091592792908117,
      "loss": 0.0044,
      "step": 7800
    },
    {
      "epoch": 0.7159795695001824,
      "grad_norm": 2605.3271484375,
      "learning_rate": 0.00024043588184465708,
      "loss": 0.0036,
      "step": 7850
    },
    {
      "epoch": 0.7205399489237505,
      "grad_norm": 506.2583312988281,
      "learning_rate": 0.00023995583576023294,
      "loss": 0.0045,
      "step": 7900
    },
    {
      "epoch": 0.7251003283473185,
      "grad_norm": 189.6920166015625,
      "learning_rate": 0.00023947578967580885,
      "loss": 0.0043,
      "step": 7950
    },
    {
      "epoch": 0.7296607077708865,
      "grad_norm": 8482.74609375,
      "learning_rate": 0.00023899574359138474,
      "loss": 0.0041,
      "step": 8000
    },
    {
      "epoch": 0.7342210871944546,
      "grad_norm": 0.17709635198116302,
      "learning_rate": 0.00023851569750696065,
      "loss": 0.0047,
      "step": 8050
    },
    {
      "epoch": 0.7387814666180226,
      "grad_norm": 5972.11865234375,
      "learning_rate": 0.00023803565142253657,
      "loss": 0.0042,
      "step": 8100
    },
    {
      "epoch": 0.7433418460415907,
      "grad_norm": 8571.2431640625,
      "learning_rate": 0.00023755560533811242,
      "loss": 0.0026,
      "step": 8150
    },
    {
      "epoch": 0.7479022254651587,
      "grad_norm": 25.185789108276367,
      "learning_rate": 0.00023707555925368834,
      "loss": 0.0037,
      "step": 8200
    },
    {
      "epoch": 0.7524626048887267,
      "grad_norm": 11149.662109375,
      "learning_rate": 0.00023659551316926422,
      "loss": 0.0048,
      "step": 8250
    },
    {
      "epoch": 0.7570229843122948,
      "grad_norm": 46.00790023803711,
      "learning_rate": 0.00023611546708484014,
      "loss": 0.0032,
      "step": 8300
    },
    {
      "epoch": 0.7615833637358628,
      "grad_norm": 7713.6103515625,
      "learning_rate": 0.00023563542100041602,
      "loss": 0.0048,
      "step": 8350
    },
    {
      "epoch": 0.7661437431594309,
      "grad_norm": 0.11827582120895386,
      "learning_rate": 0.0002351553749159919,
      "loss": 0.0033,
      "step": 8400
    },
    {
      "epoch": 0.7707041225829989,
      "grad_norm": 3009.319091796875,
      "learning_rate": 0.00023467532883156782,
      "loss": 0.0056,
      "step": 8450
    },
    {
      "epoch": 0.7752645020065669,
      "grad_norm": 4807.74755859375,
      "learning_rate": 0.0002341952827471437,
      "loss": 0.0043,
      "step": 8500
    },
    {
      "epoch": 0.779824881430135,
      "grad_norm": 21775.80078125,
      "learning_rate": 0.0002337152366627196,
      "loss": 0.0062,
      "step": 8550
    },
    {
      "epoch": 0.7843852608537031,
      "grad_norm": 15864.9794921875,
      "learning_rate": 0.00023323519057829548,
      "loss": 0.0033,
      "step": 8600
    },
    {
      "epoch": 0.7889456402772711,
      "grad_norm": 2254.37646484375,
      "learning_rate": 0.0002327551444938714,
      "loss": 0.0026,
      "step": 8650
    },
    {
      "epoch": 0.7935060197008391,
      "grad_norm": 37.41449737548828,
      "learning_rate": 0.0002322750984094473,
      "loss": 0.0051,
      "step": 8700
    },
    {
      "epoch": 0.7980663991244071,
      "grad_norm": 16913.279296875,
      "learning_rate": 0.00023179505232502317,
      "loss": 0.0043,
      "step": 8750
    },
    {
      "epoch": 0.8026267785479752,
      "grad_norm": 47.316707611083984,
      "learning_rate": 0.00023131500624059908,
      "loss": 0.0046,
      "step": 8800
    },
    {
      "epoch": 0.8071871579715433,
      "grad_norm": 0.2803684175014496,
      "learning_rate": 0.00023083496015617496,
      "loss": 0.0045,
      "step": 8850
    },
    {
      "epoch": 0.8117475373951113,
      "grad_norm": 13520.3037109375,
      "learning_rate": 0.00023035491407175088,
      "loss": 0.0045,
      "step": 8900
    },
    {
      "epoch": 0.8163079168186793,
      "grad_norm": 180.1205291748047,
      "learning_rate": 0.00022987486798732674,
      "loss": 0.0038,
      "step": 8950
    },
    {
      "epoch": 0.8208682962422473,
      "grad_norm": 4441.60546875,
      "learning_rate": 0.00022939482190290265,
      "loss": 0.0038,
      "step": 9000
    },
    {
      "epoch": 0.8254286756658153,
      "grad_norm": 0.14862088859081268,
      "learning_rate": 0.00022891477581847856,
      "loss": 0.0039,
      "step": 9050
    },
    {
      "epoch": 0.8299890550893835,
      "grad_norm": 0.36453700065612793,
      "learning_rate": 0.00022843472973405445,
      "loss": 0.0039,
      "step": 9100
    },
    {
      "epoch": 0.8345494345129515,
      "grad_norm": 0.2301269918680191,
      "learning_rate": 0.00022795468364963036,
      "loss": 0.0045,
      "step": 9150
    },
    {
      "epoch": 0.8391098139365195,
      "grad_norm": 3981.968017578125,
      "learning_rate": 0.00022747463756520622,
      "loss": 0.0056,
      "step": 9200
    },
    {
      "epoch": 0.8436701933600875,
      "grad_norm": 0.1579790860414505,
      "learning_rate": 0.00022699459148078213,
      "loss": 0.0033,
      "step": 9250
    },
    {
      "epoch": 0.8482305727836555,
      "grad_norm": 0.4207400381565094,
      "learning_rate": 0.00022651454539635802,
      "loss": 0.003,
      "step": 9300
    },
    {
      "epoch": 0.8527909522072237,
      "grad_norm": 16288.947265625,
      "learning_rate": 0.00022603449931193393,
      "loss": 0.005,
      "step": 9350
    },
    {
      "epoch": 0.8573513316307917,
      "grad_norm": 15083.19140625,
      "learning_rate": 0.00022555445322750985,
      "loss": 0.0036,
      "step": 9400
    },
    {
      "epoch": 0.8619117110543597,
      "grad_norm": 5.15146017074585,
      "learning_rate": 0.0002250744071430857,
      "loss": 0.0042,
      "step": 9450
    },
    {
      "epoch": 0.8664720904779277,
      "grad_norm": 0.22516004741191864,
      "learning_rate": 0.00022459436105866162,
      "loss": 0.0057,
      "step": 9500
    },
    {
      "epoch": 0.8710324699014959,
      "grad_norm": 0.6659408211708069,
      "learning_rate": 0.0002241143149742375,
      "loss": 0.0034,
      "step": 9550
    },
    {
      "epoch": 0.8755928493250639,
      "grad_norm": 5.212887287139893,
      "learning_rate": 0.00022363426888981342,
      "loss": 0.0036,
      "step": 9600
    },
    {
      "epoch": 0.8801532287486319,
      "grad_norm": 667.3731079101562,
      "learning_rate": 0.00022315422280538928,
      "loss": 0.0039,
      "step": 9650
    },
    {
      "epoch": 0.8847136081721999,
      "grad_norm": 0.7396458387374878,
      "learning_rate": 0.0002226741767209652,
      "loss": 0.0055,
      "step": 9700
    },
    {
      "epoch": 0.8892739875957679,
      "grad_norm": 5766.755859375,
      "learning_rate": 0.0002221941306365411,
      "loss": 0.0047,
      "step": 9750
    },
    {
      "epoch": 0.893834367019336,
      "grad_norm": 524.7015991210938,
      "learning_rate": 0.000221714084552117,
      "loss": 0.0041,
      "step": 9800
    },
    {
      "epoch": 0.8983947464429041,
      "grad_norm": 1570.28515625,
      "learning_rate": 0.00022123403846769287,
      "loss": 0.0065,
      "step": 9850
    },
    {
      "epoch": 0.9029551258664721,
      "grad_norm": 24235.71484375,
      "learning_rate": 0.00022075399238326876,
      "loss": 0.0035,
      "step": 9900
    },
    {
      "epoch": 0.9075155052900401,
      "grad_norm": 266.35626220703125,
      "learning_rate": 0.00022027394629884467,
      "loss": 0.0037,
      "step": 9950
    },
    {
      "epoch": 0.9120758847136081,
      "grad_norm": 15455.5244140625,
      "learning_rate": 0.00021979390021442056,
      "loss": 0.0048,
      "step": 10000
    },
    {
      "epoch": 0.9166362641371762,
      "grad_norm": 6553.109375,
      "learning_rate": 0.00021931385412999645,
      "loss": 0.0045,
      "step": 10050
    },
    {
      "epoch": 0.9211966435607443,
      "grad_norm": 25130.296875,
      "learning_rate": 0.00021883380804557236,
      "loss": 0.0038,
      "step": 10100
    },
    {
      "epoch": 0.9257570229843123,
      "grad_norm": 204.2277374267578,
      "learning_rate": 0.00021835376196114824,
      "loss": 0.0038,
      "step": 10150
    },
    {
      "epoch": 0.9303174024078803,
      "grad_norm": 27197.99609375,
      "learning_rate": 0.00021787371587672416,
      "loss": 0.0047,
      "step": 10200
    },
    {
      "epoch": 0.9348777818314484,
      "grad_norm": 5847.94873046875,
      "learning_rate": 0.00021739366979230002,
      "loss": 0.0051,
      "step": 10250
    },
    {
      "epoch": 0.9394381612550164,
      "grad_norm": 21443.193359375,
      "learning_rate": 0.00021691362370787593,
      "loss": 0.0023,
      "step": 10300
    },
    {
      "epoch": 0.9439985406785845,
      "grad_norm": 26163.48828125,
      "learning_rate": 0.00021643357762345184,
      "loss": 0.004,
      "step": 10350
    },
    {
      "epoch": 0.9485589201021525,
      "grad_norm": 3516.400390625,
      "learning_rate": 0.00021595353153902773,
      "loss": 0.0046,
      "step": 10400
    },
    {
      "epoch": 0.9531192995257205,
      "grad_norm": 1693.5291748046875,
      "learning_rate": 0.00021547348545460364,
      "loss": 0.0037,
      "step": 10450
    },
    {
      "epoch": 0.9576796789492886,
      "grad_norm": 19739.90234375,
      "learning_rate": 0.0002149934393701795,
      "loss": 0.003,
      "step": 10500
    },
    {
      "epoch": 0.9622400583728566,
      "grad_norm": 16610.03515625,
      "learning_rate": 0.00021451339328575541,
      "loss": 0.0044,
      "step": 10550
    },
    {
      "epoch": 0.9668004377964247,
      "grad_norm": 5892.9306640625,
      "learning_rate": 0.0002140333472013313,
      "loss": 0.0045,
      "step": 10600
    },
    {
      "epoch": 0.9713608172199927,
      "grad_norm": 19880.96484375,
      "learning_rate": 0.0002135533011169072,
      "loss": 0.0038,
      "step": 10650
    },
    {
      "epoch": 0.9759211966435607,
      "grad_norm": 3581.54638671875,
      "learning_rate": 0.00021307325503248313,
      "loss": 0.0027,
      "step": 10700
    },
    {
      "epoch": 0.9804815760671288,
      "grad_norm": 10603.330078125,
      "learning_rate": 0.00021259320894805898,
      "loss": 0.0037,
      "step": 10750
    },
    {
      "epoch": 0.9850419554906968,
      "grad_norm": 0.20070327818393707,
      "learning_rate": 0.0002121131628636349,
      "loss": 0.005,
      "step": 10800
    },
    {
      "epoch": 0.9896023349142649,
      "grad_norm": 13806.07421875,
      "learning_rate": 0.00021163311677921078,
      "loss": 0.0059,
      "step": 10850
    },
    {
      "epoch": 0.9941627143378329,
      "grad_norm": 2716.009521484375,
      "learning_rate": 0.0002111530706947867,
      "loss": 0.0043,
      "step": 10900
    },
    {
      "epoch": 0.998723093761401,
      "grad_norm": 32567.5234375,
      "learning_rate": 0.00021067302461036256,
      "loss": 0.0036,
      "step": 10950
    },
    {
      "epoch": 1.003283473184969,
      "grad_norm": 0.3876640200614929,
      "learning_rate": 0.00021019297852593847,
      "loss": 0.0027,
      "step": 11000
    },
    {
      "epoch": 1.007843852608537,
      "grad_norm": 3878.865234375,
      "learning_rate": 0.00020971293244151438,
      "loss": 0.0027,
      "step": 11050
    },
    {
      "epoch": 1.0124042320321052,
      "grad_norm": 0.08259088546037674,
      "learning_rate": 0.00020923288635709027,
      "loss": 0.0021,
      "step": 11100
    },
    {
      "epoch": 1.0169646114556732,
      "grad_norm": 0.36752867698669434,
      "learning_rate": 0.00020875284027266615,
      "loss": 0.0036,
      "step": 11150
    },
    {
      "epoch": 1.0215249908792412,
      "grad_norm": 4521.21142578125,
      "learning_rate": 0.00020827279418824204,
      "loss": 0.0037,
      "step": 11200
    },
    {
      "epoch": 1.0260853703028092,
      "grad_norm": 0.05947845056653023,
      "learning_rate": 0.00020779274810381795,
      "loss": 0.0022,
      "step": 11250
    },
    {
      "epoch": 1.0306457497263772,
      "grad_norm": 18651.173828125,
      "learning_rate": 0.00020731270201939384,
      "loss": 0.0048,
      "step": 11300
    },
    {
      "epoch": 1.0352061291499453,
      "grad_norm": 296.7940368652344,
      "learning_rate": 0.00020683265593496973,
      "loss": 0.0027,
      "step": 11350
    },
    {
      "epoch": 1.0397665085735133,
      "grad_norm": 5519.53369140625,
      "learning_rate": 0.00020635260985054564,
      "loss": 0.0041,
      "step": 11400
    },
    {
      "epoch": 1.0443268879970813,
      "grad_norm": 0.01650325395166874,
      "learning_rate": 0.00020587256376612152,
      "loss": 0.0024,
      "step": 11450
    },
    {
      "epoch": 1.0488872674206493,
      "grad_norm": 2.1907479763031006,
      "learning_rate": 0.00020539251768169744,
      "loss": 0.0045,
      "step": 11500
    },
    {
      "epoch": 1.0534476468442175,
      "grad_norm": 28325.482421875,
      "learning_rate": 0.0002049124715972733,
      "loss": 0.0035,
      "step": 11550
    },
    {
      "epoch": 1.0580080262677856,
      "grad_norm": 21161.70703125,
      "learning_rate": 0.0002044324255128492,
      "loss": 0.0051,
      "step": 11600
    },
    {
      "epoch": 1.0625684056913536,
      "grad_norm": 2.255159854888916,
      "learning_rate": 0.0002039523794284251,
      "loss": 0.0036,
      "step": 11650
    },
    {
      "epoch": 1.0671287851149216,
      "grad_norm": 50875.046875,
      "learning_rate": 0.000203472333344001,
      "loss": 0.0033,
      "step": 11700
    },
    {
      "epoch": 1.0716891645384896,
      "grad_norm": 409.81341552734375,
      "learning_rate": 0.00020299228725957692,
      "loss": 0.0034,
      "step": 11750
    },
    {
      "epoch": 1.0762495439620576,
      "grad_norm": 0.8593921065330505,
      "learning_rate": 0.00020251224117515278,
      "loss": 0.0027,
      "step": 11800
    },
    {
      "epoch": 1.0808099233856256,
      "grad_norm": 6495.0283203125,
      "learning_rate": 0.0002020321950907287,
      "loss": 0.0021,
      "step": 11850
    },
    {
      "epoch": 1.0853703028091937,
      "grad_norm": 20630.86328125,
      "learning_rate": 0.00020155214900630458,
      "loss": 0.0027,
      "step": 11900
    },
    {
      "epoch": 1.0899306822327617,
      "grad_norm": 0.11792295426130295,
      "learning_rate": 0.0002010721029218805,
      "loss": 0.0037,
      "step": 11950
    },
    {
      "epoch": 1.09449106165633,
      "grad_norm": 27092.080078125,
      "learning_rate": 0.0002005920568374564,
      "loss": 0.0032,
      "step": 12000
    },
    {
      "epoch": 1.099051441079898,
      "grad_norm": 32605.34375,
      "learning_rate": 0.00020011201075303226,
      "loss": 0.0033,
      "step": 12050
    },
    {
      "epoch": 1.103611820503466,
      "grad_norm": 6557.986328125,
      "learning_rate": 0.00019963196466860818,
      "loss": 0.003,
      "step": 12100
    },
    {
      "epoch": 1.108172199927034,
      "grad_norm": 5305.4091796875,
      "learning_rate": 0.00019915191858418406,
      "loss": 0.0038,
      "step": 12150
    },
    {
      "epoch": 1.112732579350602,
      "grad_norm": 57.02448272705078,
      "learning_rate": 0.00019867187249975998,
      "loss": 0.0043,
      "step": 12200
    },
    {
      "epoch": 1.11729295877417,
      "grad_norm": 0.2315942645072937,
      "learning_rate": 0.00019819182641533584,
      "loss": 0.0025,
      "step": 12250
    },
    {
      "epoch": 1.121853338197738,
      "grad_norm": 0.05439047887921333,
      "learning_rate": 0.00019771178033091175,
      "loss": 0.0037,
      "step": 12300
    },
    {
      "epoch": 1.126413717621306,
      "grad_norm": 6.691662311553955,
      "learning_rate": 0.00019723173424648766,
      "loss": 0.0041,
      "step": 12350
    },
    {
      "epoch": 1.130974097044874,
      "grad_norm": 5.269130229949951,
      "learning_rate": 0.00019675168816206355,
      "loss": 0.003,
      "step": 12400
    },
    {
      "epoch": 1.135534476468442,
      "grad_norm": 2.628464937210083,
      "learning_rate": 0.00019627164207763943,
      "loss": 0.0035,
      "step": 12450
    },
    {
      "epoch": 1.14009485589201,
      "grad_norm": 56568.78515625,
      "learning_rate": 0.00019579159599321532,
      "loss": 0.0028,
      "step": 12500
    },
    {
      "epoch": 1.1446552353155783,
      "grad_norm": 0.0697721540927887,
      "learning_rate": 0.00019531154990879123,
      "loss": 0.0015,
      "step": 12550
    },
    {
      "epoch": 1.1492156147391464,
      "grad_norm": 0.8621010184288025,
      "learning_rate": 0.00019483150382436712,
      "loss": 0.0043,
      "step": 12600
    },
    {
      "epoch": 1.1537759941627144,
      "grad_norm": 0.015488985925912857,
      "learning_rate": 0.000194351457739943,
      "loss": 0.0033,
      "step": 12650
    },
    {
      "epoch": 1.1583363735862824,
      "grad_norm": 17.353872299194336,
      "learning_rate": 0.00019387141165551892,
      "loss": 0.0051,
      "step": 12700
    },
    {
      "epoch": 1.1628967530098504,
      "grad_norm": 0.30894264578819275,
      "learning_rate": 0.0001933913655710948,
      "loss": 0.0021,
      "step": 12750
    },
    {
      "epoch": 1.1674571324334184,
      "grad_norm": 23046.12109375,
      "learning_rate": 0.00019291131948667072,
      "loss": 0.0041,
      "step": 12800
    },
    {
      "epoch": 1.1720175118569864,
      "grad_norm": 0.33786970376968384,
      "learning_rate": 0.00019243127340224658,
      "loss": 0.0042,
      "step": 12850
    },
    {
      "epoch": 1.1765778912805545,
      "grad_norm": 0.11727647483348846,
      "learning_rate": 0.0001919512273178225,
      "loss": 0.0033,
      "step": 12900
    },
    {
      "epoch": 1.1811382707041225,
      "grad_norm": 0.060193415731191635,
      "learning_rate": 0.00019147118123339838,
      "loss": 0.0033,
      "step": 12950
    },
    {
      "epoch": 1.1856986501276907,
      "grad_norm": 0.5368185043334961,
      "learning_rate": 0.0001909911351489743,
      "loss": 0.0019,
      "step": 13000
    },
    {
      "epoch": 1.1902590295512587,
      "grad_norm": 0.5243939161300659,
      "learning_rate": 0.0001905110890645502,
      "loss": 0.0051,
      "step": 13050
    },
    {
      "epoch": 1.1948194089748267,
      "grad_norm": 2681.46240234375,
      "learning_rate": 0.00019003104298012606,
      "loss": 0.0029,
      "step": 13100
    },
    {
      "epoch": 1.1993797883983948,
      "grad_norm": 17223.119140625,
      "learning_rate": 0.00018955099689570197,
      "loss": 0.0029,
      "step": 13150
    },
    {
      "epoch": 1.2039401678219628,
      "grad_norm": 1.8947744369506836,
      "learning_rate": 0.00018907095081127786,
      "loss": 0.0031,
      "step": 13200
    },
    {
      "epoch": 1.2085005472455308,
      "grad_norm": 15059.07421875,
      "learning_rate": 0.00018859090472685377,
      "loss": 0.0044,
      "step": 13250
    },
    {
      "epoch": 1.2130609266690988,
      "grad_norm": 0.03491253778338432,
      "learning_rate": 0.00018811085864242963,
      "loss": 0.0031,
      "step": 13300
    },
    {
      "epoch": 1.2176213060926668,
      "grad_norm": 385.53314208984375,
      "learning_rate": 0.00018763081255800554,
      "loss": 0.004,
      "step": 13350
    },
    {
      "epoch": 1.2221816855162349,
      "grad_norm": 0.25259941816329956,
      "learning_rate": 0.00018715076647358146,
      "loss": 0.0034,
      "step": 13400
    },
    {
      "epoch": 1.226742064939803,
      "grad_norm": 0.6439884901046753,
      "learning_rate": 0.00018667072038915734,
      "loss": 0.0023,
      "step": 13450
    },
    {
      "epoch": 1.231302444363371,
      "grad_norm": 0.5103828310966492,
      "learning_rate": 0.00018619067430473326,
      "loss": 0.002,
      "step": 13500
    },
    {
      "epoch": 1.2358628237869391,
      "grad_norm": 4278.16259765625,
      "learning_rate": 0.00018571062822030912,
      "loss": 0.005,
      "step": 13550
    },
    {
      "epoch": 1.2404232032105071,
      "grad_norm": 7101.34375,
      "learning_rate": 0.00018523058213588503,
      "loss": 0.003,
      "step": 13600
    },
    {
      "epoch": 1.2449835826340752,
      "grad_norm": 0.0789095088839531,
      "learning_rate": 0.00018475053605146094,
      "loss": 0.0038,
      "step": 13650
    },
    {
      "epoch": 1.2495439620576432,
      "grad_norm": 4.674648761749268,
      "learning_rate": 0.00018427048996703683,
      "loss": 0.0035,
      "step": 13700
    },
    {
      "epoch": 1.2541043414812112,
      "grad_norm": 238.70126342773438,
      "learning_rate": 0.00018379044388261271,
      "loss": 0.0025,
      "step": 13750
    },
    {
      "epoch": 1.2586647209047792,
      "grad_norm": 19936.51953125,
      "learning_rate": 0.0001833103977981886,
      "loss": 0.0032,
      "step": 13800
    },
    {
      "epoch": 1.2632251003283472,
      "grad_norm": 0.05251063406467438,
      "learning_rate": 0.0001828303517137645,
      "loss": 0.0029,
      "step": 13850
    },
    {
      "epoch": 1.2677854797519155,
      "grad_norm": 9255.0849609375,
      "learning_rate": 0.0001823503056293404,
      "loss": 0.0036,
      "step": 13900
    },
    {
      "epoch": 1.2723458591754833,
      "grad_norm": 0.7503611445426941,
      "learning_rate": 0.00018187025954491629,
      "loss": 0.003,
      "step": 13950
    },
    {
      "epoch": 1.2769062385990515,
      "grad_norm": 3907.708251953125,
      "learning_rate": 0.0001813902134604922,
      "loss": 0.0032,
      "step": 14000
    },
    {
      "epoch": 1.2814666180226195,
      "grad_norm": 6162.57568359375,
      "learning_rate": 0.00018091016737606808,
      "loss": 0.0048,
      "step": 14050
    },
    {
      "epoch": 1.2860269974461875,
      "grad_norm": 0.07699794322252274,
      "learning_rate": 0.000180430121291644,
      "loss": 0.0035,
      "step": 14100
    },
    {
      "epoch": 1.2905873768697556,
      "grad_norm": 23825.8671875,
      "learning_rate": 0.00017995007520721986,
      "loss": 0.0043,
      "step": 14150
    },
    {
      "epoch": 1.2951477562933236,
      "grad_norm": 1476.234619140625,
      "learning_rate": 0.00017947002912279577,
      "loss": 0.0035,
      "step": 14200
    },
    {
      "epoch": 1.2997081357168916,
      "grad_norm": 0.16046550869941711,
      "learning_rate": 0.00017898998303837166,
      "loss": 0.0021,
      "step": 14250
    },
    {
      "epoch": 1.3042685151404596,
      "grad_norm": 0.6982142925262451,
      "learning_rate": 0.00017850993695394757,
      "loss": 0.0031,
      "step": 14300
    },
    {
      "epoch": 1.3088288945640278,
      "grad_norm": 170.8339080810547,
      "learning_rate": 0.00017802989086952348,
      "loss": 0.004,
      "step": 14350
    },
    {
      "epoch": 1.3133892739875956,
      "grad_norm": 0.8098143935203552,
      "learning_rate": 0.00017754984478509934,
      "loss": 0.0031,
      "step": 14400
    },
    {
      "epoch": 1.3179496534111639,
      "grad_norm": 4232.1572265625,
      "learning_rate": 0.00017706979870067525,
      "loss": 0.0034,
      "step": 14450
    },
    {
      "epoch": 1.322510032834732,
      "grad_norm": 0.1110318973660469,
      "learning_rate": 0.00017658975261625114,
      "loss": 0.0036,
      "step": 14500
    },
    {
      "epoch": 1.3270704122583,
      "grad_norm": 0.04229433089494705,
      "learning_rate": 0.00017610970653182705,
      "loss": 0.0028,
      "step": 14550
    },
    {
      "epoch": 1.331630791681868,
      "grad_norm": 0.2250213623046875,
      "learning_rate": 0.0001756296604474029,
      "loss": 0.0034,
      "step": 14600
    },
    {
      "epoch": 1.336191171105436,
      "grad_norm": 0.026883434504270554,
      "learning_rate": 0.00017514961436297882,
      "loss": 0.0027,
      "step": 14650
    },
    {
      "epoch": 1.340751550529004,
      "grad_norm": 7930.41015625,
      "learning_rate": 0.00017466956827855474,
      "loss": 0.0045,
      "step": 14700
    },
    {
      "epoch": 1.345311929952572,
      "grad_norm": 2728.629638671875,
      "learning_rate": 0.00017418952219413062,
      "loss": 0.004,
      "step": 14750
    },
    {
      "epoch": 1.3498723093761402,
      "grad_norm": 8508.71875,
      "learning_rate": 0.00017370947610970654,
      "loss": 0.0023,
      "step": 14800
    },
    {
      "epoch": 1.354432688799708,
      "grad_norm": 38566.49609375,
      "learning_rate": 0.0001732294300252824,
      "loss": 0.0041,
      "step": 14850
    },
    {
      "epoch": 1.3589930682232763,
      "grad_norm": 2.113966703414917,
      "learning_rate": 0.0001727493839408583,
      "loss": 0.0032,
      "step": 14900
    },
    {
      "epoch": 1.3635534476468443,
      "grad_norm": 0.562562108039856,
      "learning_rate": 0.0001722693378564342,
      "loss": 0.0052,
      "step": 14950
    },
    {
      "epoch": 1.3681138270704123,
      "grad_norm": 4245.60791015625,
      "learning_rate": 0.0001717892917720101,
      "loss": 0.0032,
      "step": 15000
    },
    {
      "epoch": 1.3726742064939803,
      "grad_norm": 28937.55859375,
      "learning_rate": 0.000171309245687586,
      "loss": 0.0035,
      "step": 15050
    },
    {
      "epoch": 1.3772345859175483,
      "grad_norm": 1.2576286792755127,
      "learning_rate": 0.00017082919960316188,
      "loss": 0.0045,
      "step": 15100
    },
    {
      "epoch": 1.3817949653411163,
      "grad_norm": 29613.03125,
      "learning_rate": 0.0001703491535187378,
      "loss": 0.0028,
      "step": 15150
    },
    {
      "epoch": 1.3863553447646844,
      "grad_norm": 1444.068115234375,
      "learning_rate": 0.00016986910743431368,
      "loss": 0.0037,
      "step": 15200
    },
    {
      "epoch": 1.3909157241882526,
      "grad_norm": 18829.705078125,
      "learning_rate": 0.00016938906134988957,
      "loss": 0.0032,
      "step": 15250
    },
    {
      "epoch": 1.3954761036118204,
      "grad_norm": 0.3990632891654968,
      "learning_rate": 0.00016890901526546548,
      "loss": 0.0027,
      "step": 15300
    },
    {
      "epoch": 1.4000364830353886,
      "grad_norm": 11589.033203125,
      "learning_rate": 0.00016842896918104136,
      "loss": 0.0041,
      "step": 15350
    },
    {
      "epoch": 1.4045968624589567,
      "grad_norm": 0.37232938408851624,
      "learning_rate": 0.00016794892309661728,
      "loss": 0.0034,
      "step": 15400
    },
    {
      "epoch": 1.4091572418825247,
      "grad_norm": 0.3410988450050354,
      "learning_rate": 0.00016746887701219314,
      "loss": 0.0047,
      "step": 15450
    },
    {
      "epoch": 1.4137176213060927,
      "grad_norm": 31498.185546875,
      "learning_rate": 0.00016698883092776905,
      "loss": 0.0042,
      "step": 15500
    },
    {
      "epoch": 1.4182780007296607,
      "grad_norm": 27162.18359375,
      "learning_rate": 0.00016650878484334494,
      "loss": 0.0031,
      "step": 15550
    },
    {
      "epoch": 1.4228383801532287,
      "grad_norm": 0.0800720602273941,
      "learning_rate": 0.00016602873875892085,
      "loss": 0.003,
      "step": 15600
    },
    {
      "epoch": 1.4273987595767967,
      "grad_norm": 2521.1865234375,
      "learning_rate": 0.00016554869267449676,
      "loss": 0.0047,
      "step": 15650
    },
    {
      "epoch": 1.4319591390003648,
      "grad_norm": 0.04565214365720749,
      "learning_rate": 0.00016506864659007262,
      "loss": 0.0028,
      "step": 15700
    },
    {
      "epoch": 1.4365195184239328,
      "grad_norm": 0.026535307988524437,
      "learning_rate": 0.00016458860050564853,
      "loss": 0.0029,
      "step": 15750
    },
    {
      "epoch": 1.441079897847501,
      "grad_norm": 3634.86572265625,
      "learning_rate": 0.00016410855442122442,
      "loss": 0.0047,
      "step": 15800
    },
    {
      "epoch": 1.445640277271069,
      "grad_norm": 0.8684106469154358,
      "learning_rate": 0.00016362850833680033,
      "loss": 0.0037,
      "step": 15850
    },
    {
      "epoch": 1.450200656694637,
      "grad_norm": 0.2747885286808014,
      "learning_rate": 0.0001631484622523762,
      "loss": 0.004,
      "step": 15900
    },
    {
      "epoch": 1.454761036118205,
      "grad_norm": 24.457054138183594,
      "learning_rate": 0.0001626684161679521,
      "loss": 0.0037,
      "step": 15950
    },
    {
      "epoch": 1.459321415541773,
      "grad_norm": 8261.6796875,
      "learning_rate": 0.00016218837008352802,
      "loss": 0.0042,
      "step": 16000
    },
    {
      "epoch": 1.463881794965341,
      "grad_norm": 19835.951171875,
      "learning_rate": 0.0001617083239991039,
      "loss": 0.0032,
      "step": 16050
    },
    {
      "epoch": 1.4684421743889091,
      "grad_norm": 0.1319260448217392,
      "learning_rate": 0.00016122827791467982,
      "loss": 0.0027,
      "step": 16100
    },
    {
      "epoch": 1.4730025538124771,
      "grad_norm": 30241.6796875,
      "learning_rate": 0.00016074823183025568,
      "loss": 0.003,
      "step": 16150
    },
    {
      "epoch": 1.4775629332360452,
      "grad_norm": 25.72944450378418,
      "learning_rate": 0.0001602681857458316,
      "loss": 0.0036,
      "step": 16200
    },
    {
      "epoch": 1.4821233126596134,
      "grad_norm": 0.016656262800097466,
      "learning_rate": 0.00015978813966140748,
      "loss": 0.003,
      "step": 16250
    },
    {
      "epoch": 1.4866836920831812,
      "grad_norm": 18779.494140625,
      "learning_rate": 0.0001593080935769834,
      "loss": 0.0038,
      "step": 16300
    },
    {
      "epoch": 1.4912440715067494,
      "grad_norm": 20460.123046875,
      "learning_rate": 0.00015882804749255927,
      "loss": 0.0038,
      "step": 16350
    },
    {
      "epoch": 1.4958044509303174,
      "grad_norm": 0.9366283416748047,
      "learning_rate": 0.00015834800140813516,
      "loss": 0.0034,
      "step": 16400
    },
    {
      "epoch": 1.5003648303538855,
      "grad_norm": 0.020961785688996315,
      "learning_rate": 0.00015786795532371107,
      "loss": 0.002,
      "step": 16450
    },
    {
      "epoch": 1.5049252097774535,
      "grad_norm": 0.029149208217859268,
      "learning_rate": 0.00015738790923928696,
      "loss": 0.0034,
      "step": 16500
    },
    {
      "epoch": 1.5094855892010215,
      "grad_norm": 0.70670086145401,
      "learning_rate": 0.00015690786315486285,
      "loss": 0.0036,
      "step": 16550
    },
    {
      "epoch": 1.5140459686245895,
      "grad_norm": 0.11508173495531082,
      "learning_rate": 0.00015642781707043873,
      "loss": 0.0029,
      "step": 16600
    },
    {
      "epoch": 1.5186063480481575,
      "grad_norm": 0.1751098483800888,
      "learning_rate": 0.00015594777098601464,
      "loss": 0.0025,
      "step": 16650
    },
    {
      "epoch": 1.5231667274717258,
      "grad_norm": 0.02450675703585148,
      "learning_rate": 0.00015546772490159056,
      "loss": 0.0021,
      "step": 16700
    },
    {
      "epoch": 1.5277271068952936,
      "grad_norm": 0.032794125378131866,
      "learning_rate": 0.00015498767881716642,
      "loss": 0.0035,
      "step": 16750
    },
    {
      "epoch": 1.5322874863188618,
      "grad_norm": 0.01664322428405285,
      "learning_rate": 0.00015450763273274233,
      "loss": 0.0029,
      "step": 16800
    },
    {
      "epoch": 1.5368478657424298,
      "grad_norm": 0.13811558485031128,
      "learning_rate": 0.00015402758664831822,
      "loss": 0.0026,
      "step": 16850
    },
    {
      "epoch": 1.5414082451659978,
      "grad_norm": 11.69924259185791,
      "learning_rate": 0.00015354754056389413,
      "loss": 0.0035,
      "step": 16900
    },
    {
      "epoch": 1.5459686245895659,
      "grad_norm": 17029.1015625,
      "learning_rate": 0.00015306749447947004,
      "loss": 0.0023,
      "step": 16950
    },
    {
      "epoch": 1.5505290040131339,
      "grad_norm": 0.041776735335588455,
      "learning_rate": 0.0001525874483950459,
      "loss": 0.0033,
      "step": 17000
    },
    {
      "epoch": 1.5550893834367019,
      "grad_norm": 7442.642578125,
      "learning_rate": 0.00015210740231062181,
      "loss": 0.0027,
      "step": 17050
    },
    {
      "epoch": 1.55964976286027,
      "grad_norm": 45076.40234375,
      "learning_rate": 0.0001516273562261977,
      "loss": 0.0017,
      "step": 17100
    },
    {
      "epoch": 1.5642101422838381,
      "grad_norm": 3636.59521484375,
      "learning_rate": 0.0001511473101417736,
      "loss": 0.0042,
      "step": 17150
    },
    {
      "epoch": 1.568770521707406,
      "grad_norm": 2.850878953933716,
      "learning_rate": 0.00015066726405734947,
      "loss": 0.0024,
      "step": 17200
    },
    {
      "epoch": 1.5733309011309742,
      "grad_norm": 3882.6943359375,
      "learning_rate": 0.00015018721797292538,
      "loss": 0.0035,
      "step": 17250
    },
    {
      "epoch": 1.577891280554542,
      "grad_norm": 0.06388580054044724,
      "learning_rate": 0.00014970717188850127,
      "loss": 0.0031,
      "step": 17300
    },
    {
      "epoch": 1.5824516599781102,
      "grad_norm": 0.6590938568115234,
      "learning_rate": 0.00014922712580407718,
      "loss": 0.0027,
      "step": 17350
    },
    {
      "epoch": 1.5870120394016782,
      "grad_norm": 0.2934914827346802,
      "learning_rate": 0.00014874707971965307,
      "loss": 0.0033,
      "step": 17400
    },
    {
      "epoch": 1.5915724188252462,
      "grad_norm": 3608.79443359375,
      "learning_rate": 0.00014826703363522898,
      "loss": 0.0028,
      "step": 17450
    },
    {
      "epoch": 1.5961327982488143,
      "grad_norm": 62365.04296875,
      "learning_rate": 0.00014778698755080487,
      "loss": 0.0035,
      "step": 17500
    },
    {
      "epoch": 1.6006931776723823,
      "grad_norm": 36616.21875,
      "learning_rate": 0.00014730694146638076,
      "loss": 0.0032,
      "step": 17550
    },
    {
      "epoch": 1.6052535570959505,
      "grad_norm": 0.045033521950244904,
      "learning_rate": 0.00014682689538195667,
      "loss": 0.004,
      "step": 17600
    },
    {
      "epoch": 1.6098139365195183,
      "grad_norm": 0.19047175347805023,
      "learning_rate": 0.00014634684929753255,
      "loss": 0.0035,
      "step": 17650
    },
    {
      "epoch": 1.6143743159430866,
      "grad_norm": 8285.3671875,
      "learning_rate": 0.00014586680321310844,
      "loss": 0.0032,
      "step": 17700
    },
    {
      "epoch": 1.6189346953666544,
      "grad_norm": 0.05372479185461998,
      "learning_rate": 0.00014538675712868433,
      "loss": 0.0037,
      "step": 17750
    },
    {
      "epoch": 1.6234950747902226,
      "grad_norm": 0.06894957274198532,
      "learning_rate": 0.00014490671104426024,
      "loss": 0.0031,
      "step": 17800
    },
    {
      "epoch": 1.6280554542137906,
      "grad_norm": 22.193649291992188,
      "learning_rate": 0.00014442666495983613,
      "loss": 0.0028,
      "step": 17850
    },
    {
      "epoch": 1.6326158336373586,
      "grad_norm": 0.040129657834768295,
      "learning_rate": 0.00014394661887541204,
      "loss": 0.0023,
      "step": 17900
    },
    {
      "epoch": 1.6371762130609266,
      "grad_norm": 0.86887127161026,
      "learning_rate": 0.00014346657279098792,
      "loss": 0.0028,
      "step": 17950
    },
    {
      "epoch": 1.6417365924844947,
      "grad_norm": 0.03646788373589516,
      "learning_rate": 0.0001429865267065638,
      "loss": 0.0045,
      "step": 18000
    },
    {
      "epoch": 1.646296971908063,
      "grad_norm": 24146.234375,
      "learning_rate": 0.0001425064806221397,
      "loss": 0.0029,
      "step": 18050
    },
    {
      "epoch": 1.6508573513316307,
      "grad_norm": 7433.06103515625,
      "learning_rate": 0.0001420264345377156,
      "loss": 0.0025,
      "step": 18100
    },
    {
      "epoch": 1.655417730755199,
      "grad_norm": 0.10100560635328293,
      "learning_rate": 0.00014154638845329152,
      "loss": 0.0016,
      "step": 18150
    },
    {
      "epoch": 1.6599781101787667,
      "grad_norm": 15236.38671875,
      "learning_rate": 0.0001410663423688674,
      "loss": 0.0049,
      "step": 18200
    },
    {
      "epoch": 1.664538489602335,
      "grad_norm": 3761.83984375,
      "learning_rate": 0.0001405862962844433,
      "loss": 0.0031,
      "step": 18250
    },
    {
      "epoch": 1.669098869025903,
      "grad_norm": 0.026660175994038582,
      "learning_rate": 0.00014010625020001918,
      "loss": 0.0028,
      "step": 18300
    },
    {
      "epoch": 1.673659248449471,
      "grad_norm": 43.5379524230957,
      "learning_rate": 0.0001396262041155951,
      "loss": 0.0029,
      "step": 18350
    },
    {
      "epoch": 1.678219627873039,
      "grad_norm": 4485.64599609375,
      "learning_rate": 0.00013914615803117098,
      "loss": 0.0021,
      "step": 18400
    },
    {
      "epoch": 1.682780007296607,
      "grad_norm": 20.341028213500977,
      "learning_rate": 0.0001386661119467469,
      "loss": 0.0018,
      "step": 18450
    },
    {
      "epoch": 1.6873403867201753,
      "grad_norm": 0.05637165904045105,
      "learning_rate": 0.00013818606586232278,
      "loss": 0.0032,
      "step": 18500
    },
    {
      "epoch": 1.691900766143743,
      "grad_norm": 0.07918210327625275,
      "learning_rate": 0.00013770601977789866,
      "loss": 0.0021,
      "step": 18550
    },
    {
      "epoch": 1.6964611455673113,
      "grad_norm": 5.140122413635254,
      "learning_rate": 0.00013722597369347455,
      "loss": 0.0013,
      "step": 18600
    },
    {
      "epoch": 1.701021524990879,
      "grad_norm": 0.4726804494857788,
      "learning_rate": 0.00013674592760905046,
      "loss": 0.0025,
      "step": 18650
    },
    {
      "epoch": 1.7055819044144473,
      "grad_norm": 0.006519020069390535,
      "learning_rate": 0.00013626588152462635,
      "loss": 0.0023,
      "step": 18700
    },
    {
      "epoch": 1.7101422838380154,
      "grad_norm": 16141.5556640625,
      "learning_rate": 0.00013578583544020224,
      "loss": 0.0033,
      "step": 18750
    },
    {
      "epoch": 1.7147026632615834,
      "grad_norm": 0.34066829085350037,
      "learning_rate": 0.00013530578935577815,
      "loss": 0.0038,
      "step": 18800
    },
    {
      "epoch": 1.7192630426851514,
      "grad_norm": 0.03895588591694832,
      "learning_rate": 0.00013482574327135404,
      "loss": 0.0036,
      "step": 18850
    },
    {
      "epoch": 1.7238234221087194,
      "grad_norm": 0.0550091415643692,
      "learning_rate": 0.00013434569718692995,
      "loss": 0.0029,
      "step": 18900
    },
    {
      "epoch": 1.7283838015322874,
      "grad_norm": 0.7973347902297974,
      "learning_rate": 0.00013386565110250583,
      "loss": 0.0025,
      "step": 18950
    },
    {
      "epoch": 1.7329441809558555,
      "grad_norm": 35276.36328125,
      "learning_rate": 0.00013338560501808172,
      "loss": 0.004,
      "step": 19000
    },
    {
      "epoch": 1.7375045603794237,
      "grad_norm": 1.1493160724639893,
      "learning_rate": 0.0001329055589336576,
      "loss": 0.0027,
      "step": 19050
    },
    {
      "epoch": 1.7420649398029915,
      "grad_norm": 0.051426179707050323,
      "learning_rate": 0.00013242551284923352,
      "loss": 0.0038,
      "step": 19100
    },
    {
      "epoch": 1.7466253192265597,
      "grad_norm": 4459.54638671875,
      "learning_rate": 0.0001319454667648094,
      "loss": 0.003,
      "step": 19150
    },
    {
      "epoch": 1.7511856986501277,
      "grad_norm": 4538.22802734375,
      "learning_rate": 0.00013146542068038532,
      "loss": 0.0026,
      "step": 19200
    },
    {
      "epoch": 1.7557460780736958,
      "grad_norm": 1.5835508108139038,
      "learning_rate": 0.0001309853745959612,
      "loss": 0.0038,
      "step": 19250
    },
    {
      "epoch": 1.7603064574972638,
      "grad_norm": 0.1683024913072586,
      "learning_rate": 0.0001305053285115371,
      "loss": 0.004,
      "step": 19300
    },
    {
      "epoch": 1.7648668369208318,
      "grad_norm": 0.14233067631721497,
      "learning_rate": 0.00013002528242711298,
      "loss": 0.0034,
      "step": 19350
    },
    {
      "epoch": 1.7694272163443998,
      "grad_norm": 36686.96484375,
      "learning_rate": 0.0001295452363426889,
      "loss": 0.0032,
      "step": 19400
    },
    {
      "epoch": 1.7739875957679678,
      "grad_norm": 0.30023568868637085,
      "learning_rate": 0.00012906519025826478,
      "loss": 0.0035,
      "step": 19450
    },
    {
      "epoch": 1.778547975191536,
      "grad_norm": 2.329025983810425,
      "learning_rate": 0.0001285851441738407,
      "loss": 0.0038,
      "step": 19500
    },
    {
      "epoch": 1.7831083546151039,
      "grad_norm": 0.04244843125343323,
      "learning_rate": 0.00012810509808941657,
      "loss": 0.004,
      "step": 19550
    },
    {
      "epoch": 1.787668734038672,
      "grad_norm": 44883.90234375,
      "learning_rate": 0.00012762505200499246,
      "loss": 0.0039,
      "step": 19600
    },
    {
      "epoch": 1.79222911346224,
      "grad_norm": 18669.72265625,
      "learning_rate": 0.00012714500592056837,
      "loss": 0.0038,
      "step": 19650
    },
    {
      "epoch": 1.7967894928858081,
      "grad_norm": 10.40857219696045,
      "learning_rate": 0.00012666495983614426,
      "loss": 0.0019,
      "step": 19700
    },
    {
      "epoch": 1.8013498723093762,
      "grad_norm": 0.16267651319503784,
      "learning_rate": 0.00012618491375172015,
      "loss": 0.002,
      "step": 19750
    },
    {
      "epoch": 1.8059102517329442,
      "grad_norm": 6609.6640625,
      "learning_rate": 0.00012570486766729606,
      "loss": 0.0024,
      "step": 19800
    },
    {
      "epoch": 1.8104706311565122,
      "grad_norm": 0.3650516867637634,
      "learning_rate": 0.00012522482158287194,
      "loss": 0.0039,
      "step": 19850
    },
    {
      "epoch": 1.8150310105800802,
      "grad_norm": 3865.26708984375,
      "learning_rate": 0.00012474477549844783,
      "loss": 0.0023,
      "step": 19900
    },
    {
      "epoch": 1.8195913900036484,
      "grad_norm": 4614.8798828125,
      "learning_rate": 0.00012426472941402374,
      "loss": 0.0036,
      "step": 19950
    },
    {
      "epoch": 1.8241517694272162,
      "grad_norm": 38064.40234375,
      "learning_rate": 0.00012378468332959963,
      "loss": 0.0041,
      "step": 20000
    },
    {
      "epoch": 1.8287121488507845,
      "grad_norm": 2.6146039962768555,
      "learning_rate": 0.00012330463724517552,
      "loss": 0.0026,
      "step": 20050
    },
    {
      "epoch": 1.8332725282743523,
      "grad_norm": 0.08844021707773209,
      "learning_rate": 0.00012282459116075143,
      "loss": 0.0016,
      "step": 20100
    },
    {
      "epoch": 1.8378329076979205,
      "grad_norm": 0.3553329408168793,
      "learning_rate": 0.00012234454507632731,
      "loss": 0.003,
      "step": 20150
    },
    {
      "epoch": 1.8423932871214885,
      "grad_norm": 0.7090244293212891,
      "learning_rate": 0.00012186449899190321,
      "loss": 0.002,
      "step": 20200
    },
    {
      "epoch": 1.8469536665450565,
      "grad_norm": 4231.20458984375,
      "learning_rate": 0.0001213844529074791,
      "loss": 0.0028,
      "step": 20250
    },
    {
      "epoch": 1.8515140459686246,
      "grad_norm": 0.053012121468782425,
      "learning_rate": 0.000120904406823055,
      "loss": 0.0028,
      "step": 20300
    },
    {
      "epoch": 1.8560744253921926,
      "grad_norm": 0.34640631079673767,
      "learning_rate": 0.00012042436073863089,
      "loss": 0.0025,
      "step": 20350
    },
    {
      "epoch": 1.8606348048157608,
      "grad_norm": 0.5880560278892517,
      "learning_rate": 0.00011994431465420679,
      "loss": 0.0035,
      "step": 20400
    },
    {
      "epoch": 1.8651951842393286,
      "grad_norm": 14996.9990234375,
      "learning_rate": 0.0001194642685697827,
      "loss": 0.0035,
      "step": 20450
    },
    {
      "epoch": 1.8697555636628969,
      "grad_norm": 11031.048828125,
      "learning_rate": 0.00011898422248535858,
      "loss": 0.0027,
      "step": 20500
    },
    {
      "epoch": 1.8743159430864647,
      "grad_norm": 75223.4765625,
      "learning_rate": 0.00011850417640093448,
      "loss": 0.0033,
      "step": 20550
    },
    {
      "epoch": 1.878876322510033,
      "grad_norm": 604.8280029296875,
      "learning_rate": 0.00011802413031651037,
      "loss": 0.0033,
      "step": 20600
    },
    {
      "epoch": 1.883436701933601,
      "grad_norm": 0.1404799222946167,
      "learning_rate": 0.00011754408423208627,
      "loss": 0.0027,
      "step": 20650
    },
    {
      "epoch": 1.887997081357169,
      "grad_norm": 3707.977294921875,
      "learning_rate": 0.00011706403814766216,
      "loss": 0.0039,
      "step": 20700
    },
    {
      "epoch": 1.892557460780737,
      "grad_norm": 0.1688435971736908,
      "learning_rate": 0.00011658399206323807,
      "loss": 0.0027,
      "step": 20750
    },
    {
      "epoch": 1.897117840204305,
      "grad_norm": 0.2943316698074341,
      "learning_rate": 0.00011610394597881395,
      "loss": 0.0017,
      "step": 20800
    },
    {
      "epoch": 1.9016782196278732,
      "grad_norm": 0.19117026031017303,
      "learning_rate": 0.00011562389989438985,
      "loss": 0.0033,
      "step": 20850
    },
    {
      "epoch": 1.906238599051441,
      "grad_norm": 0.05698743835091591,
      "learning_rate": 0.00011514385380996574,
      "loss": 0.0019,
      "step": 20900
    },
    {
      "epoch": 1.9107989784750092,
      "grad_norm": 5.793420314788818,
      "learning_rate": 0.00011466380772554164,
      "loss": 0.0037,
      "step": 20950
    },
    {
      "epoch": 1.915359357898577,
      "grad_norm": 6254.2705078125,
      "learning_rate": 0.00011418376164111753,
      "loss": 0.0037,
      "step": 21000
    },
    {
      "epoch": 1.9199197373221453,
      "grad_norm": 0.05002039298415184,
      "learning_rate": 0.00011370371555669343,
      "loss": 0.0021,
      "step": 21050
    },
    {
      "epoch": 1.9244801167457133,
      "grad_norm": 1.3335723876953125,
      "learning_rate": 0.00011322366947226934,
      "loss": 0.0032,
      "step": 21100
    },
    {
      "epoch": 1.9290404961692813,
      "grad_norm": 0.07933121174573898,
      "learning_rate": 0.00011274362338784522,
      "loss": 0.0027,
      "step": 21150
    },
    {
      "epoch": 1.9336008755928493,
      "grad_norm": 6266.72314453125,
      "learning_rate": 0.00011226357730342112,
      "loss": 0.0027,
      "step": 21200
    },
    {
      "epoch": 1.9381612550164173,
      "grad_norm": 0.01566566713154316,
      "learning_rate": 0.00011178353121899701,
      "loss": 0.003,
      "step": 21250
    },
    {
      "epoch": 1.9427216344399854,
      "grad_norm": 3384.154541015625,
      "learning_rate": 0.00011130348513457291,
      "loss": 0.0028,
      "step": 21300
    },
    {
      "epoch": 1.9472820138635534,
      "grad_norm": 0.01317903958261013,
      "learning_rate": 0.0001108234390501488,
      "loss": 0.0038,
      "step": 21350
    },
    {
      "epoch": 1.9518423932871216,
      "grad_norm": 8.292830467224121,
      "learning_rate": 0.0001103433929657247,
      "loss": 0.0027,
      "step": 21400
    },
    {
      "epoch": 1.9564027727106894,
      "grad_norm": 0.1222776547074318,
      "learning_rate": 0.0001098633468813006,
      "loss": 0.0021,
      "step": 21450
    },
    {
      "epoch": 1.9609631521342576,
      "grad_norm": 0.02265009470283985,
      "learning_rate": 0.0001093833007968765,
      "loss": 0.0017,
      "step": 21500
    },
    {
      "epoch": 1.9655235315578257,
      "grad_norm": 0.1876453459262848,
      "learning_rate": 0.00010890325471245238,
      "loss": 0.003,
      "step": 21550
    },
    {
      "epoch": 1.9700839109813937,
      "grad_norm": 0.1137092337012291,
      "learning_rate": 0.00010842320862802828,
      "loss": 0.0043,
      "step": 21600
    },
    {
      "epoch": 1.9746442904049617,
      "grad_norm": 38731.265625,
      "learning_rate": 0.00010794316254360417,
      "loss": 0.0028,
      "step": 21650
    },
    {
      "epoch": 1.9792046698285297,
      "grad_norm": 0.6806749105453491,
      "learning_rate": 0.00010746311645918007,
      "loss": 0.003,
      "step": 21700
    },
    {
      "epoch": 1.9837650492520977,
      "grad_norm": 0.05217931419610977,
      "learning_rate": 0.00010698307037475598,
      "loss": 0.0017,
      "step": 21750
    },
    {
      "epoch": 1.9883254286756658,
      "grad_norm": 0.06577259302139282,
      "learning_rate": 0.00010650302429033186,
      "loss": 0.0029,
      "step": 21800
    },
    {
      "epoch": 1.992885808099234,
      "grad_norm": 19563.8671875,
      "learning_rate": 0.00010602297820590776,
      "loss": 0.0031,
      "step": 21850
    },
    {
      "epoch": 1.9974461875228018,
      "grad_norm": 4579.1337890625,
      "learning_rate": 0.00010554293212148365,
      "loss": 0.0041,
      "step": 21900
    },
    {
      "epoch": 2.00200656694637,
      "grad_norm": 0.05012001097202301,
      "learning_rate": 0.00010506288603705955,
      "loss": 0.0016,
      "step": 21950
    },
    {
      "epoch": 2.006566946369938,
      "grad_norm": 0.06193343177437782,
      "learning_rate": 0.00010458283995263544,
      "loss": 0.0027,
      "step": 22000
    },
    {
      "epoch": 2.011127325793506,
      "grad_norm": 0.03155224770307541,
      "learning_rate": 0.00010410279386821134,
      "loss": 0.0023,
      "step": 22050
    },
    {
      "epoch": 2.015687705217074,
      "grad_norm": 0.1927330642938614,
      "learning_rate": 0.00010362274778378723,
      "loss": 0.0018,
      "step": 22100
    },
    {
      "epoch": 2.020248084640642,
      "grad_norm": 0.015113835223019123,
      "learning_rate": 0.00010314270169936313,
      "loss": 0.0022,
      "step": 22150
    },
    {
      "epoch": 2.0248084640642103,
      "grad_norm": 2.056905746459961,
      "learning_rate": 0.00010266265561493902,
      "loss": 0.0024,
      "step": 22200
    },
    {
      "epoch": 2.029368843487778,
      "grad_norm": 14.43913745880127,
      "learning_rate": 0.00010218260953051492,
      "loss": 0.0021,
      "step": 22250
    },
    {
      "epoch": 2.0339292229113464,
      "grad_norm": 0.07525146007537842,
      "learning_rate": 0.0001017025634460908,
      "loss": 0.0026,
      "step": 22300
    },
    {
      "epoch": 2.038489602334914,
      "grad_norm": 0.0161854550242424,
      "learning_rate": 0.0001012225173616667,
      "loss": 0.0024,
      "step": 22350
    },
    {
      "epoch": 2.0430499817584824,
      "grad_norm": 0.016340458765625954,
      "learning_rate": 0.00010074247127724262,
      "loss": 0.0021,
      "step": 22400
    },
    {
      "epoch": 2.04761036118205,
      "grad_norm": 153.19984436035156,
      "learning_rate": 0.0001002624251928185,
      "loss": 0.0017,
      "step": 22450
    },
    {
      "epoch": 2.0521707406056184,
      "grad_norm": 3925.54345703125,
      "learning_rate": 9.97823791083944e-05,
      "loss": 0.001,
      "step": 22500
    },
    {
      "epoch": 2.0567311200291862,
      "grad_norm": 5449.10205078125,
      "learning_rate": 9.930233302397029e-05,
      "loss": 0.0034,
      "step": 22550
    },
    {
      "epoch": 2.0612914994527545,
      "grad_norm": 0.04313468560576439,
      "learning_rate": 9.882228693954619e-05,
      "loss": 0.0029,
      "step": 22600
    },
    {
      "epoch": 2.0658518788763227,
      "grad_norm": 0.14794223010540009,
      "learning_rate": 9.834224085512208e-05,
      "loss": 0.0027,
      "step": 22650
    },
    {
      "epoch": 2.0704122582998905,
      "grad_norm": 0.04434658959507942,
      "learning_rate": 9.786219477069798e-05,
      "loss": 0.0026,
      "step": 22700
    },
    {
      "epoch": 2.0749726377234587,
      "grad_norm": 6919.8310546875,
      "learning_rate": 9.738214868627387e-05,
      "loss": 0.0023,
      "step": 22750
    },
    {
      "epoch": 2.0795330171470265,
      "grad_norm": 0.09895185381174088,
      "learning_rate": 9.690210260184977e-05,
      "loss": 0.0024,
      "step": 22800
    },
    {
      "epoch": 2.084093396570595,
      "grad_norm": 7952.43408203125,
      "learning_rate": 9.642205651742566e-05,
      "loss": 0.0027,
      "step": 22850
    },
    {
      "epoch": 2.0886537759941626,
      "grad_norm": 3559.571533203125,
      "learning_rate": 9.594201043300156e-05,
      "loss": 0.0027,
      "step": 22900
    },
    {
      "epoch": 2.093214155417731,
      "grad_norm": 12518.537109375,
      "learning_rate": 9.546196434857745e-05,
      "loss": 0.0016,
      "step": 22950
    },
    {
      "epoch": 2.0977745348412986,
      "grad_norm": 0.0227490421384573,
      "learning_rate": 9.498191826415335e-05,
      "loss": 0.0032,
      "step": 23000
    },
    {
      "epoch": 2.102334914264867,
      "grad_norm": 0.019405636936426163,
      "learning_rate": 9.450187217972923e-05,
      "loss": 0.0019,
      "step": 23050
    },
    {
      "epoch": 2.106895293688435,
      "grad_norm": 0.18057216703891754,
      "learning_rate": 9.402182609530514e-05,
      "loss": 0.0015,
      "step": 23100
    },
    {
      "epoch": 2.111455673112003,
      "grad_norm": 0.04216352105140686,
      "learning_rate": 9.354178001088104e-05,
      "loss": 0.0016,
      "step": 23150
    },
    {
      "epoch": 2.116016052535571,
      "grad_norm": 0.19979147613048553,
      "learning_rate": 9.306173392645693e-05,
      "loss": 0.0027,
      "step": 23200
    },
    {
      "epoch": 2.120576431959139,
      "grad_norm": 5037.73828125,
      "learning_rate": 9.258168784203283e-05,
      "loss": 0.0023,
      "step": 23250
    },
    {
      "epoch": 2.125136811382707,
      "grad_norm": 63.627906799316406,
      "learning_rate": 9.210164175760872e-05,
      "loss": 0.0013,
      "step": 23300
    },
    {
      "epoch": 2.129697190806275,
      "grad_norm": 0.0883428230881691,
      "learning_rate": 9.162159567318462e-05,
      "loss": 0.0014,
      "step": 23350
    },
    {
      "epoch": 2.134257570229843,
      "grad_norm": 4633.60400390625,
      "learning_rate": 9.114154958876051e-05,
      "loss": 0.0027,
      "step": 23400
    },
    {
      "epoch": 2.138817949653411,
      "grad_norm": 5642.34521484375,
      "learning_rate": 9.066150350433641e-05,
      "loss": 0.0025,
      "step": 23450
    },
    {
      "epoch": 2.1433783290769792,
      "grad_norm": 0.03905538469552994,
      "learning_rate": 9.01814574199123e-05,
      "loss": 0.0026,
      "step": 23500
    },
    {
      "epoch": 2.147938708500547,
      "grad_norm": 1.1055183410644531,
      "learning_rate": 8.97014113354882e-05,
      "loss": 0.0026,
      "step": 23550
    },
    {
      "epoch": 2.1524990879241153,
      "grad_norm": 0.08812165260314941,
      "learning_rate": 8.922136525106409e-05,
      "loss": 0.0025,
      "step": 23600
    },
    {
      "epoch": 2.1570594673476835,
      "grad_norm": 0.4244779944419861,
      "learning_rate": 8.874131916663999e-05,
      "loss": 0.0026,
      "step": 23650
    },
    {
      "epoch": 2.1616198467712513,
      "grad_norm": 618.9578247070312,
      "learning_rate": 8.826127308221587e-05,
      "loss": 0.0025,
      "step": 23700
    },
    {
      "epoch": 2.1661802261948195,
      "grad_norm": 0.028306765481829643,
      "learning_rate": 8.778122699779178e-05,
      "loss": 0.0021,
      "step": 23750
    },
    {
      "epoch": 2.1707406056183873,
      "grad_norm": 0.07972612231969833,
      "learning_rate": 8.730118091336768e-05,
      "loss": 0.0021,
      "step": 23800
    },
    {
      "epoch": 2.1753009850419556,
      "grad_norm": 0.11687915772199631,
      "learning_rate": 8.682113482894357e-05,
      "loss": 0.0018,
      "step": 23850
    },
    {
      "epoch": 2.1798613644655234,
      "grad_norm": 0.10917386412620544,
      "learning_rate": 8.634108874451947e-05,
      "loss": 0.002,
      "step": 23900
    },
    {
      "epoch": 2.1844217438890916,
      "grad_norm": 0.047200556844472885,
      "learning_rate": 8.586104266009536e-05,
      "loss": 0.0022,
      "step": 23950
    },
    {
      "epoch": 2.18898212331266,
      "grad_norm": 0.2868720293045044,
      "learning_rate": 8.538099657567126e-05,
      "loss": 0.003,
      "step": 24000
    },
    {
      "epoch": 2.1935425027362276,
      "grad_norm": 44454.390625,
      "learning_rate": 8.490095049124715e-05,
      "loss": 0.0017,
      "step": 24050
    },
    {
      "epoch": 2.198102882159796,
      "grad_norm": 0.15086306631565094,
      "learning_rate": 8.442090440682305e-05,
      "loss": 0.0017,
      "step": 24100
    },
    {
      "epoch": 2.2026632615833637,
      "grad_norm": 0.017763232812285423,
      "learning_rate": 8.394085832239894e-05,
      "loss": 0.0019,
      "step": 24150
    },
    {
      "epoch": 2.207223641006932,
      "grad_norm": 4779.3134765625,
      "learning_rate": 8.346081223797484e-05,
      "loss": 0.0019,
      "step": 24200
    },
    {
      "epoch": 2.2117840204304997,
      "grad_norm": 0.04136299341917038,
      "learning_rate": 8.298076615355073e-05,
      "loss": 0.0025,
      "step": 24250
    },
    {
      "epoch": 2.216344399854068,
      "grad_norm": 3426.871337890625,
      "learning_rate": 8.250072006912663e-05,
      "loss": 0.0016,
      "step": 24300
    },
    {
      "epoch": 2.2209047792776357,
      "grad_norm": 0.03517298027873039,
      "learning_rate": 8.202067398470251e-05,
      "loss": 0.0013,
      "step": 24350
    },
    {
      "epoch": 2.225465158701204,
      "grad_norm": 0.017099132761359215,
      "learning_rate": 8.154062790027842e-05,
      "loss": 0.0028,
      "step": 24400
    },
    {
      "epoch": 2.2300255381247718,
      "grad_norm": 0.02021479234099388,
      "learning_rate": 8.106058181585432e-05,
      "loss": 0.0019,
      "step": 24450
    },
    {
      "epoch": 2.23458591754834,
      "grad_norm": 236.51824951171875,
      "learning_rate": 8.058053573143021e-05,
      "loss": 0.0023,
      "step": 24500
    },
    {
      "epoch": 2.2391462969719083,
      "grad_norm": 0.03770322725176811,
      "learning_rate": 8.010048964700611e-05,
      "loss": 0.0021,
      "step": 24550
    },
    {
      "epoch": 2.243706676395476,
      "grad_norm": 4.61721658706665,
      "learning_rate": 7.9620443562582e-05,
      "loss": 0.001,
      "step": 24600
    },
    {
      "epoch": 2.2482670558190443,
      "grad_norm": 14728.6513671875,
      "learning_rate": 7.91403974781579e-05,
      "loss": 0.004,
      "step": 24650
    },
    {
      "epoch": 2.252827435242612,
      "grad_norm": 6517.13818359375,
      "learning_rate": 7.866035139373378e-05,
      "loss": 0.0024,
      "step": 24700
    },
    {
      "epoch": 2.2573878146661803,
      "grad_norm": 0.07086466997861862,
      "learning_rate": 7.81803053093097e-05,
      "loss": 0.0028,
      "step": 24750
    },
    {
      "epoch": 2.261948194089748,
      "grad_norm": 12654.666015625,
      "learning_rate": 7.770025922488558e-05,
      "loss": 0.0027,
      "step": 24800
    },
    {
      "epoch": 2.2665085735133164,
      "grad_norm": 57.50972366333008,
      "learning_rate": 7.722021314046148e-05,
      "loss": 0.002,
      "step": 24850
    },
    {
      "epoch": 2.271068952936884,
      "grad_norm": 16735.998046875,
      "learning_rate": 7.674016705603737e-05,
      "loss": 0.0025,
      "step": 24900
    },
    {
      "epoch": 2.2756293323604524,
      "grad_norm": 0.01568036526441574,
      "learning_rate": 7.626012097161327e-05,
      "loss": 0.0019,
      "step": 24950
    },
    {
      "epoch": 2.28018971178402,
      "grad_norm": 0.20179085433483124,
      "learning_rate": 7.578007488718915e-05,
      "loss": 0.003,
      "step": 25000
    },
    {
      "epoch": 2.2847500912075884,
      "grad_norm": 0.4180944263935089,
      "learning_rate": 7.530002880276506e-05,
      "loss": 0.0019,
      "step": 25050
    },
    {
      "epoch": 2.2893104706311567,
      "grad_norm": 0.5447505712509155,
      "learning_rate": 7.481998271834095e-05,
      "loss": 0.001,
      "step": 25100
    },
    {
      "epoch": 2.2938708500547245,
      "grad_norm": 1.3536326885223389,
      "learning_rate": 7.433993663391685e-05,
      "loss": 0.0024,
      "step": 25150
    },
    {
      "epoch": 2.2984312294782927,
      "grad_norm": 0.042514868080616,
      "learning_rate": 7.385989054949275e-05,
      "loss": 0.0026,
      "step": 25200
    },
    {
      "epoch": 2.3029916089018605,
      "grad_norm": 0.14677055180072784,
      "learning_rate": 7.337984446506864e-05,
      "loss": 0.0023,
      "step": 25250
    },
    {
      "epoch": 2.3075519883254287,
      "grad_norm": 0.05594419687986374,
      "learning_rate": 7.289979838064454e-05,
      "loss": 0.0033,
      "step": 25300
    },
    {
      "epoch": 2.3121123677489965,
      "grad_norm": 62703.234375,
      "learning_rate": 7.241975229622043e-05,
      "loss": 0.0019,
      "step": 25350
    },
    {
      "epoch": 2.3166727471725648,
      "grad_norm": 0.021967608481645584,
      "learning_rate": 7.193970621179632e-05,
      "loss": 0.0026,
      "step": 25400
    },
    {
      "epoch": 2.321233126596133,
      "grad_norm": 12556.9072265625,
      "learning_rate": 7.145966012737222e-05,
      "loss": 0.0024,
      "step": 25450
    },
    {
      "epoch": 2.325793506019701,
      "grad_norm": 0.9244653582572937,
      "learning_rate": 7.097961404294812e-05,
      "loss": 0.0022,
      "step": 25500
    },
    {
      "epoch": 2.330353885443269,
      "grad_norm": 4804.9150390625,
      "learning_rate": 7.0499567958524e-05,
      "loss": 0.0029,
      "step": 25550
    },
    {
      "epoch": 2.334914264866837,
      "grad_norm": 0.01469819713383913,
      "learning_rate": 7.00195218740999e-05,
      "loss": 0.0018,
      "step": 25600
    },
    {
      "epoch": 2.339474644290405,
      "grad_norm": 0.018321242183446884,
      "learning_rate": 6.95394757896758e-05,
      "loss": 0.0007,
      "step": 25650
    },
    {
      "epoch": 2.344035023713973,
      "grad_norm": 0.0072083668783307076,
      "learning_rate": 6.90594297052517e-05,
      "loss": 0.0022,
      "step": 25700
    },
    {
      "epoch": 2.348595403137541,
      "grad_norm": 15674.6611328125,
      "learning_rate": 6.857938362082759e-05,
      "loss": 0.0025,
      "step": 25750
    },
    {
      "epoch": 2.353155782561109,
      "grad_norm": 0.9591525793075562,
      "learning_rate": 6.809933753640349e-05,
      "loss": 0.0029,
      "step": 25800
    },
    {
      "epoch": 2.357716161984677,
      "grad_norm": 0.18057215213775635,
      "learning_rate": 6.761929145197939e-05,
      "loss": 0.0027,
      "step": 25850
    },
    {
      "epoch": 2.362276541408245,
      "grad_norm": 0.017437120899558067,
      "learning_rate": 6.713924536755528e-05,
      "loss": 0.0023,
      "step": 25900
    },
    {
      "epoch": 2.366836920831813,
      "grad_norm": 0.0936257541179657,
      "learning_rate": 6.665919928313118e-05,
      "loss": 0.0025,
      "step": 25950
    },
    {
      "epoch": 2.3713973002553814,
      "grad_norm": 38892.01953125,
      "learning_rate": 6.617915319870707e-05,
      "loss": 0.0024,
      "step": 26000
    },
    {
      "epoch": 2.375957679678949,
      "grad_norm": 0.2452092468738556,
      "learning_rate": 6.569910711428296e-05,
      "loss": 0.0024,
      "step": 26050
    },
    {
      "epoch": 2.3805180591025175,
      "grad_norm": 0.20887602865695953,
      "learning_rate": 6.521906102985886e-05,
      "loss": 0.0028,
      "step": 26100
    },
    {
      "epoch": 2.3850784385260853,
      "grad_norm": 25588.130859375,
      "learning_rate": 6.473901494543476e-05,
      "loss": 0.0027,
      "step": 26150
    },
    {
      "epoch": 2.3896388179496535,
      "grad_norm": 0.038271404802799225,
      "learning_rate": 6.425896886101065e-05,
      "loss": 0.0027,
      "step": 26200
    },
    {
      "epoch": 2.3941991973732213,
      "grad_norm": 4018.161376953125,
      "learning_rate": 6.377892277658655e-05,
      "loss": 0.0026,
      "step": 26250
    },
    {
      "epoch": 2.3987595767967895,
      "grad_norm": 4386.45849609375,
      "learning_rate": 6.329887669216245e-05,
      "loss": 0.003,
      "step": 26300
    },
    {
      "epoch": 2.4033199562203578,
      "grad_norm": 21795.58984375,
      "learning_rate": 6.281883060773834e-05,
      "loss": 0.0018,
      "step": 26350
    },
    {
      "epoch": 2.4078803356439256,
      "grad_norm": 0.3791772723197937,
      "learning_rate": 6.233878452331423e-05,
      "loss": 0.0017,
      "step": 26400
    },
    {
      "epoch": 2.412440715067494,
      "grad_norm": 0.05954933539032936,
      "learning_rate": 6.185873843889013e-05,
      "loss": 0.0017,
      "step": 26450
    },
    {
      "epoch": 2.4170010944910616,
      "grad_norm": 0.08753537386655807,
      "learning_rate": 6.137869235446603e-05,
      "loss": 0.0029,
      "step": 26500
    },
    {
      "epoch": 2.42156147391463,
      "grad_norm": 0.2533838450908661,
      "learning_rate": 6.0898646270041916e-05,
      "loss": 0.0023,
      "step": 26550
    },
    {
      "epoch": 2.4261218533381976,
      "grad_norm": 0.2958216071128845,
      "learning_rate": 6.041860018561781e-05,
      "loss": 0.0012,
      "step": 26600
    },
    {
      "epoch": 2.430682232761766,
      "grad_norm": 0.008156552910804749,
      "learning_rate": 5.9938554101193715e-05,
      "loss": 0.0021,
      "step": 26650
    },
    {
      "epoch": 2.4352426121853337,
      "grad_norm": 0.06203844025731087,
      "learning_rate": 5.945850801676961e-05,
      "loss": 0.0015,
      "step": 26700
    },
    {
      "epoch": 2.439802991608902,
      "grad_norm": 3885.037841796875,
      "learning_rate": 5.89784619323455e-05,
      "loss": 0.0019,
      "step": 26750
    },
    {
      "epoch": 2.4443633710324697,
      "grad_norm": 0.013169749639928341,
      "learning_rate": 5.84984158479214e-05,
      "loss": 0.002,
      "step": 26800
    },
    {
      "epoch": 2.448923750456038,
      "grad_norm": 0.024282515048980713,
      "learning_rate": 5.801836976349729e-05,
      "loss": 0.002,
      "step": 26850
    },
    {
      "epoch": 2.453484129879606,
      "grad_norm": 40509.26953125,
      "learning_rate": 5.7538323679073186e-05,
      "loss": 0.003,
      "step": 26900
    },
    {
      "epoch": 2.458044509303174,
      "grad_norm": 0.03494150564074516,
      "learning_rate": 5.7058277594649085e-05,
      "loss": 0.0028,
      "step": 26950
    },
    {
      "epoch": 2.462604888726742,
      "grad_norm": 0.004376737400889397,
      "learning_rate": 5.657823151022498e-05,
      "loss": 0.0018,
      "step": 27000
    },
    {
      "epoch": 2.46716526815031,
      "grad_norm": 0.014810506254434586,
      "learning_rate": 5.609818542580087e-05,
      "loss": 0.0032,
      "step": 27050
    },
    {
      "epoch": 2.4717256475738782,
      "grad_norm": 6308.4130859375,
      "learning_rate": 5.5618139341376763e-05,
      "loss": 0.0017,
      "step": 27100
    },
    {
      "epoch": 2.476286026997446,
      "grad_norm": 0.023406943306326866,
      "learning_rate": 5.513809325695266e-05,
      "loss": 0.0032,
      "step": 27150
    },
    {
      "epoch": 2.4808464064210143,
      "grad_norm": 0.050862353295087814,
      "learning_rate": 5.4658047172528556e-05,
      "loss": 0.0014,
      "step": 27200
    },
    {
      "epoch": 2.485406785844582,
      "grad_norm": 0.33741652965545654,
      "learning_rate": 5.417800108810445e-05,
      "loss": 0.0011,
      "step": 27250
    },
    {
      "epoch": 2.4899671652681503,
      "grad_norm": 0.022705387324094772,
      "learning_rate": 5.369795500368035e-05,
      "loss": 0.0019,
      "step": 27300
    },
    {
      "epoch": 2.494527544691718,
      "grad_norm": 4005.51123046875,
      "learning_rate": 5.321790891925624e-05,
      "loss": 0.0026,
      "step": 27350
    },
    {
      "epoch": 2.4990879241152864,
      "grad_norm": 20395.263671875,
      "learning_rate": 5.2737862834832134e-05,
      "loss": 0.0023,
      "step": 27400
    },
    {
      "epoch": 2.5036483035388546,
      "grad_norm": 0.27770495414733887,
      "learning_rate": 5.225781675040804e-05,
      "loss": 0.0017,
      "step": 27450
    },
    {
      "epoch": 2.5082086829624224,
      "grad_norm": 110678.7734375,
      "learning_rate": 5.177777066598393e-05,
      "loss": 0.0023,
      "step": 27500
    },
    {
      "epoch": 2.5127690623859906,
      "grad_norm": 0.03764258325099945,
      "learning_rate": 5.1297724581559826e-05,
      "loss": 0.0031,
      "step": 27550
    },
    {
      "epoch": 2.5173294418095584,
      "grad_norm": 4636.35595703125,
      "learning_rate": 5.081767849713572e-05,
      "loss": 0.0019,
      "step": 27600
    },
    {
      "epoch": 2.5218898212331267,
      "grad_norm": 0.07069792598485947,
      "learning_rate": 5.033763241271162e-05,
      "loss": 0.0013,
      "step": 27650
    },
    {
      "epoch": 2.5264502006566945,
      "grad_norm": 0.11087818443775177,
      "learning_rate": 4.985758632828751e-05,
      "loss": 0.0019,
      "step": 27700
    },
    {
      "epoch": 2.5310105800802627,
      "grad_norm": 0.025537969544529915,
      "learning_rate": 4.9377540243863403e-05,
      "loss": 0.002,
      "step": 27750
    },
    {
      "epoch": 2.535570959503831,
      "grad_norm": 0.16235260665416718,
      "learning_rate": 4.88974941594393e-05,
      "loss": 0.0014,
      "step": 27800
    },
    {
      "epoch": 2.5401313389273987,
      "grad_norm": 0.010386292822659016,
      "learning_rate": 4.8417448075015196e-05,
      "loss": 0.0014,
      "step": 27850
    },
    {
      "epoch": 2.5446917183509665,
      "grad_norm": 0.021908631548285484,
      "learning_rate": 4.793740199059109e-05,
      "loss": 0.0027,
      "step": 27900
    },
    {
      "epoch": 2.5492520977745348,
      "grad_norm": 4620.9580078125,
      "learning_rate": 4.745735590616699e-05,
      "loss": 0.0013,
      "step": 27950
    },
    {
      "epoch": 2.553812477198103,
      "grad_norm": 0.023728810250759125,
      "learning_rate": 4.697730982174288e-05,
      "loss": 0.002,
      "step": 28000
    },
    {
      "epoch": 2.558372856621671,
      "grad_norm": 0.08665106445550919,
      "learning_rate": 4.6497263737318774e-05,
      "loss": 0.0027,
      "step": 28050
    },
    {
      "epoch": 2.562933236045239,
      "grad_norm": 0.06493136286735535,
      "learning_rate": 4.601721765289468e-05,
      "loss": 0.0019,
      "step": 28100
    },
    {
      "epoch": 2.5674936154688073,
      "grad_norm": 550.1314697265625,
      "learning_rate": 4.553717156847057e-05,
      "loss": 0.0018,
      "step": 28150
    },
    {
      "epoch": 2.572053994892375,
      "grad_norm": 11207.78515625,
      "learning_rate": 4.5057125484046466e-05,
      "loss": 0.0021,
      "step": 28200
    },
    {
      "epoch": 2.576614374315943,
      "grad_norm": 0.03361942991614342,
      "learning_rate": 4.457707939962236e-05,
      "loss": 0.0011,
      "step": 28250
    },
    {
      "epoch": 2.581174753739511,
      "grad_norm": 6476.5341796875,
      "learning_rate": 4.409703331519826e-05,
      "loss": 0.0018,
      "step": 28300
    },
    {
      "epoch": 2.5857351331630793,
      "grad_norm": 0.2759189009666443,
      "learning_rate": 4.361698723077415e-05,
      "loss": 0.0025,
      "step": 28350
    },
    {
      "epoch": 2.590295512586647,
      "grad_norm": 0.03275848180055618,
      "learning_rate": 4.3136941146350043e-05,
      "loss": 0.0014,
      "step": 28400
    },
    {
      "epoch": 2.5948558920102154,
      "grad_norm": 0.00689814705401659,
      "learning_rate": 4.265689506192594e-05,
      "loss": 0.0021,
      "step": 28450
    },
    {
      "epoch": 2.599416271433783,
      "grad_norm": 0.011799314990639687,
      "learning_rate": 4.2176848977501836e-05,
      "loss": 0.0014,
      "step": 28500
    },
    {
      "epoch": 2.6039766508573514,
      "grad_norm": 0.049140769988298416,
      "learning_rate": 4.169680289307773e-05,
      "loss": 0.0019,
      "step": 28550
    },
    {
      "epoch": 2.608537030280919,
      "grad_norm": 0.022630590945482254,
      "learning_rate": 4.121675680865363e-05,
      "loss": 0.0011,
      "step": 28600
    },
    {
      "epoch": 2.6130974097044875,
      "grad_norm": 5372.22900390625,
      "learning_rate": 4.073671072422952e-05,
      "loss": 0.0022,
      "step": 28650
    },
    {
      "epoch": 2.6176577891280557,
      "grad_norm": 48290.84765625,
      "learning_rate": 4.0256664639805414e-05,
      "loss": 0.0031,
      "step": 28700
    },
    {
      "epoch": 2.6222181685516235,
      "grad_norm": 0.006928108632564545,
      "learning_rate": 3.9776618555381306e-05,
      "loss": 0.0021,
      "step": 28750
    },
    {
      "epoch": 2.6267785479751913,
      "grad_norm": 0.020098509266972542,
      "learning_rate": 3.929657247095721e-05,
      "loss": 0.001,
      "step": 28800
    },
    {
      "epoch": 2.6313389273987595,
      "grad_norm": 0.03649640455842018,
      "learning_rate": 3.8816526386533106e-05,
      "loss": 0.003,
      "step": 28850
    },
    {
      "epoch": 2.6358993068223278,
      "grad_norm": 0.004097095225006342,
      "learning_rate": 3.8336480302109e-05,
      "loss": 0.0023,
      "step": 28900
    },
    {
      "epoch": 2.6404596862458956,
      "grad_norm": 0.058576878160238266,
      "learning_rate": 3.78564342176849e-05,
      "loss": 0.0024,
      "step": 28950
    },
    {
      "epoch": 2.645020065669464,
      "grad_norm": 27237.83984375,
      "learning_rate": 3.737638813326079e-05,
      "loss": 0.0032,
      "step": 29000
    },
    {
      "epoch": 2.6495804450930316,
      "grad_norm": 0.24368338286876678,
      "learning_rate": 3.689634204883669e-05,
      "loss": 0.0017,
      "step": 29050
    },
    {
      "epoch": 2.6541408245166,
      "grad_norm": 4583.61279296875,
      "learning_rate": 3.641629596441258e-05,
      "loss": 0.0019,
      "step": 29100
    },
    {
      "epoch": 2.6587012039401676,
      "grad_norm": 0.022848447784781456,
      "learning_rate": 3.5936249879988476e-05,
      "loss": 0.0011,
      "step": 29150
    },
    {
      "epoch": 2.663261583363736,
      "grad_norm": 1257.57861328125,
      "learning_rate": 3.545620379556437e-05,
      "loss": 0.0022,
      "step": 29200
    },
    {
      "epoch": 2.667821962787304,
      "grad_norm": 0.08393263816833496,
      "learning_rate": 3.497615771114027e-05,
      "loss": 0.0014,
      "step": 29250
    },
    {
      "epoch": 2.672382342210872,
      "grad_norm": 0.0650704950094223,
      "learning_rate": 3.449611162671616e-05,
      "loss": 0.001,
      "step": 29300
    },
    {
      "epoch": 2.67694272163444,
      "grad_norm": 8.02863597869873,
      "learning_rate": 3.4016065542292054e-05,
      "loss": 0.0019,
      "step": 29350
    },
    {
      "epoch": 2.681503101058008,
      "grad_norm": 0.18527330458164215,
      "learning_rate": 3.353601945786795e-05,
      "loss": 0.0019,
      "step": 29400
    },
    {
      "epoch": 2.686063480481576,
      "grad_norm": 12443.419921875,
      "learning_rate": 3.3055973373443846e-05,
      "loss": 0.0023,
      "step": 29450
    },
    {
      "epoch": 2.690623859905144,
      "grad_norm": 1523.5067138671875,
      "learning_rate": 3.2575927289019745e-05,
      "loss": 0.0032,
      "step": 29500
    },
    {
      "epoch": 2.695184239328712,
      "grad_norm": 0.03838145732879639,
      "learning_rate": 3.209588120459564e-05,
      "loss": 0.0019,
      "step": 29550
    },
    {
      "epoch": 2.6997446187522804,
      "grad_norm": 0.0765739232301712,
      "learning_rate": 3.161583512017153e-05,
      "loss": 0.0027,
      "step": 29600
    },
    {
      "epoch": 2.7043049981758482,
      "grad_norm": 12136.9443359375,
      "learning_rate": 3.113578903574743e-05,
      "loss": 0.0019,
      "step": 29650
    },
    {
      "epoch": 2.708865377599416,
      "grad_norm": 128250.5,
      "learning_rate": 3.065574295132332e-05,
      "loss": 0.0018,
      "step": 29700
    },
    {
      "epoch": 2.7134257570229843,
      "grad_norm": 0.008589762263000011,
      "learning_rate": 3.017569686689922e-05,
      "loss": 0.0028,
      "step": 29750
    },
    {
      "epoch": 2.7179861364465525,
      "grad_norm": 10204.5869140625,
      "learning_rate": 2.9695650782475116e-05,
      "loss": 0.0034,
      "step": 29800
    },
    {
      "epoch": 2.7225465158701203,
      "grad_norm": 3.7925214767456055,
      "learning_rate": 2.921560469805101e-05,
      "loss": 0.002,
      "step": 29850
    },
    {
      "epoch": 2.7271068952936885,
      "grad_norm": 0.043364062905311584,
      "learning_rate": 2.8735558613626908e-05,
      "loss": 0.0017,
      "step": 29900
    },
    {
      "epoch": 2.7316672747172563,
      "grad_norm": 0.018397733569145203,
      "learning_rate": 2.82555125292028e-05,
      "loss": 0.0026,
      "step": 29950
    },
    {
      "epoch": 2.7362276541408246,
      "grad_norm": 0.7383148074150085,
      "learning_rate": 2.7775466444778697e-05,
      "loss": 0.002,
      "step": 30000
    },
    {
      "epoch": 2.7407880335643924,
      "grad_norm": 0.07984141260385513,
      "learning_rate": 2.7295420360354593e-05,
      "loss": 0.0024,
      "step": 30050
    },
    {
      "epoch": 2.7453484129879606,
      "grad_norm": 0.037259917706251144,
      "learning_rate": 2.6815374275930486e-05,
      "loss": 0.0021,
      "step": 30100
    },
    {
      "epoch": 2.749908792411529,
      "grad_norm": 0.09658356010913849,
      "learning_rate": 2.6335328191506382e-05,
      "loss": 0.0011,
      "step": 30150
    },
    {
      "epoch": 2.7544691718350967,
      "grad_norm": 0.026694735512137413,
      "learning_rate": 2.5855282107082275e-05,
      "loss": 0.0023,
      "step": 30200
    },
    {
      "epoch": 2.7590295512586644,
      "grad_norm": 10192.3408203125,
      "learning_rate": 2.5375236022658174e-05,
      "loss": 0.0025,
      "step": 30250
    },
    {
      "epoch": 2.7635899306822327,
      "grad_norm": 0.24145261943340302,
      "learning_rate": 2.489518993823407e-05,
      "loss": 0.0032,
      "step": 30300
    },
    {
      "epoch": 2.768150310105801,
      "grad_norm": 0.022287115454673767,
      "learning_rate": 2.4415143853809963e-05,
      "loss": 0.0018,
      "step": 30350
    },
    {
      "epoch": 2.7727106895293687,
      "grad_norm": 0.03029988892376423,
      "learning_rate": 2.393509776938586e-05,
      "loss": 0.0019,
      "step": 30400
    },
    {
      "epoch": 2.777271068952937,
      "grad_norm": 0.011222177185118198,
      "learning_rate": 2.3455051684961752e-05,
      "loss": 0.0024,
      "step": 30450
    },
    {
      "epoch": 2.781831448376505,
      "grad_norm": 34716.90625,
      "learning_rate": 2.297500560053765e-05,
      "loss": 0.0032,
      "step": 30500
    },
    {
      "epoch": 2.786391827800073,
      "grad_norm": 0.01666863076388836,
      "learning_rate": 2.2494959516113545e-05,
      "loss": 0.0022,
      "step": 30550
    },
    {
      "epoch": 2.790952207223641,
      "grad_norm": 0.0694865882396698,
      "learning_rate": 2.2014913431689437e-05,
      "loss": 0.0028,
      "step": 30600
    },
    {
      "epoch": 2.795512586647209,
      "grad_norm": 0.01877935230731964,
      "learning_rate": 2.1534867347265337e-05,
      "loss": 0.0026,
      "step": 30650
    },
    {
      "epoch": 2.8000729660707773,
      "grad_norm": 0.13830885291099548,
      "learning_rate": 2.105482126284123e-05,
      "loss": 0.0025,
      "step": 30700
    },
    {
      "epoch": 2.804633345494345,
      "grad_norm": 0.09926923364400864,
      "learning_rate": 2.0574775178417126e-05,
      "loss": 0.0021,
      "step": 30750
    },
    {
      "epoch": 2.8091937249179133,
      "grad_norm": 0.20454305410385132,
      "learning_rate": 2.0094729093993022e-05,
      "loss": 0.0008,
      "step": 30800
    },
    {
      "epoch": 2.813754104341481,
      "grad_norm": 5911.451171875,
      "learning_rate": 1.9614683009568915e-05,
      "loss": 0.0013,
      "step": 30850
    },
    {
      "epoch": 2.8183144837650493,
      "grad_norm": 0.018621176481246948,
      "learning_rate": 1.913463692514481e-05,
      "loss": 0.0027,
      "step": 30900
    },
    {
      "epoch": 2.822874863188617,
      "grad_norm": 0.07130476087331772,
      "learning_rate": 1.8654590840720707e-05,
      "loss": 0.0017,
      "step": 30950
    },
    {
      "epoch": 2.8274352426121854,
      "grad_norm": 5.5181965827941895,
      "learning_rate": 1.8174544756296603e-05,
      "loss": 0.003,
      "step": 31000
    },
    {
      "epoch": 2.8319956220357536,
      "grad_norm": 0.08535806834697723,
      "learning_rate": 1.7694498671872496e-05,
      "loss": 0.0016,
      "step": 31050
    },
    {
      "epoch": 2.8365560014593214,
      "grad_norm": 14559.0458984375,
      "learning_rate": 1.7214452587448396e-05,
      "loss": 0.0027,
      "step": 31100
    },
    {
      "epoch": 2.841116380882889,
      "grad_norm": 0.12892033159732819,
      "learning_rate": 1.673440650302429e-05,
      "loss": 0.0025,
      "step": 31150
    },
    {
      "epoch": 2.8456767603064574,
      "grad_norm": 1.5229461193084717,
      "learning_rate": 1.6254360418600185e-05,
      "loss": 0.0018,
      "step": 31200
    },
    {
      "epoch": 2.8502371397300257,
      "grad_norm": 0.06333719938993454,
      "learning_rate": 1.5774314334176077e-05,
      "loss": 0.0025,
      "step": 31250
    },
    {
      "epoch": 2.8547975191535935,
      "grad_norm": 5084.66357421875,
      "learning_rate": 1.5294268249751977e-05,
      "loss": 0.0019,
      "step": 31300
    },
    {
      "epoch": 2.8593578985771617,
      "grad_norm": 0.04376747086644173,
      "learning_rate": 1.4814222165327871e-05,
      "loss": 0.0012,
      "step": 31350
    },
    {
      "epoch": 2.8639182780007295,
      "grad_norm": 0.07859046012163162,
      "learning_rate": 1.4334176080903766e-05,
      "loss": 0.0015,
      "step": 31400
    },
    {
      "epoch": 2.8684786574242978,
      "grad_norm": 0.04258549213409424,
      "learning_rate": 1.385412999647966e-05,
      "loss": 0.0018,
      "step": 31450
    },
    {
      "epoch": 2.8730390368478655,
      "grad_norm": 0.24253404140472412,
      "learning_rate": 1.3374083912055555e-05,
      "loss": 0.002,
      "step": 31500
    },
    {
      "epoch": 2.877599416271434,
      "grad_norm": 0.8886758089065552,
      "learning_rate": 1.2894037827631453e-05,
      "loss": 0.0012,
      "step": 31550
    },
    {
      "epoch": 2.882159795695002,
      "grad_norm": 0.7611606121063232,
      "learning_rate": 1.2413991743207347e-05,
      "loss": 0.0021,
      "step": 31600
    },
    {
      "epoch": 2.88672017511857,
      "grad_norm": 0.04321696609258652,
      "learning_rate": 1.1933945658783242e-05,
      "loss": 0.0018,
      "step": 31650
    },
    {
      "epoch": 2.891280554542138,
      "grad_norm": 0.10712961107492447,
      "learning_rate": 1.1453899574359138e-05,
      "loss": 0.0024,
      "step": 31700
    },
    {
      "epoch": 2.895840933965706,
      "grad_norm": 0.06838704645633698,
      "learning_rate": 1.0973853489935032e-05,
      "loss": 0.0031,
      "step": 31750
    },
    {
      "epoch": 2.900401313389274,
      "grad_norm": 0.01762707531452179,
      "learning_rate": 1.0493807405510928e-05,
      "loss": 0.0017,
      "step": 31800
    },
    {
      "epoch": 2.904961692812842,
      "grad_norm": 0.34210729598999023,
      "learning_rate": 1.0013761321086825e-05,
      "loss": 0.004,
      "step": 31850
    },
    {
      "epoch": 2.90952207223641,
      "grad_norm": 0.01755433715879917,
      "learning_rate": 9.533715236662719e-06,
      "loss": 0.002,
      "step": 31900
    },
    {
      "epoch": 2.9140824516599784,
      "grad_norm": 0.04298795014619827,
      "learning_rate": 9.053669152238615e-06,
      "loss": 0.0018,
      "step": 31950
    },
    {
      "epoch": 2.918642831083546,
      "grad_norm": 0.9341235756874084,
      "learning_rate": 8.57362306781451e-06,
      "loss": 0.0028,
      "step": 32000
    },
    {
      "epoch": 2.923203210507114,
      "grad_norm": 17042.546875,
      "learning_rate": 8.093576983390404e-06,
      "loss": 0.0027,
      "step": 32050
    },
    {
      "epoch": 2.927763589930682,
      "grad_norm": 41374.9140625,
      "learning_rate": 7.6135308989663e-06,
      "loss": 0.0015,
      "step": 32100
    },
    {
      "epoch": 2.9323239693542504,
      "grad_norm": 0.13370466232299805,
      "learning_rate": 7.133484814542195e-06,
      "loss": 0.0009,
      "step": 32150
    },
    {
      "epoch": 2.9368843487778182,
      "grad_norm": 0.823557436466217,
      "learning_rate": 6.653438730118091e-06,
      "loss": 0.0015,
      "step": 32200
    },
    {
      "epoch": 2.9414447282013865,
      "grad_norm": 5540.50927734375,
      "learning_rate": 6.173392645693986e-06,
      "loss": 0.0026,
      "step": 32250
    },
    {
      "epoch": 2.9460051076249543,
      "grad_norm": 0.02236560359597206,
      "learning_rate": 5.693346561269881e-06,
      "loss": 0.0014,
      "step": 32300
    },
    {
      "epoch": 2.9505654870485225,
      "grad_norm": 10214.837890625,
      "learning_rate": 5.213300476845777e-06,
      "loss": 0.0021,
      "step": 32350
    },
    {
      "epoch": 2.9551258664720903,
      "grad_norm": 0.030340339988470078,
      "learning_rate": 4.7332543924216714e-06,
      "loss": 0.0027,
      "step": 32400
    },
    {
      "epoch": 2.9596862458956585,
      "grad_norm": 0.24063685536384583,
      "learning_rate": 4.253208307997567e-06,
      "loss": 0.0024,
      "step": 32450
    },
    {
      "epoch": 2.964246625319227,
      "grad_norm": 0.02008916065096855,
      "learning_rate": 3.7731622235734625e-06,
      "loss": 0.0015,
      "step": 32500
    },
    {
      "epoch": 2.9688070047427946,
      "grad_norm": 1575.9354248046875,
      "learning_rate": 3.293116139149358e-06,
      "loss": 0.0027,
      "step": 32550
    },
    {
      "epoch": 2.9733673841663624,
      "grad_norm": 11552.0966796875,
      "learning_rate": 2.8130700547252536e-06,
      "loss": 0.0017,
      "step": 32600
    },
    {
      "epoch": 2.9779277635899306,
      "grad_norm": 0.39759451150894165,
      "learning_rate": 2.333023970301149e-06,
      "loss": 0.0029,
      "step": 32650
    },
    {
      "epoch": 2.982488143013499,
      "grad_norm": 5506.13720703125,
      "learning_rate": 1.852977885877044e-06,
      "loss": 0.0023,
      "step": 32700
    },
    {
      "epoch": 2.9870485224370666,
      "grad_norm": 0.0938933715224266,
      "learning_rate": 1.3729318014529393e-06,
      "loss": 0.0016,
      "step": 32750
    },
    {
      "epoch": 2.991608901860635,
      "grad_norm": 22893.466796875,
      "learning_rate": 8.928857170288346e-07,
      "loss": 0.002,
      "step": 32800
    },
    {
      "epoch": 2.996169281284203,
      "grad_norm": 0.15623918175697327,
      "learning_rate": 4.1283963260473e-07,
      "loss": 0.0021,
      "step": 32850
    }
  ],
  "logging_steps": 50,
  "max_steps": 32892,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.891364733852058e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
