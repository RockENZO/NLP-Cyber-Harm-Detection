{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 10964,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004560379423568041,
      "grad_norm": 5191381.0,
      "learning_rate": 8.936170212765958e-06,
      "loss": 31.7092,
      "step": 50
    },
    {
      "epoch": 0.009120758847136081,
      "grad_norm": 2986101.5,
      "learning_rate": 1.8054711246200608e-05,
      "loss": 20.2918,
      "step": 100
    },
    {
      "epoch": 0.013681138270704123,
      "grad_norm": 1009308.5625,
      "learning_rate": 2.7173252279635254e-05,
      "loss": 10.1357,
      "step": 150
    },
    {
      "epoch": 0.018241517694272163,
      "grad_norm": 1024604.0,
      "learning_rate": 3.629179331306991e-05,
      "loss": 4.296,
      "step": 200
    },
    {
      "epoch": 0.022801897117840204,
      "grad_norm": 1150882.625,
      "learning_rate": 4.541033434650456e-05,
      "loss": 2.3434,
      "step": 250
    },
    {
      "epoch": 0.027362276541408246,
      "grad_norm": 917637.0,
      "learning_rate": 5.452887537993921e-05,
      "loss": 1.2138,
      "step": 300
    },
    {
      "epoch": 0.03192265596497629,
      "grad_norm": 352328.40625,
      "learning_rate": 6.364741641337386e-05,
      "loss": 0.4973,
      "step": 350
    },
    {
      "epoch": 0.036483035388544326,
      "grad_norm": 94084.1328125,
      "learning_rate": 7.276595744680851e-05,
      "loss": 0.1717,
      "step": 400
    },
    {
      "epoch": 0.04104341481211237,
      "grad_norm": 41576.3203125,
      "learning_rate": 8.188449848024316e-05,
      "loss": 0.0748,
      "step": 450
    },
    {
      "epoch": 0.04560379423568041,
      "grad_norm": 11227.4306640625,
      "learning_rate": 9.100303951367781e-05,
      "loss": 0.0403,
      "step": 500
    },
    {
      "epoch": 0.05016417365924845,
      "grad_norm": 12313.716796875,
      "learning_rate": 0.00010012158054711244,
      "loss": 0.0249,
      "step": 550
    },
    {
      "epoch": 0.05472455308281649,
      "grad_norm": 20233.6953125,
      "learning_rate": 0.0001092401215805471,
      "loss": 0.0176,
      "step": 600
    },
    {
      "epoch": 0.05928493250638453,
      "grad_norm": 15492.6396484375,
      "learning_rate": 0.00011835866261398174,
      "loss": 0.0116,
      "step": 650
    },
    {
      "epoch": 0.06384531192995258,
      "grad_norm": 7781.10986328125,
      "learning_rate": 0.0001274772036474164,
      "loss": 0.0092,
      "step": 700
    },
    {
      "epoch": 0.0684056913535206,
      "grad_norm": 5014.93408203125,
      "learning_rate": 0.00013659574468085104,
      "loss": 0.0093,
      "step": 750
    },
    {
      "epoch": 0.07296607077708865,
      "grad_norm": 10414.806640625,
      "learning_rate": 0.0001457142857142857,
      "loss": 0.0076,
      "step": 800
    },
    {
      "epoch": 0.0775264502006567,
      "grad_norm": 5240.27099609375,
      "learning_rate": 0.00015483282674772034,
      "loss": 0.0062,
      "step": 850
    },
    {
      "epoch": 0.08208682962422474,
      "grad_norm": 4882.24755859375,
      "learning_rate": 0.000163951367781155,
      "loss": 0.0062,
      "step": 900
    },
    {
      "epoch": 0.08664720904779277,
      "grad_norm": 6396.0517578125,
      "learning_rate": 0.00017306990881458964,
      "loss": 0.0056,
      "step": 950
    },
    {
      "epoch": 0.09120758847136082,
      "grad_norm": 6876.47509765625,
      "learning_rate": 0.0001821884498480243,
      "loss": 0.0052,
      "step": 1000
    },
    {
      "epoch": 0.09576796789492886,
      "grad_norm": 1745.3070068359375,
      "learning_rate": 0.00019130699088145894,
      "loss": 0.0048,
      "step": 1050
    },
    {
      "epoch": 0.1003283473184969,
      "grad_norm": 2925.928466796875,
      "learning_rate": 0.0002004255319148936,
      "loss": 0.005,
      "step": 1100
    },
    {
      "epoch": 0.10488872674206494,
      "grad_norm": 3129.099365234375,
      "learning_rate": 0.00020954407294832824,
      "loss": 0.0045,
      "step": 1150
    },
    {
      "epoch": 0.10944910616563298,
      "grad_norm": 4802.99609375,
      "learning_rate": 0.0002186626139817629,
      "loss": 0.0043,
      "step": 1200
    },
    {
      "epoch": 0.11400948558920102,
      "grad_norm": 3725.22705078125,
      "learning_rate": 0.00022778115501519757,
      "loss": 0.0038,
      "step": 1250
    },
    {
      "epoch": 0.11856986501276906,
      "grad_norm": 2246.4873046875,
      "learning_rate": 0.0002368996960486322,
      "loss": 0.0046,
      "step": 1300
    },
    {
      "epoch": 0.1231302444363371,
      "grad_norm": 1279.67822265625,
      "learning_rate": 0.00024601823708206684,
      "loss": 0.005,
      "step": 1350
    },
    {
      "epoch": 0.12769062385990515,
      "grad_norm": 4941.34375,
      "learning_rate": 0.0002551367781155015,
      "loss": 0.0042,
      "step": 1400
    },
    {
      "epoch": 0.1322510032834732,
      "grad_norm": 11591.8203125,
      "learning_rate": 0.00026425531914893614,
      "loss": 0.0044,
      "step": 1450
    },
    {
      "epoch": 0.1368113827070412,
      "grad_norm": 2826.35400390625,
      "learning_rate": 0.0002733738601823708,
      "loss": 0.0053,
      "step": 1500
    },
    {
      "epoch": 0.14137176213060926,
      "grad_norm": 4483.15283203125,
      "learning_rate": 0.00028249240121580544,
      "loss": 0.0041,
      "step": 1550
    },
    {
      "epoch": 0.1459321415541773,
      "grad_norm": 6124.10595703125,
      "learning_rate": 0.0002916109422492401,
      "loss": 0.0039,
      "step": 1600
    },
    {
      "epoch": 0.15049252097774535,
      "grad_norm": 3820.394287109375,
      "learning_rate": 0.000299961596313246,
      "loss": 0.0039,
      "step": 1650
    },
    {
      "epoch": 0.1550529004013134,
      "grad_norm": 5245.4638671875,
      "learning_rate": 0.00029948155022882197,
      "loss": 0.0036,
      "step": 1700
    },
    {
      "epoch": 0.15961327982488144,
      "grad_norm": 5578.05712890625,
      "learning_rate": 0.00029900150414439785,
      "loss": 0.0043,
      "step": 1750
    },
    {
      "epoch": 0.16417365924844948,
      "grad_norm": 18701.275390625,
      "learning_rate": 0.00029852145805997374,
      "loss": 0.0045,
      "step": 1800
    },
    {
      "epoch": 0.1687340386720175,
      "grad_norm": 2198.85107421875,
      "learning_rate": 0.0002980414119755496,
      "loss": 0.0043,
      "step": 1850
    },
    {
      "epoch": 0.17329441809558555,
      "grad_norm": 4847.12890625,
      "learning_rate": 0.0002975613658911255,
      "loss": 0.0052,
      "step": 1900
    },
    {
      "epoch": 0.1778547975191536,
      "grad_norm": 776.0476684570312,
      "learning_rate": 0.00029708131980670145,
      "loss": 0.0028,
      "step": 1950
    },
    {
      "epoch": 0.18241517694272164,
      "grad_norm": 1443.686767578125,
      "learning_rate": 0.0002966012737222773,
      "loss": 0.003,
      "step": 2000
    },
    {
      "epoch": 0.18697555636628968,
      "grad_norm": 3676.90478515625,
      "learning_rate": 0.0002961212276378532,
      "loss": 0.0039,
      "step": 2050
    },
    {
      "epoch": 0.19153593578985773,
      "grad_norm": 40.87212371826172,
      "learning_rate": 0.0002956411815534291,
      "loss": 0.0034,
      "step": 2100
    },
    {
      "epoch": 0.19609631521342577,
      "grad_norm": 6979.63818359375,
      "learning_rate": 0.000295161135469005,
      "loss": 0.0037,
      "step": 2150
    },
    {
      "epoch": 0.2006566946369938,
      "grad_norm": 3042.655517578125,
      "learning_rate": 0.0002946810893845809,
      "loss": 0.004,
      "step": 2200
    },
    {
      "epoch": 0.20521707406056183,
      "grad_norm": 11051.544921875,
      "learning_rate": 0.00029420104330015677,
      "loss": 0.0031,
      "step": 2250
    },
    {
      "epoch": 0.20977745348412988,
      "grad_norm": 1560.3939208984375,
      "learning_rate": 0.0002937209972157327,
      "loss": 0.0035,
      "step": 2300
    },
    {
      "epoch": 0.21433783290769792,
      "grad_norm": 1301.023193359375,
      "learning_rate": 0.0002932409511313086,
      "loss": 0.0027,
      "step": 2350
    },
    {
      "epoch": 0.21889821233126597,
      "grad_norm": 4049.370361328125,
      "learning_rate": 0.0002927609050468845,
      "loss": 0.0034,
      "step": 2400
    },
    {
      "epoch": 0.223458591754834,
      "grad_norm": 3727.81005859375,
      "learning_rate": 0.00029228085896246036,
      "loss": 0.0039,
      "step": 2450
    },
    {
      "epoch": 0.22801897117840203,
      "grad_norm": 23.087020874023438,
      "learning_rate": 0.00029180081287803625,
      "loss": 0.0025,
      "step": 2500
    },
    {
      "epoch": 0.23257935060197008,
      "grad_norm": 6355.607421875,
      "learning_rate": 0.0002913207667936122,
      "loss": 0.0039,
      "step": 2550
    },
    {
      "epoch": 0.23713973002553812,
      "grad_norm": 5175.6767578125,
      "learning_rate": 0.000290840720709188,
      "loss": 0.0029,
      "step": 2600
    },
    {
      "epoch": 0.24170010944910617,
      "grad_norm": 3428.44677734375,
      "learning_rate": 0.00029036067462476396,
      "loss": 0.0037,
      "step": 2650
    },
    {
      "epoch": 0.2462604888726742,
      "grad_norm": 1918.808837890625,
      "learning_rate": 0.00028988062854033985,
      "loss": 0.0038,
      "step": 2700
    },
    {
      "epoch": 0.2508208682962422,
      "grad_norm": 873.9747314453125,
      "learning_rate": 0.00028940058245591573,
      "loss": 0.0031,
      "step": 2750
    },
    {
      "epoch": 0.2553812477198103,
      "grad_norm": 894.5855712890625,
      "learning_rate": 0.0002889205363714917,
      "loss": 0.0029,
      "step": 2800
    },
    {
      "epoch": 0.2599416271433783,
      "grad_norm": 3663.469970703125,
      "learning_rate": 0.0002884404902870675,
      "loss": 0.004,
      "step": 2850
    },
    {
      "epoch": 0.2645020065669464,
      "grad_norm": 5029.2021484375,
      "learning_rate": 0.00028796044420264345,
      "loss": 0.0029,
      "step": 2900
    },
    {
      "epoch": 0.2690623859905144,
      "grad_norm": 1186.6944580078125,
      "learning_rate": 0.00028748039811821933,
      "loss": 0.0033,
      "step": 2950
    },
    {
      "epoch": 0.2736227654140824,
      "grad_norm": 1336.5592041015625,
      "learning_rate": 0.0002870003520337952,
      "loss": 0.0038,
      "step": 3000
    },
    {
      "epoch": 0.2781831448376505,
      "grad_norm": 162.01199340820312,
      "learning_rate": 0.0002865203059493711,
      "loss": 0.0036,
      "step": 3050
    },
    {
      "epoch": 0.2827435242612185,
      "grad_norm": 2847.91796875,
      "learning_rate": 0.000286040259864947,
      "loss": 0.0035,
      "step": 3100
    },
    {
      "epoch": 0.2873039036847866,
      "grad_norm": 3522.3447265625,
      "learning_rate": 0.00028556021378052293,
      "loss": 0.0036,
      "step": 3150
    },
    {
      "epoch": 0.2918642831083546,
      "grad_norm": 7830.3466796875,
      "learning_rate": 0.0002850801676960988,
      "loss": 0.0026,
      "step": 3200
    },
    {
      "epoch": 0.2964246625319227,
      "grad_norm": 8993.009765625,
      "learning_rate": 0.0002846001216116747,
      "loss": 0.0035,
      "step": 3250
    },
    {
      "epoch": 0.3009850419554907,
      "grad_norm": 4850.8564453125,
      "learning_rate": 0.0002841200755272506,
      "loss": 0.0037,
      "step": 3300
    },
    {
      "epoch": 0.3055454213790587,
      "grad_norm": 6726.22412109375,
      "learning_rate": 0.0002836400294428265,
      "loss": 0.0041,
      "step": 3350
    },
    {
      "epoch": 0.3101058008026268,
      "grad_norm": 4995.31396484375,
      "learning_rate": 0.0002831599833584024,
      "loss": 0.004,
      "step": 3400
    },
    {
      "epoch": 0.3146661802261948,
      "grad_norm": 13345.486328125,
      "learning_rate": 0.0002826799372739783,
      "loss": 0.0032,
      "step": 3450
    },
    {
      "epoch": 0.3192265596497629,
      "grad_norm": 2827.42431640625,
      "learning_rate": 0.0002821998911895542,
      "loss": 0.0025,
      "step": 3500
    },
    {
      "epoch": 0.3237869390733309,
      "grad_norm": 12352.2509765625,
      "learning_rate": 0.00028171984510513007,
      "loss": 0.0036,
      "step": 3550
    },
    {
      "epoch": 0.32834731849689897,
      "grad_norm": 6555.58349609375,
      "learning_rate": 0.00028123979902070596,
      "loss": 0.0036,
      "step": 3600
    },
    {
      "epoch": 0.332907697920467,
      "grad_norm": 5263.6025390625,
      "learning_rate": 0.00028075975293628184,
      "loss": 0.0028,
      "step": 3650
    },
    {
      "epoch": 0.337468077344035,
      "grad_norm": 4955.0693359375,
      "learning_rate": 0.00028027970685185773,
      "loss": 0.0039,
      "step": 3700
    },
    {
      "epoch": 0.3420284567676031,
      "grad_norm": 4084.581787109375,
      "learning_rate": 0.00027979966076743367,
      "loss": 0.0028,
      "step": 3750
    },
    {
      "epoch": 0.3465888361911711,
      "grad_norm": 52.824527740478516,
      "learning_rate": 0.00027931961468300956,
      "loss": 0.0024,
      "step": 3800
    },
    {
      "epoch": 0.35114921561473916,
      "grad_norm": 36.61239242553711,
      "learning_rate": 0.00027883956859858544,
      "loss": 0.004,
      "step": 3850
    },
    {
      "epoch": 0.3557095950383072,
      "grad_norm": 8609.90625,
      "learning_rate": 0.00027835952251416133,
      "loss": 0.0029,
      "step": 3900
    },
    {
      "epoch": 0.36026997446187525,
      "grad_norm": 7232.01708984375,
      "learning_rate": 0.0002778794764297372,
      "loss": 0.0029,
      "step": 3950
    },
    {
      "epoch": 0.36483035388544327,
      "grad_norm": 3892.7958984375,
      "learning_rate": 0.0002773994303453131,
      "loss": 0.0037,
      "step": 4000
    },
    {
      "epoch": 0.3693907333090113,
      "grad_norm": 7876.177734375,
      "learning_rate": 0.00027691938426088904,
      "loss": 0.0032,
      "step": 4050
    },
    {
      "epoch": 0.37395111273257936,
      "grad_norm": 111.283203125,
      "learning_rate": 0.0002764393381764649,
      "loss": 0.004,
      "step": 4100
    },
    {
      "epoch": 0.3785114921561474,
      "grad_norm": 2361.9755859375,
      "learning_rate": 0.0002759592920920408,
      "loss": 0.0044,
      "step": 4150
    },
    {
      "epoch": 0.38307187157971545,
      "grad_norm": 75.35592651367188,
      "learning_rate": 0.0002754792460076167,
      "loss": 0.003,
      "step": 4200
    },
    {
      "epoch": 0.38763225100328347,
      "grad_norm": 8587.8662109375,
      "learning_rate": 0.0002749991999231926,
      "loss": 0.0036,
      "step": 4250
    },
    {
      "epoch": 0.39219263042685154,
      "grad_norm": 9101.6474609375,
      "learning_rate": 0.0002745191538387685,
      "loss": 0.0036,
      "step": 4300
    },
    {
      "epoch": 0.39675300985041956,
      "grad_norm": 8242.1396484375,
      "learning_rate": 0.00027403910775434436,
      "loss": 0.0039,
      "step": 4350
    },
    {
      "epoch": 0.4013133892739876,
      "grad_norm": 7261.2919921875,
      "learning_rate": 0.0002735590616699203,
      "loss": 0.0036,
      "step": 4400
    },
    {
      "epoch": 0.40587376869755565,
      "grad_norm": 11546.7353515625,
      "learning_rate": 0.0002730790155854962,
      "loss": 0.0022,
      "step": 4450
    },
    {
      "epoch": 0.41043414812112367,
      "grad_norm": 13710.5146484375,
      "learning_rate": 0.00027259896950107207,
      "loss": 0.005,
      "step": 4500
    },
    {
      "epoch": 0.41499452754469174,
      "grad_norm": 10107.23828125,
      "learning_rate": 0.000272118923416648,
      "loss": 0.0037,
      "step": 4550
    },
    {
      "epoch": 0.41955490696825976,
      "grad_norm": 9945.3603515625,
      "learning_rate": 0.00027163887733222384,
      "loss": 0.0043,
      "step": 4600
    },
    {
      "epoch": 0.4241152863918278,
      "grad_norm": 223.8675079345703,
      "learning_rate": 0.0002711588312477998,
      "loss": 0.0032,
      "step": 4650
    },
    {
      "epoch": 0.42867566581539585,
      "grad_norm": 7325.44482421875,
      "learning_rate": 0.00027067878516337567,
      "loss": 0.0044,
      "step": 4700
    },
    {
      "epoch": 0.43323604523896386,
      "grad_norm": 18206.541015625,
      "learning_rate": 0.00027019873907895155,
      "loss": 0.0044,
      "step": 4750
    },
    {
      "epoch": 0.43779642466253194,
      "grad_norm": 19797.138671875,
      "learning_rate": 0.00026971869299452744,
      "loss": 0.0036,
      "step": 4800
    },
    {
      "epoch": 0.44235680408609995,
      "grad_norm": 25315.513671875,
      "learning_rate": 0.0002692386469101033,
      "loss": 0.003,
      "step": 4850
    },
    {
      "epoch": 0.446917183509668,
      "grad_norm": 71.27204132080078,
      "learning_rate": 0.00026875860082567927,
      "loss": 0.0051,
      "step": 4900
    },
    {
      "epoch": 0.45147756293323604,
      "grad_norm": 94.12933349609375,
      "learning_rate": 0.00026827855474125515,
      "loss": 0.004,
      "step": 4950
    },
    {
      "epoch": 0.45603794235680406,
      "grad_norm": 20835.52734375,
      "learning_rate": 0.00026779850865683104,
      "loss": 0.0028,
      "step": 5000
    },
    {
      "epoch": 0.46059832178037213,
      "grad_norm": 10.855692863464355,
      "learning_rate": 0.0002673184625724069,
      "loss": 0.0038,
      "step": 5050
    },
    {
      "epoch": 0.46515870120394015,
      "grad_norm": 14723.458984375,
      "learning_rate": 0.0002668384164879828,
      "loss": 0.0033,
      "step": 5100
    },
    {
      "epoch": 0.4697190806275082,
      "grad_norm": 9277.3291015625,
      "learning_rate": 0.00026635837040355875,
      "loss": 0.0042,
      "step": 5150
    },
    {
      "epoch": 0.47427946005107624,
      "grad_norm": 3981.341796875,
      "learning_rate": 0.0002658783243191346,
      "loss": 0.0055,
      "step": 5200
    },
    {
      "epoch": 0.4788398394746443,
      "grad_norm": 9875.7001953125,
      "learning_rate": 0.0002653982782347105,
      "loss": 0.0031,
      "step": 5250
    },
    {
      "epoch": 0.48340021889821233,
      "grad_norm": 0.7528746724128723,
      "learning_rate": 0.0002649182321502864,
      "loss": 0.004,
      "step": 5300
    },
    {
      "epoch": 0.48796059832178035,
      "grad_norm": 13251.9052734375,
      "learning_rate": 0.0002644381860658623,
      "loss": 0.0067,
      "step": 5350
    },
    {
      "epoch": 0.4925209777453484,
      "grad_norm": 3365.882568359375,
      "learning_rate": 0.00026395813998143823,
      "loss": 0.0028,
      "step": 5400
    },
    {
      "epoch": 0.49708135716891644,
      "grad_norm": 3.9904885292053223,
      "learning_rate": 0.00026347809389701407,
      "loss": 0.0033,
      "step": 5450
    },
    {
      "epoch": 0.5016417365924845,
      "grad_norm": 4237.01953125,
      "learning_rate": 0.00026299804781259,
      "loss": 0.0042,
      "step": 5500
    },
    {
      "epoch": 0.5062021160160526,
      "grad_norm": 0.5582995414733887,
      "learning_rate": 0.0002625180017281659,
      "loss": 0.0049,
      "step": 5550
    },
    {
      "epoch": 0.5107624954396206,
      "grad_norm": 8.926702499389648,
      "learning_rate": 0.0002620379556437418,
      "loss": 0.0051,
      "step": 5600
    },
    {
      "epoch": 0.5153228748631886,
      "grad_norm": 0.11075832694768906,
      "learning_rate": 0.00026155790955931766,
      "loss": 0.0052,
      "step": 5650
    },
    {
      "epoch": 0.5198832542867566,
      "grad_norm": 13742.435546875,
      "learning_rate": 0.00026107786347489355,
      "loss": 0.0063,
      "step": 5700
    },
    {
      "epoch": 0.5244436337103247,
      "grad_norm": 2153.0947265625,
      "learning_rate": 0.0002605978173904695,
      "loss": 0.004,
      "step": 5750
    },
    {
      "epoch": 0.5290040131338928,
      "grad_norm": 9385.3115234375,
      "learning_rate": 0.0002601177713060454,
      "loss": 0.0042,
      "step": 5800
    },
    {
      "epoch": 0.5335643925574608,
      "grad_norm": 6672.72509765625,
      "learning_rate": 0.00025963772522162126,
      "loss": 0.0043,
      "step": 5850
    },
    {
      "epoch": 0.5381247719810288,
      "grad_norm": 9525.65234375,
      "learning_rate": 0.00025915767913719715,
      "loss": 0.0053,
      "step": 5900
    },
    {
      "epoch": 0.5426851514045968,
      "grad_norm": 232.60498046875,
      "learning_rate": 0.00025867763305277303,
      "loss": 0.0037,
      "step": 5950
    },
    {
      "epoch": 0.5472455308281649,
      "grad_norm": 17895.58984375,
      "learning_rate": 0.0002581975869683489,
      "loss": 0.0049,
      "step": 6000
    },
    {
      "epoch": 0.551805910251733,
      "grad_norm": 12737.7802734375,
      "learning_rate": 0.00025771754088392486,
      "loss": 0.0059,
      "step": 6050
    },
    {
      "epoch": 0.556366289675301,
      "grad_norm": 18965.5,
      "learning_rate": 0.00025723749479950075,
      "loss": 0.0043,
      "step": 6100
    },
    {
      "epoch": 0.560926669098869,
      "grad_norm": 4996.62646484375,
      "learning_rate": 0.00025675744871507663,
      "loss": 0.0042,
      "step": 6150
    },
    {
      "epoch": 0.565487048522437,
      "grad_norm": 13.310291290283203,
      "learning_rate": 0.0002562774026306525,
      "loss": 0.0027,
      "step": 6200
    },
    {
      "epoch": 0.5700474279460052,
      "grad_norm": 8628.5615234375,
      "learning_rate": 0.0002557973565462284,
      "loss": 0.0035,
      "step": 6250
    },
    {
      "epoch": 0.5746078073695732,
      "grad_norm": 19231.8046875,
      "learning_rate": 0.0002553173104618043,
      "loss": 0.0032,
      "step": 6300
    },
    {
      "epoch": 0.5791681867931412,
      "grad_norm": 18334.12109375,
      "learning_rate": 0.0002548372643773802,
      "loss": 0.0047,
      "step": 6350
    },
    {
      "epoch": 0.5837285662167092,
      "grad_norm": 31634.16796875,
      "learning_rate": 0.0002543572182929561,
      "loss": 0.0048,
      "step": 6400
    },
    {
      "epoch": 0.5882889456402772,
      "grad_norm": 0.27193963527679443,
      "learning_rate": 0.000253877172208532,
      "loss": 0.0037,
      "step": 6450
    },
    {
      "epoch": 0.5928493250638454,
      "grad_norm": 5134.4482421875,
      "learning_rate": 0.0002533971261241079,
      "loss": 0.0049,
      "step": 6500
    },
    {
      "epoch": 0.5974097044874134,
      "grad_norm": 10275.345703125,
      "learning_rate": 0.0002529170800396838,
      "loss": 0.0057,
      "step": 6550
    },
    {
      "epoch": 0.6019700839109814,
      "grad_norm": 13363.2197265625,
      "learning_rate": 0.00025243703395525966,
      "loss": 0.005,
      "step": 6600
    },
    {
      "epoch": 0.6065304633345494,
      "grad_norm": 993.8096923828125,
      "learning_rate": 0.0002519569878708356,
      "loss": 0.0039,
      "step": 6650
    },
    {
      "epoch": 0.6110908427581174,
      "grad_norm": 5708.0478515625,
      "learning_rate": 0.0002514769417864115,
      "loss": 0.0037,
      "step": 6700
    },
    {
      "epoch": 0.6156512221816856,
      "grad_norm": 7.295849800109863,
      "learning_rate": 0.0002509968957019874,
      "loss": 0.0059,
      "step": 6750
    },
    {
      "epoch": 0.6202116016052536,
      "grad_norm": 10413.23046875,
      "learning_rate": 0.00025051684961756326,
      "loss": 0.0036,
      "step": 6800
    },
    {
      "epoch": 0.6247719810288216,
      "grad_norm": 337.8074645996094,
      "learning_rate": 0.00025003680353313914,
      "loss": 0.0042,
      "step": 6850
    },
    {
      "epoch": 0.6293323604523896,
      "grad_norm": 5516.873046875,
      "learning_rate": 0.0002495567574487151,
      "loss": 0.0042,
      "step": 6900
    },
    {
      "epoch": 0.6338927398759577,
      "grad_norm": 11528.994140625,
      "learning_rate": 0.0002490767113642909,
      "loss": 0.0058,
      "step": 6950
    },
    {
      "epoch": 0.6384531192995258,
      "grad_norm": 9465.6904296875,
      "learning_rate": 0.00024859666527986686,
      "loss": 0.0044,
      "step": 7000
    },
    {
      "epoch": 0.6430134987230938,
      "grad_norm": 6091.9970703125,
      "learning_rate": 0.00024811661919544274,
      "loss": 0.0041,
      "step": 7050
    },
    {
      "epoch": 0.6475738781466618,
      "grad_norm": 2880.179931640625,
      "learning_rate": 0.00024763657311101863,
      "loss": 0.0035,
      "step": 7100
    },
    {
      "epoch": 0.6521342575702298,
      "grad_norm": 14479.642578125,
      "learning_rate": 0.0002471565270265945,
      "loss": 0.0038,
      "step": 7150
    },
    {
      "epoch": 0.6566946369937979,
      "grad_norm": 20063.322265625,
      "learning_rate": 0.0002466764809421704,
      "loss": 0.0039,
      "step": 7200
    },
    {
      "epoch": 0.661255016417366,
      "grad_norm": 14122.59375,
      "learning_rate": 0.00024619643485774634,
      "loss": 0.005,
      "step": 7250
    },
    {
      "epoch": 0.665815395840934,
      "grad_norm": 0.047965772449970245,
      "learning_rate": 0.00024571638877332223,
      "loss": 0.0045,
      "step": 7300
    },
    {
      "epoch": 0.670375775264502,
      "grad_norm": 7790.6357421875,
      "learning_rate": 0.0002452363426888981,
      "loss": 0.0038,
      "step": 7350
    },
    {
      "epoch": 0.67493615468807,
      "grad_norm": 8388.1767578125,
      "learning_rate": 0.000244756296604474,
      "loss": 0.004,
      "step": 7400
    },
    {
      "epoch": 0.6794965341116381,
      "grad_norm": 39.791282653808594,
      "learning_rate": 0.0002442762505200499,
      "loss": 0.0033,
      "step": 7450
    },
    {
      "epoch": 0.6840569135352061,
      "grad_norm": 19.19691276550293,
      "learning_rate": 0.0002437962044356258,
      "loss": 0.0043,
      "step": 7500
    },
    {
      "epoch": 0.6886172929587742,
      "grad_norm": 0.6072261333465576,
      "learning_rate": 0.00024331615835120168,
      "loss": 0.0039,
      "step": 7550
    },
    {
      "epoch": 0.6931776723823422,
      "grad_norm": 12369.353515625,
      "learning_rate": 0.0002428361122667776,
      "loss": 0.0047,
      "step": 7600
    },
    {
      "epoch": 0.6977380518059102,
      "grad_norm": 0.123549684882164,
      "learning_rate": 0.00024235606618235346,
      "loss": 0.0043,
      "step": 7650
    },
    {
      "epoch": 0.7022984312294783,
      "grad_norm": 830.545654296875,
      "learning_rate": 0.00024187602009792937,
      "loss": 0.0048,
      "step": 7700
    },
    {
      "epoch": 0.7068588106530463,
      "grad_norm": 7564.1455078125,
      "learning_rate": 0.00024139597401350528,
      "loss": 0.0052,
      "step": 7750
    },
    {
      "epoch": 0.7114191900766144,
      "grad_norm": 7156.69873046875,
      "learning_rate": 0.00024091592792908117,
      "loss": 0.0044,
      "step": 7800
    },
    {
      "epoch": 0.7159795695001824,
      "grad_norm": 2605.3271484375,
      "learning_rate": 0.00024043588184465708,
      "loss": 0.0036,
      "step": 7850
    },
    {
      "epoch": 0.7205399489237505,
      "grad_norm": 506.2583312988281,
      "learning_rate": 0.00023995583576023294,
      "loss": 0.0045,
      "step": 7900
    },
    {
      "epoch": 0.7251003283473185,
      "grad_norm": 189.6920166015625,
      "learning_rate": 0.00023947578967580885,
      "loss": 0.0043,
      "step": 7950
    },
    {
      "epoch": 0.7296607077708865,
      "grad_norm": 8482.74609375,
      "learning_rate": 0.00023899574359138474,
      "loss": 0.0041,
      "step": 8000
    },
    {
      "epoch": 0.7342210871944546,
      "grad_norm": 0.17709635198116302,
      "learning_rate": 0.00023851569750696065,
      "loss": 0.0047,
      "step": 8050
    },
    {
      "epoch": 0.7387814666180226,
      "grad_norm": 5972.11865234375,
      "learning_rate": 0.00023803565142253657,
      "loss": 0.0042,
      "step": 8100
    },
    {
      "epoch": 0.7433418460415907,
      "grad_norm": 8571.2431640625,
      "learning_rate": 0.00023755560533811242,
      "loss": 0.0026,
      "step": 8150
    },
    {
      "epoch": 0.7479022254651587,
      "grad_norm": 25.185789108276367,
      "learning_rate": 0.00023707555925368834,
      "loss": 0.0037,
      "step": 8200
    },
    {
      "epoch": 0.7524626048887267,
      "grad_norm": 11149.662109375,
      "learning_rate": 0.00023659551316926422,
      "loss": 0.0048,
      "step": 8250
    },
    {
      "epoch": 0.7570229843122948,
      "grad_norm": 46.00790023803711,
      "learning_rate": 0.00023611546708484014,
      "loss": 0.0032,
      "step": 8300
    },
    {
      "epoch": 0.7615833637358628,
      "grad_norm": 7713.6103515625,
      "learning_rate": 0.00023563542100041602,
      "loss": 0.0048,
      "step": 8350
    },
    {
      "epoch": 0.7661437431594309,
      "grad_norm": 0.11827582120895386,
      "learning_rate": 0.0002351553749159919,
      "loss": 0.0033,
      "step": 8400
    },
    {
      "epoch": 0.7707041225829989,
      "grad_norm": 3009.319091796875,
      "learning_rate": 0.00023467532883156782,
      "loss": 0.0056,
      "step": 8450
    },
    {
      "epoch": 0.7752645020065669,
      "grad_norm": 4807.74755859375,
      "learning_rate": 0.0002341952827471437,
      "loss": 0.0043,
      "step": 8500
    },
    {
      "epoch": 0.779824881430135,
      "grad_norm": 21775.80078125,
      "learning_rate": 0.0002337152366627196,
      "loss": 0.0062,
      "step": 8550
    },
    {
      "epoch": 0.7843852608537031,
      "grad_norm": 15864.9794921875,
      "learning_rate": 0.00023323519057829548,
      "loss": 0.0033,
      "step": 8600
    },
    {
      "epoch": 0.7889456402772711,
      "grad_norm": 2254.37646484375,
      "learning_rate": 0.0002327551444938714,
      "loss": 0.0026,
      "step": 8650
    },
    {
      "epoch": 0.7935060197008391,
      "grad_norm": 37.41449737548828,
      "learning_rate": 0.0002322750984094473,
      "loss": 0.0051,
      "step": 8700
    },
    {
      "epoch": 0.7980663991244071,
      "grad_norm": 16913.279296875,
      "learning_rate": 0.00023179505232502317,
      "loss": 0.0043,
      "step": 8750
    },
    {
      "epoch": 0.8026267785479752,
      "grad_norm": 47.316707611083984,
      "learning_rate": 0.00023131500624059908,
      "loss": 0.0046,
      "step": 8800
    },
    {
      "epoch": 0.8071871579715433,
      "grad_norm": 0.2803684175014496,
      "learning_rate": 0.00023083496015617496,
      "loss": 0.0045,
      "step": 8850
    },
    {
      "epoch": 0.8117475373951113,
      "grad_norm": 13520.3037109375,
      "learning_rate": 0.00023035491407175088,
      "loss": 0.0045,
      "step": 8900
    },
    {
      "epoch": 0.8163079168186793,
      "grad_norm": 180.1205291748047,
      "learning_rate": 0.00022987486798732674,
      "loss": 0.0038,
      "step": 8950
    },
    {
      "epoch": 0.8208682962422473,
      "grad_norm": 4441.60546875,
      "learning_rate": 0.00022939482190290265,
      "loss": 0.0038,
      "step": 9000
    },
    {
      "epoch": 0.8254286756658153,
      "grad_norm": 0.14862088859081268,
      "learning_rate": 0.00022891477581847856,
      "loss": 0.0039,
      "step": 9050
    },
    {
      "epoch": 0.8299890550893835,
      "grad_norm": 0.36453700065612793,
      "learning_rate": 0.00022843472973405445,
      "loss": 0.0039,
      "step": 9100
    },
    {
      "epoch": 0.8345494345129515,
      "grad_norm": 0.2301269918680191,
      "learning_rate": 0.00022795468364963036,
      "loss": 0.0045,
      "step": 9150
    },
    {
      "epoch": 0.8391098139365195,
      "grad_norm": 3981.968017578125,
      "learning_rate": 0.00022747463756520622,
      "loss": 0.0056,
      "step": 9200
    },
    {
      "epoch": 0.8436701933600875,
      "grad_norm": 0.1579790860414505,
      "learning_rate": 0.00022699459148078213,
      "loss": 0.0033,
      "step": 9250
    },
    {
      "epoch": 0.8482305727836555,
      "grad_norm": 0.4207400381565094,
      "learning_rate": 0.00022651454539635802,
      "loss": 0.003,
      "step": 9300
    },
    {
      "epoch": 0.8527909522072237,
      "grad_norm": 16288.947265625,
      "learning_rate": 0.00022603449931193393,
      "loss": 0.005,
      "step": 9350
    },
    {
      "epoch": 0.8573513316307917,
      "grad_norm": 15083.19140625,
      "learning_rate": 0.00022555445322750985,
      "loss": 0.0036,
      "step": 9400
    },
    {
      "epoch": 0.8619117110543597,
      "grad_norm": 5.15146017074585,
      "learning_rate": 0.0002250744071430857,
      "loss": 0.0042,
      "step": 9450
    },
    {
      "epoch": 0.8664720904779277,
      "grad_norm": 0.22516004741191864,
      "learning_rate": 0.00022459436105866162,
      "loss": 0.0057,
      "step": 9500
    },
    {
      "epoch": 0.8710324699014959,
      "grad_norm": 0.6659408211708069,
      "learning_rate": 0.0002241143149742375,
      "loss": 0.0034,
      "step": 9550
    },
    {
      "epoch": 0.8755928493250639,
      "grad_norm": 5.212887287139893,
      "learning_rate": 0.00022363426888981342,
      "loss": 0.0036,
      "step": 9600
    },
    {
      "epoch": 0.8801532287486319,
      "grad_norm": 667.3731079101562,
      "learning_rate": 0.00022315422280538928,
      "loss": 0.0039,
      "step": 9650
    },
    {
      "epoch": 0.8847136081721999,
      "grad_norm": 0.7396458387374878,
      "learning_rate": 0.0002226741767209652,
      "loss": 0.0055,
      "step": 9700
    },
    {
      "epoch": 0.8892739875957679,
      "grad_norm": 5766.755859375,
      "learning_rate": 0.0002221941306365411,
      "loss": 0.0047,
      "step": 9750
    },
    {
      "epoch": 0.893834367019336,
      "grad_norm": 524.7015991210938,
      "learning_rate": 0.000221714084552117,
      "loss": 0.0041,
      "step": 9800
    },
    {
      "epoch": 0.8983947464429041,
      "grad_norm": 1570.28515625,
      "learning_rate": 0.00022123403846769287,
      "loss": 0.0065,
      "step": 9850
    },
    {
      "epoch": 0.9029551258664721,
      "grad_norm": 24235.71484375,
      "learning_rate": 0.00022075399238326876,
      "loss": 0.0035,
      "step": 9900
    },
    {
      "epoch": 0.9075155052900401,
      "grad_norm": 266.35626220703125,
      "learning_rate": 0.00022027394629884467,
      "loss": 0.0037,
      "step": 9950
    },
    {
      "epoch": 0.9120758847136081,
      "grad_norm": 15455.5244140625,
      "learning_rate": 0.00021979390021442056,
      "loss": 0.0048,
      "step": 10000
    },
    {
      "epoch": 0.9166362641371762,
      "grad_norm": 6553.109375,
      "learning_rate": 0.00021931385412999645,
      "loss": 0.0045,
      "step": 10050
    },
    {
      "epoch": 0.9211966435607443,
      "grad_norm": 25130.296875,
      "learning_rate": 0.00021883380804557236,
      "loss": 0.0038,
      "step": 10100
    },
    {
      "epoch": 0.9257570229843123,
      "grad_norm": 204.2277374267578,
      "learning_rate": 0.00021835376196114824,
      "loss": 0.0038,
      "step": 10150
    },
    {
      "epoch": 0.9303174024078803,
      "grad_norm": 27197.99609375,
      "learning_rate": 0.00021787371587672416,
      "loss": 0.0047,
      "step": 10200
    },
    {
      "epoch": 0.9348777818314484,
      "grad_norm": 5847.94873046875,
      "learning_rate": 0.00021739366979230002,
      "loss": 0.0051,
      "step": 10250
    },
    {
      "epoch": 0.9394381612550164,
      "grad_norm": 21443.193359375,
      "learning_rate": 0.00021691362370787593,
      "loss": 0.0023,
      "step": 10300
    },
    {
      "epoch": 0.9439985406785845,
      "grad_norm": 26163.48828125,
      "learning_rate": 0.00021643357762345184,
      "loss": 0.004,
      "step": 10350
    },
    {
      "epoch": 0.9485589201021525,
      "grad_norm": 3516.400390625,
      "learning_rate": 0.00021595353153902773,
      "loss": 0.0046,
      "step": 10400
    },
    {
      "epoch": 0.9531192995257205,
      "grad_norm": 1693.5291748046875,
      "learning_rate": 0.00021547348545460364,
      "loss": 0.0037,
      "step": 10450
    },
    {
      "epoch": 0.9576796789492886,
      "grad_norm": 19739.90234375,
      "learning_rate": 0.0002149934393701795,
      "loss": 0.003,
      "step": 10500
    },
    {
      "epoch": 0.9622400583728566,
      "grad_norm": 16610.03515625,
      "learning_rate": 0.00021451339328575541,
      "loss": 0.0044,
      "step": 10550
    },
    {
      "epoch": 0.9668004377964247,
      "grad_norm": 5892.9306640625,
      "learning_rate": 0.0002140333472013313,
      "loss": 0.0045,
      "step": 10600
    },
    {
      "epoch": 0.9713608172199927,
      "grad_norm": 19880.96484375,
      "learning_rate": 0.0002135533011169072,
      "loss": 0.0038,
      "step": 10650
    },
    {
      "epoch": 0.9759211966435607,
      "grad_norm": 3581.54638671875,
      "learning_rate": 0.00021307325503248313,
      "loss": 0.0027,
      "step": 10700
    },
    {
      "epoch": 0.9804815760671288,
      "grad_norm": 10603.330078125,
      "learning_rate": 0.00021259320894805898,
      "loss": 0.0037,
      "step": 10750
    },
    {
      "epoch": 0.9850419554906968,
      "grad_norm": 0.20070327818393707,
      "learning_rate": 0.0002121131628636349,
      "loss": 0.005,
      "step": 10800
    },
    {
      "epoch": 0.9896023349142649,
      "grad_norm": 13806.07421875,
      "learning_rate": 0.00021163311677921078,
      "loss": 0.0059,
      "step": 10850
    },
    {
      "epoch": 0.9941627143378329,
      "grad_norm": 2716.009521484375,
      "learning_rate": 0.0002111530706947867,
      "loss": 0.0043,
      "step": 10900
    },
    {
      "epoch": 0.998723093761401,
      "grad_norm": 32567.5234375,
      "learning_rate": 0.00021067302461036256,
      "loss": 0.0036,
      "step": 10950
    }
  ],
  "logging_steps": 50,
  "max_steps": 32892,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6304549112840192e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
