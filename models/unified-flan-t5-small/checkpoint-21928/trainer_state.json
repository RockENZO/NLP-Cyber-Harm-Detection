{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 21928,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004560379423568041,
      "grad_norm": 5191381.0,
      "learning_rate": 8.936170212765958e-06,
      "loss": 31.7092,
      "step": 50
    },
    {
      "epoch": 0.009120758847136081,
      "grad_norm": 2986101.5,
      "learning_rate": 1.8054711246200608e-05,
      "loss": 20.2918,
      "step": 100
    },
    {
      "epoch": 0.013681138270704123,
      "grad_norm": 1009308.5625,
      "learning_rate": 2.7173252279635254e-05,
      "loss": 10.1357,
      "step": 150
    },
    {
      "epoch": 0.018241517694272163,
      "grad_norm": 1024604.0,
      "learning_rate": 3.629179331306991e-05,
      "loss": 4.296,
      "step": 200
    },
    {
      "epoch": 0.022801897117840204,
      "grad_norm": 1150882.625,
      "learning_rate": 4.541033434650456e-05,
      "loss": 2.3434,
      "step": 250
    },
    {
      "epoch": 0.027362276541408246,
      "grad_norm": 917637.0,
      "learning_rate": 5.452887537993921e-05,
      "loss": 1.2138,
      "step": 300
    },
    {
      "epoch": 0.03192265596497629,
      "grad_norm": 352328.40625,
      "learning_rate": 6.364741641337386e-05,
      "loss": 0.4973,
      "step": 350
    },
    {
      "epoch": 0.036483035388544326,
      "grad_norm": 94084.1328125,
      "learning_rate": 7.276595744680851e-05,
      "loss": 0.1717,
      "step": 400
    },
    {
      "epoch": 0.04104341481211237,
      "grad_norm": 41576.3203125,
      "learning_rate": 8.188449848024316e-05,
      "loss": 0.0748,
      "step": 450
    },
    {
      "epoch": 0.04560379423568041,
      "grad_norm": 11227.4306640625,
      "learning_rate": 9.100303951367781e-05,
      "loss": 0.0403,
      "step": 500
    },
    {
      "epoch": 0.05016417365924845,
      "grad_norm": 12313.716796875,
      "learning_rate": 0.00010012158054711244,
      "loss": 0.0249,
      "step": 550
    },
    {
      "epoch": 0.05472455308281649,
      "grad_norm": 20233.6953125,
      "learning_rate": 0.0001092401215805471,
      "loss": 0.0176,
      "step": 600
    },
    {
      "epoch": 0.05928493250638453,
      "grad_norm": 15492.6396484375,
      "learning_rate": 0.00011835866261398174,
      "loss": 0.0116,
      "step": 650
    },
    {
      "epoch": 0.06384531192995258,
      "grad_norm": 7781.10986328125,
      "learning_rate": 0.0001274772036474164,
      "loss": 0.0092,
      "step": 700
    },
    {
      "epoch": 0.0684056913535206,
      "grad_norm": 5014.93408203125,
      "learning_rate": 0.00013659574468085104,
      "loss": 0.0093,
      "step": 750
    },
    {
      "epoch": 0.07296607077708865,
      "grad_norm": 10414.806640625,
      "learning_rate": 0.0001457142857142857,
      "loss": 0.0076,
      "step": 800
    },
    {
      "epoch": 0.0775264502006567,
      "grad_norm": 5240.27099609375,
      "learning_rate": 0.00015483282674772034,
      "loss": 0.0062,
      "step": 850
    },
    {
      "epoch": 0.08208682962422474,
      "grad_norm": 4882.24755859375,
      "learning_rate": 0.000163951367781155,
      "loss": 0.0062,
      "step": 900
    },
    {
      "epoch": 0.08664720904779277,
      "grad_norm": 6396.0517578125,
      "learning_rate": 0.00017306990881458964,
      "loss": 0.0056,
      "step": 950
    },
    {
      "epoch": 0.09120758847136082,
      "grad_norm": 6876.47509765625,
      "learning_rate": 0.0001821884498480243,
      "loss": 0.0052,
      "step": 1000
    },
    {
      "epoch": 0.09576796789492886,
      "grad_norm": 1745.3070068359375,
      "learning_rate": 0.00019130699088145894,
      "loss": 0.0048,
      "step": 1050
    },
    {
      "epoch": 0.1003283473184969,
      "grad_norm": 2925.928466796875,
      "learning_rate": 0.0002004255319148936,
      "loss": 0.005,
      "step": 1100
    },
    {
      "epoch": 0.10488872674206494,
      "grad_norm": 3129.099365234375,
      "learning_rate": 0.00020954407294832824,
      "loss": 0.0045,
      "step": 1150
    },
    {
      "epoch": 0.10944910616563298,
      "grad_norm": 4802.99609375,
      "learning_rate": 0.0002186626139817629,
      "loss": 0.0043,
      "step": 1200
    },
    {
      "epoch": 0.11400948558920102,
      "grad_norm": 3725.22705078125,
      "learning_rate": 0.00022778115501519757,
      "loss": 0.0038,
      "step": 1250
    },
    {
      "epoch": 0.11856986501276906,
      "grad_norm": 2246.4873046875,
      "learning_rate": 0.0002368996960486322,
      "loss": 0.0046,
      "step": 1300
    },
    {
      "epoch": 0.1231302444363371,
      "grad_norm": 1279.67822265625,
      "learning_rate": 0.00024601823708206684,
      "loss": 0.005,
      "step": 1350
    },
    {
      "epoch": 0.12769062385990515,
      "grad_norm": 4941.34375,
      "learning_rate": 0.0002551367781155015,
      "loss": 0.0042,
      "step": 1400
    },
    {
      "epoch": 0.1322510032834732,
      "grad_norm": 11591.8203125,
      "learning_rate": 0.00026425531914893614,
      "loss": 0.0044,
      "step": 1450
    },
    {
      "epoch": 0.1368113827070412,
      "grad_norm": 2826.35400390625,
      "learning_rate": 0.0002733738601823708,
      "loss": 0.0053,
      "step": 1500
    },
    {
      "epoch": 0.14137176213060926,
      "grad_norm": 4483.15283203125,
      "learning_rate": 0.00028249240121580544,
      "loss": 0.0041,
      "step": 1550
    },
    {
      "epoch": 0.1459321415541773,
      "grad_norm": 6124.10595703125,
      "learning_rate": 0.0002916109422492401,
      "loss": 0.0039,
      "step": 1600
    },
    {
      "epoch": 0.15049252097774535,
      "grad_norm": 3820.394287109375,
      "learning_rate": 0.000299961596313246,
      "loss": 0.0039,
      "step": 1650
    },
    {
      "epoch": 0.1550529004013134,
      "grad_norm": 5245.4638671875,
      "learning_rate": 0.00029948155022882197,
      "loss": 0.0036,
      "step": 1700
    },
    {
      "epoch": 0.15961327982488144,
      "grad_norm": 5578.05712890625,
      "learning_rate": 0.00029900150414439785,
      "loss": 0.0043,
      "step": 1750
    },
    {
      "epoch": 0.16417365924844948,
      "grad_norm": 18701.275390625,
      "learning_rate": 0.00029852145805997374,
      "loss": 0.0045,
      "step": 1800
    },
    {
      "epoch": 0.1687340386720175,
      "grad_norm": 2198.85107421875,
      "learning_rate": 0.0002980414119755496,
      "loss": 0.0043,
      "step": 1850
    },
    {
      "epoch": 0.17329441809558555,
      "grad_norm": 4847.12890625,
      "learning_rate": 0.0002975613658911255,
      "loss": 0.0052,
      "step": 1900
    },
    {
      "epoch": 0.1778547975191536,
      "grad_norm": 776.0476684570312,
      "learning_rate": 0.00029708131980670145,
      "loss": 0.0028,
      "step": 1950
    },
    {
      "epoch": 0.18241517694272164,
      "grad_norm": 1443.686767578125,
      "learning_rate": 0.0002966012737222773,
      "loss": 0.003,
      "step": 2000
    },
    {
      "epoch": 0.18697555636628968,
      "grad_norm": 3676.90478515625,
      "learning_rate": 0.0002961212276378532,
      "loss": 0.0039,
      "step": 2050
    },
    {
      "epoch": 0.19153593578985773,
      "grad_norm": 40.87212371826172,
      "learning_rate": 0.0002956411815534291,
      "loss": 0.0034,
      "step": 2100
    },
    {
      "epoch": 0.19609631521342577,
      "grad_norm": 6979.63818359375,
      "learning_rate": 0.000295161135469005,
      "loss": 0.0037,
      "step": 2150
    },
    {
      "epoch": 0.2006566946369938,
      "grad_norm": 3042.655517578125,
      "learning_rate": 0.0002946810893845809,
      "loss": 0.004,
      "step": 2200
    },
    {
      "epoch": 0.20521707406056183,
      "grad_norm": 11051.544921875,
      "learning_rate": 0.00029420104330015677,
      "loss": 0.0031,
      "step": 2250
    },
    {
      "epoch": 0.20977745348412988,
      "grad_norm": 1560.3939208984375,
      "learning_rate": 0.0002937209972157327,
      "loss": 0.0035,
      "step": 2300
    },
    {
      "epoch": 0.21433783290769792,
      "grad_norm": 1301.023193359375,
      "learning_rate": 0.0002932409511313086,
      "loss": 0.0027,
      "step": 2350
    },
    {
      "epoch": 0.21889821233126597,
      "grad_norm": 4049.370361328125,
      "learning_rate": 0.0002927609050468845,
      "loss": 0.0034,
      "step": 2400
    },
    {
      "epoch": 0.223458591754834,
      "grad_norm": 3727.81005859375,
      "learning_rate": 0.00029228085896246036,
      "loss": 0.0039,
      "step": 2450
    },
    {
      "epoch": 0.22801897117840203,
      "grad_norm": 23.087020874023438,
      "learning_rate": 0.00029180081287803625,
      "loss": 0.0025,
      "step": 2500
    },
    {
      "epoch": 0.23257935060197008,
      "grad_norm": 6355.607421875,
      "learning_rate": 0.0002913207667936122,
      "loss": 0.0039,
      "step": 2550
    },
    {
      "epoch": 0.23713973002553812,
      "grad_norm": 5175.6767578125,
      "learning_rate": 0.000290840720709188,
      "loss": 0.0029,
      "step": 2600
    },
    {
      "epoch": 0.24170010944910617,
      "grad_norm": 3428.44677734375,
      "learning_rate": 0.00029036067462476396,
      "loss": 0.0037,
      "step": 2650
    },
    {
      "epoch": 0.2462604888726742,
      "grad_norm": 1918.808837890625,
      "learning_rate": 0.00028988062854033985,
      "loss": 0.0038,
      "step": 2700
    },
    {
      "epoch": 0.2508208682962422,
      "grad_norm": 873.9747314453125,
      "learning_rate": 0.00028940058245591573,
      "loss": 0.0031,
      "step": 2750
    },
    {
      "epoch": 0.2553812477198103,
      "grad_norm": 894.5855712890625,
      "learning_rate": 0.0002889205363714917,
      "loss": 0.0029,
      "step": 2800
    },
    {
      "epoch": 0.2599416271433783,
      "grad_norm": 3663.469970703125,
      "learning_rate": 0.0002884404902870675,
      "loss": 0.004,
      "step": 2850
    },
    {
      "epoch": 0.2645020065669464,
      "grad_norm": 5029.2021484375,
      "learning_rate": 0.00028796044420264345,
      "loss": 0.0029,
      "step": 2900
    },
    {
      "epoch": 0.2690623859905144,
      "grad_norm": 1186.6944580078125,
      "learning_rate": 0.00028748039811821933,
      "loss": 0.0033,
      "step": 2950
    },
    {
      "epoch": 0.2736227654140824,
      "grad_norm": 1336.5592041015625,
      "learning_rate": 0.0002870003520337952,
      "loss": 0.0038,
      "step": 3000
    },
    {
      "epoch": 0.2781831448376505,
      "grad_norm": 162.01199340820312,
      "learning_rate": 0.0002865203059493711,
      "loss": 0.0036,
      "step": 3050
    },
    {
      "epoch": 0.2827435242612185,
      "grad_norm": 2847.91796875,
      "learning_rate": 0.000286040259864947,
      "loss": 0.0035,
      "step": 3100
    },
    {
      "epoch": 0.2873039036847866,
      "grad_norm": 3522.3447265625,
      "learning_rate": 0.00028556021378052293,
      "loss": 0.0036,
      "step": 3150
    },
    {
      "epoch": 0.2918642831083546,
      "grad_norm": 7830.3466796875,
      "learning_rate": 0.0002850801676960988,
      "loss": 0.0026,
      "step": 3200
    },
    {
      "epoch": 0.2964246625319227,
      "grad_norm": 8993.009765625,
      "learning_rate": 0.0002846001216116747,
      "loss": 0.0035,
      "step": 3250
    },
    {
      "epoch": 0.3009850419554907,
      "grad_norm": 4850.8564453125,
      "learning_rate": 0.0002841200755272506,
      "loss": 0.0037,
      "step": 3300
    },
    {
      "epoch": 0.3055454213790587,
      "grad_norm": 6726.22412109375,
      "learning_rate": 0.0002836400294428265,
      "loss": 0.0041,
      "step": 3350
    },
    {
      "epoch": 0.3101058008026268,
      "grad_norm": 4995.31396484375,
      "learning_rate": 0.0002831599833584024,
      "loss": 0.004,
      "step": 3400
    },
    {
      "epoch": 0.3146661802261948,
      "grad_norm": 13345.486328125,
      "learning_rate": 0.0002826799372739783,
      "loss": 0.0032,
      "step": 3450
    },
    {
      "epoch": 0.3192265596497629,
      "grad_norm": 2827.42431640625,
      "learning_rate": 0.0002821998911895542,
      "loss": 0.0025,
      "step": 3500
    },
    {
      "epoch": 0.3237869390733309,
      "grad_norm": 12352.2509765625,
      "learning_rate": 0.00028171984510513007,
      "loss": 0.0036,
      "step": 3550
    },
    {
      "epoch": 0.32834731849689897,
      "grad_norm": 6555.58349609375,
      "learning_rate": 0.00028123979902070596,
      "loss": 0.0036,
      "step": 3600
    },
    {
      "epoch": 0.332907697920467,
      "grad_norm": 5263.6025390625,
      "learning_rate": 0.00028075975293628184,
      "loss": 0.0028,
      "step": 3650
    },
    {
      "epoch": 0.337468077344035,
      "grad_norm": 4955.0693359375,
      "learning_rate": 0.00028027970685185773,
      "loss": 0.0039,
      "step": 3700
    },
    {
      "epoch": 0.3420284567676031,
      "grad_norm": 4084.581787109375,
      "learning_rate": 0.00027979966076743367,
      "loss": 0.0028,
      "step": 3750
    },
    {
      "epoch": 0.3465888361911711,
      "grad_norm": 52.824527740478516,
      "learning_rate": 0.00027931961468300956,
      "loss": 0.0024,
      "step": 3800
    },
    {
      "epoch": 0.35114921561473916,
      "grad_norm": 36.61239242553711,
      "learning_rate": 0.00027883956859858544,
      "loss": 0.004,
      "step": 3850
    },
    {
      "epoch": 0.3557095950383072,
      "grad_norm": 8609.90625,
      "learning_rate": 0.00027835952251416133,
      "loss": 0.0029,
      "step": 3900
    },
    {
      "epoch": 0.36026997446187525,
      "grad_norm": 7232.01708984375,
      "learning_rate": 0.0002778794764297372,
      "loss": 0.0029,
      "step": 3950
    },
    {
      "epoch": 0.36483035388544327,
      "grad_norm": 3892.7958984375,
      "learning_rate": 0.0002773994303453131,
      "loss": 0.0037,
      "step": 4000
    },
    {
      "epoch": 0.3693907333090113,
      "grad_norm": 7876.177734375,
      "learning_rate": 0.00027691938426088904,
      "loss": 0.0032,
      "step": 4050
    },
    {
      "epoch": 0.37395111273257936,
      "grad_norm": 111.283203125,
      "learning_rate": 0.0002764393381764649,
      "loss": 0.004,
      "step": 4100
    },
    {
      "epoch": 0.3785114921561474,
      "grad_norm": 2361.9755859375,
      "learning_rate": 0.0002759592920920408,
      "loss": 0.0044,
      "step": 4150
    },
    {
      "epoch": 0.38307187157971545,
      "grad_norm": 75.35592651367188,
      "learning_rate": 0.0002754792460076167,
      "loss": 0.003,
      "step": 4200
    },
    {
      "epoch": 0.38763225100328347,
      "grad_norm": 8587.8662109375,
      "learning_rate": 0.0002749991999231926,
      "loss": 0.0036,
      "step": 4250
    },
    {
      "epoch": 0.39219263042685154,
      "grad_norm": 9101.6474609375,
      "learning_rate": 0.0002745191538387685,
      "loss": 0.0036,
      "step": 4300
    },
    {
      "epoch": 0.39675300985041956,
      "grad_norm": 8242.1396484375,
      "learning_rate": 0.00027403910775434436,
      "loss": 0.0039,
      "step": 4350
    },
    {
      "epoch": 0.4013133892739876,
      "grad_norm": 7261.2919921875,
      "learning_rate": 0.0002735590616699203,
      "loss": 0.0036,
      "step": 4400
    },
    {
      "epoch": 0.40587376869755565,
      "grad_norm": 11546.7353515625,
      "learning_rate": 0.0002730790155854962,
      "loss": 0.0022,
      "step": 4450
    },
    {
      "epoch": 0.41043414812112367,
      "grad_norm": 13710.5146484375,
      "learning_rate": 0.00027259896950107207,
      "loss": 0.005,
      "step": 4500
    },
    {
      "epoch": 0.41499452754469174,
      "grad_norm": 10107.23828125,
      "learning_rate": 0.000272118923416648,
      "loss": 0.0037,
      "step": 4550
    },
    {
      "epoch": 0.41955490696825976,
      "grad_norm": 9945.3603515625,
      "learning_rate": 0.00027163887733222384,
      "loss": 0.0043,
      "step": 4600
    },
    {
      "epoch": 0.4241152863918278,
      "grad_norm": 223.8675079345703,
      "learning_rate": 0.0002711588312477998,
      "loss": 0.0032,
      "step": 4650
    },
    {
      "epoch": 0.42867566581539585,
      "grad_norm": 7325.44482421875,
      "learning_rate": 0.00027067878516337567,
      "loss": 0.0044,
      "step": 4700
    },
    {
      "epoch": 0.43323604523896386,
      "grad_norm": 18206.541015625,
      "learning_rate": 0.00027019873907895155,
      "loss": 0.0044,
      "step": 4750
    },
    {
      "epoch": 0.43779642466253194,
      "grad_norm": 19797.138671875,
      "learning_rate": 0.00026971869299452744,
      "loss": 0.0036,
      "step": 4800
    },
    {
      "epoch": 0.44235680408609995,
      "grad_norm": 25315.513671875,
      "learning_rate": 0.0002692386469101033,
      "loss": 0.003,
      "step": 4850
    },
    {
      "epoch": 0.446917183509668,
      "grad_norm": 71.27204132080078,
      "learning_rate": 0.00026875860082567927,
      "loss": 0.0051,
      "step": 4900
    },
    {
      "epoch": 0.45147756293323604,
      "grad_norm": 94.12933349609375,
      "learning_rate": 0.00026827855474125515,
      "loss": 0.004,
      "step": 4950
    },
    {
      "epoch": 0.45603794235680406,
      "grad_norm": 20835.52734375,
      "learning_rate": 0.00026779850865683104,
      "loss": 0.0028,
      "step": 5000
    },
    {
      "epoch": 0.46059832178037213,
      "grad_norm": 10.855692863464355,
      "learning_rate": 0.0002673184625724069,
      "loss": 0.0038,
      "step": 5050
    },
    {
      "epoch": 0.46515870120394015,
      "grad_norm": 14723.458984375,
      "learning_rate": 0.0002668384164879828,
      "loss": 0.0033,
      "step": 5100
    },
    {
      "epoch": 0.4697190806275082,
      "grad_norm": 9277.3291015625,
      "learning_rate": 0.00026635837040355875,
      "loss": 0.0042,
      "step": 5150
    },
    {
      "epoch": 0.47427946005107624,
      "grad_norm": 3981.341796875,
      "learning_rate": 0.0002658783243191346,
      "loss": 0.0055,
      "step": 5200
    },
    {
      "epoch": 0.4788398394746443,
      "grad_norm": 9875.7001953125,
      "learning_rate": 0.0002653982782347105,
      "loss": 0.0031,
      "step": 5250
    },
    {
      "epoch": 0.48340021889821233,
      "grad_norm": 0.7528746724128723,
      "learning_rate": 0.0002649182321502864,
      "loss": 0.004,
      "step": 5300
    },
    {
      "epoch": 0.48796059832178035,
      "grad_norm": 13251.9052734375,
      "learning_rate": 0.0002644381860658623,
      "loss": 0.0067,
      "step": 5350
    },
    {
      "epoch": 0.4925209777453484,
      "grad_norm": 3365.882568359375,
      "learning_rate": 0.00026395813998143823,
      "loss": 0.0028,
      "step": 5400
    },
    {
      "epoch": 0.49708135716891644,
      "grad_norm": 3.9904885292053223,
      "learning_rate": 0.00026347809389701407,
      "loss": 0.0033,
      "step": 5450
    },
    {
      "epoch": 0.5016417365924845,
      "grad_norm": 4237.01953125,
      "learning_rate": 0.00026299804781259,
      "loss": 0.0042,
      "step": 5500
    },
    {
      "epoch": 0.5062021160160526,
      "grad_norm": 0.5582995414733887,
      "learning_rate": 0.0002625180017281659,
      "loss": 0.0049,
      "step": 5550
    },
    {
      "epoch": 0.5107624954396206,
      "grad_norm": 8.926702499389648,
      "learning_rate": 0.0002620379556437418,
      "loss": 0.0051,
      "step": 5600
    },
    {
      "epoch": 0.5153228748631886,
      "grad_norm": 0.11075832694768906,
      "learning_rate": 0.00026155790955931766,
      "loss": 0.0052,
      "step": 5650
    },
    {
      "epoch": 0.5198832542867566,
      "grad_norm": 13742.435546875,
      "learning_rate": 0.00026107786347489355,
      "loss": 0.0063,
      "step": 5700
    },
    {
      "epoch": 0.5244436337103247,
      "grad_norm": 2153.0947265625,
      "learning_rate": 0.0002605978173904695,
      "loss": 0.004,
      "step": 5750
    },
    {
      "epoch": 0.5290040131338928,
      "grad_norm": 9385.3115234375,
      "learning_rate": 0.0002601177713060454,
      "loss": 0.0042,
      "step": 5800
    },
    {
      "epoch": 0.5335643925574608,
      "grad_norm": 6672.72509765625,
      "learning_rate": 0.00025963772522162126,
      "loss": 0.0043,
      "step": 5850
    },
    {
      "epoch": 0.5381247719810288,
      "grad_norm": 9525.65234375,
      "learning_rate": 0.00025915767913719715,
      "loss": 0.0053,
      "step": 5900
    },
    {
      "epoch": 0.5426851514045968,
      "grad_norm": 232.60498046875,
      "learning_rate": 0.00025867763305277303,
      "loss": 0.0037,
      "step": 5950
    },
    {
      "epoch": 0.5472455308281649,
      "grad_norm": 17895.58984375,
      "learning_rate": 0.0002581975869683489,
      "loss": 0.0049,
      "step": 6000
    },
    {
      "epoch": 0.551805910251733,
      "grad_norm": 12737.7802734375,
      "learning_rate": 0.00025771754088392486,
      "loss": 0.0059,
      "step": 6050
    },
    {
      "epoch": 0.556366289675301,
      "grad_norm": 18965.5,
      "learning_rate": 0.00025723749479950075,
      "loss": 0.0043,
      "step": 6100
    },
    {
      "epoch": 0.560926669098869,
      "grad_norm": 4996.62646484375,
      "learning_rate": 0.00025675744871507663,
      "loss": 0.0042,
      "step": 6150
    },
    {
      "epoch": 0.565487048522437,
      "grad_norm": 13.310291290283203,
      "learning_rate": 0.0002562774026306525,
      "loss": 0.0027,
      "step": 6200
    },
    {
      "epoch": 0.5700474279460052,
      "grad_norm": 8628.5615234375,
      "learning_rate": 0.0002557973565462284,
      "loss": 0.0035,
      "step": 6250
    },
    {
      "epoch": 0.5746078073695732,
      "grad_norm": 19231.8046875,
      "learning_rate": 0.0002553173104618043,
      "loss": 0.0032,
      "step": 6300
    },
    {
      "epoch": 0.5791681867931412,
      "grad_norm": 18334.12109375,
      "learning_rate": 0.0002548372643773802,
      "loss": 0.0047,
      "step": 6350
    },
    {
      "epoch": 0.5837285662167092,
      "grad_norm": 31634.16796875,
      "learning_rate": 0.0002543572182929561,
      "loss": 0.0048,
      "step": 6400
    },
    {
      "epoch": 0.5882889456402772,
      "grad_norm": 0.27193963527679443,
      "learning_rate": 0.000253877172208532,
      "loss": 0.0037,
      "step": 6450
    },
    {
      "epoch": 0.5928493250638454,
      "grad_norm": 5134.4482421875,
      "learning_rate": 0.0002533971261241079,
      "loss": 0.0049,
      "step": 6500
    },
    {
      "epoch": 0.5974097044874134,
      "grad_norm": 10275.345703125,
      "learning_rate": 0.0002529170800396838,
      "loss": 0.0057,
      "step": 6550
    },
    {
      "epoch": 0.6019700839109814,
      "grad_norm": 13363.2197265625,
      "learning_rate": 0.00025243703395525966,
      "loss": 0.005,
      "step": 6600
    },
    {
      "epoch": 0.6065304633345494,
      "grad_norm": 993.8096923828125,
      "learning_rate": 0.0002519569878708356,
      "loss": 0.0039,
      "step": 6650
    },
    {
      "epoch": 0.6110908427581174,
      "grad_norm": 5708.0478515625,
      "learning_rate": 0.0002514769417864115,
      "loss": 0.0037,
      "step": 6700
    },
    {
      "epoch": 0.6156512221816856,
      "grad_norm": 7.295849800109863,
      "learning_rate": 0.0002509968957019874,
      "loss": 0.0059,
      "step": 6750
    },
    {
      "epoch": 0.6202116016052536,
      "grad_norm": 10413.23046875,
      "learning_rate": 0.00025051684961756326,
      "loss": 0.0036,
      "step": 6800
    },
    {
      "epoch": 0.6247719810288216,
      "grad_norm": 337.8074645996094,
      "learning_rate": 0.00025003680353313914,
      "loss": 0.0042,
      "step": 6850
    },
    {
      "epoch": 0.6293323604523896,
      "grad_norm": 5516.873046875,
      "learning_rate": 0.0002495567574487151,
      "loss": 0.0042,
      "step": 6900
    },
    {
      "epoch": 0.6338927398759577,
      "grad_norm": 11528.994140625,
      "learning_rate": 0.0002490767113642909,
      "loss": 0.0058,
      "step": 6950
    },
    {
      "epoch": 0.6384531192995258,
      "grad_norm": 9465.6904296875,
      "learning_rate": 0.00024859666527986686,
      "loss": 0.0044,
      "step": 7000
    },
    {
      "epoch": 0.6430134987230938,
      "grad_norm": 6091.9970703125,
      "learning_rate": 0.00024811661919544274,
      "loss": 0.0041,
      "step": 7050
    },
    {
      "epoch": 0.6475738781466618,
      "grad_norm": 2880.179931640625,
      "learning_rate": 0.00024763657311101863,
      "loss": 0.0035,
      "step": 7100
    },
    {
      "epoch": 0.6521342575702298,
      "grad_norm": 14479.642578125,
      "learning_rate": 0.0002471565270265945,
      "loss": 0.0038,
      "step": 7150
    },
    {
      "epoch": 0.6566946369937979,
      "grad_norm": 20063.322265625,
      "learning_rate": 0.0002466764809421704,
      "loss": 0.0039,
      "step": 7200
    },
    {
      "epoch": 0.661255016417366,
      "grad_norm": 14122.59375,
      "learning_rate": 0.00024619643485774634,
      "loss": 0.005,
      "step": 7250
    },
    {
      "epoch": 0.665815395840934,
      "grad_norm": 0.047965772449970245,
      "learning_rate": 0.00024571638877332223,
      "loss": 0.0045,
      "step": 7300
    },
    {
      "epoch": 0.670375775264502,
      "grad_norm": 7790.6357421875,
      "learning_rate": 0.0002452363426888981,
      "loss": 0.0038,
      "step": 7350
    },
    {
      "epoch": 0.67493615468807,
      "grad_norm": 8388.1767578125,
      "learning_rate": 0.000244756296604474,
      "loss": 0.004,
      "step": 7400
    },
    {
      "epoch": 0.6794965341116381,
      "grad_norm": 39.791282653808594,
      "learning_rate": 0.0002442762505200499,
      "loss": 0.0033,
      "step": 7450
    },
    {
      "epoch": 0.6840569135352061,
      "grad_norm": 19.19691276550293,
      "learning_rate": 0.0002437962044356258,
      "loss": 0.0043,
      "step": 7500
    },
    {
      "epoch": 0.6886172929587742,
      "grad_norm": 0.6072261333465576,
      "learning_rate": 0.00024331615835120168,
      "loss": 0.0039,
      "step": 7550
    },
    {
      "epoch": 0.6931776723823422,
      "grad_norm": 12369.353515625,
      "learning_rate": 0.0002428361122667776,
      "loss": 0.0047,
      "step": 7600
    },
    {
      "epoch": 0.6977380518059102,
      "grad_norm": 0.123549684882164,
      "learning_rate": 0.00024235606618235346,
      "loss": 0.0043,
      "step": 7650
    },
    {
      "epoch": 0.7022984312294783,
      "grad_norm": 830.545654296875,
      "learning_rate": 0.00024187602009792937,
      "loss": 0.0048,
      "step": 7700
    },
    {
      "epoch": 0.7068588106530463,
      "grad_norm": 7564.1455078125,
      "learning_rate": 0.00024139597401350528,
      "loss": 0.0052,
      "step": 7750
    },
    {
      "epoch": 0.7114191900766144,
      "grad_norm": 7156.69873046875,
      "learning_rate": 0.00024091592792908117,
      "loss": 0.0044,
      "step": 7800
    },
    {
      "epoch": 0.7159795695001824,
      "grad_norm": 2605.3271484375,
      "learning_rate": 0.00024043588184465708,
      "loss": 0.0036,
      "step": 7850
    },
    {
      "epoch": 0.7205399489237505,
      "grad_norm": 506.2583312988281,
      "learning_rate": 0.00023995583576023294,
      "loss": 0.0045,
      "step": 7900
    },
    {
      "epoch": 0.7251003283473185,
      "grad_norm": 189.6920166015625,
      "learning_rate": 0.00023947578967580885,
      "loss": 0.0043,
      "step": 7950
    },
    {
      "epoch": 0.7296607077708865,
      "grad_norm": 8482.74609375,
      "learning_rate": 0.00023899574359138474,
      "loss": 0.0041,
      "step": 8000
    },
    {
      "epoch": 0.7342210871944546,
      "grad_norm": 0.17709635198116302,
      "learning_rate": 0.00023851569750696065,
      "loss": 0.0047,
      "step": 8050
    },
    {
      "epoch": 0.7387814666180226,
      "grad_norm": 5972.11865234375,
      "learning_rate": 0.00023803565142253657,
      "loss": 0.0042,
      "step": 8100
    },
    {
      "epoch": 0.7433418460415907,
      "grad_norm": 8571.2431640625,
      "learning_rate": 0.00023755560533811242,
      "loss": 0.0026,
      "step": 8150
    },
    {
      "epoch": 0.7479022254651587,
      "grad_norm": 25.185789108276367,
      "learning_rate": 0.00023707555925368834,
      "loss": 0.0037,
      "step": 8200
    },
    {
      "epoch": 0.7524626048887267,
      "grad_norm": 11149.662109375,
      "learning_rate": 0.00023659551316926422,
      "loss": 0.0048,
      "step": 8250
    },
    {
      "epoch": 0.7570229843122948,
      "grad_norm": 46.00790023803711,
      "learning_rate": 0.00023611546708484014,
      "loss": 0.0032,
      "step": 8300
    },
    {
      "epoch": 0.7615833637358628,
      "grad_norm": 7713.6103515625,
      "learning_rate": 0.00023563542100041602,
      "loss": 0.0048,
      "step": 8350
    },
    {
      "epoch": 0.7661437431594309,
      "grad_norm": 0.11827582120895386,
      "learning_rate": 0.0002351553749159919,
      "loss": 0.0033,
      "step": 8400
    },
    {
      "epoch": 0.7707041225829989,
      "grad_norm": 3009.319091796875,
      "learning_rate": 0.00023467532883156782,
      "loss": 0.0056,
      "step": 8450
    },
    {
      "epoch": 0.7752645020065669,
      "grad_norm": 4807.74755859375,
      "learning_rate": 0.0002341952827471437,
      "loss": 0.0043,
      "step": 8500
    },
    {
      "epoch": 0.779824881430135,
      "grad_norm": 21775.80078125,
      "learning_rate": 0.0002337152366627196,
      "loss": 0.0062,
      "step": 8550
    },
    {
      "epoch": 0.7843852608537031,
      "grad_norm": 15864.9794921875,
      "learning_rate": 0.00023323519057829548,
      "loss": 0.0033,
      "step": 8600
    },
    {
      "epoch": 0.7889456402772711,
      "grad_norm": 2254.37646484375,
      "learning_rate": 0.0002327551444938714,
      "loss": 0.0026,
      "step": 8650
    },
    {
      "epoch": 0.7935060197008391,
      "grad_norm": 37.41449737548828,
      "learning_rate": 0.0002322750984094473,
      "loss": 0.0051,
      "step": 8700
    },
    {
      "epoch": 0.7980663991244071,
      "grad_norm": 16913.279296875,
      "learning_rate": 0.00023179505232502317,
      "loss": 0.0043,
      "step": 8750
    },
    {
      "epoch": 0.8026267785479752,
      "grad_norm": 47.316707611083984,
      "learning_rate": 0.00023131500624059908,
      "loss": 0.0046,
      "step": 8800
    },
    {
      "epoch": 0.8071871579715433,
      "grad_norm": 0.2803684175014496,
      "learning_rate": 0.00023083496015617496,
      "loss": 0.0045,
      "step": 8850
    },
    {
      "epoch": 0.8117475373951113,
      "grad_norm": 13520.3037109375,
      "learning_rate": 0.00023035491407175088,
      "loss": 0.0045,
      "step": 8900
    },
    {
      "epoch": 0.8163079168186793,
      "grad_norm": 180.1205291748047,
      "learning_rate": 0.00022987486798732674,
      "loss": 0.0038,
      "step": 8950
    },
    {
      "epoch": 0.8208682962422473,
      "grad_norm": 4441.60546875,
      "learning_rate": 0.00022939482190290265,
      "loss": 0.0038,
      "step": 9000
    },
    {
      "epoch": 0.8254286756658153,
      "grad_norm": 0.14862088859081268,
      "learning_rate": 0.00022891477581847856,
      "loss": 0.0039,
      "step": 9050
    },
    {
      "epoch": 0.8299890550893835,
      "grad_norm": 0.36453700065612793,
      "learning_rate": 0.00022843472973405445,
      "loss": 0.0039,
      "step": 9100
    },
    {
      "epoch": 0.8345494345129515,
      "grad_norm": 0.2301269918680191,
      "learning_rate": 0.00022795468364963036,
      "loss": 0.0045,
      "step": 9150
    },
    {
      "epoch": 0.8391098139365195,
      "grad_norm": 3981.968017578125,
      "learning_rate": 0.00022747463756520622,
      "loss": 0.0056,
      "step": 9200
    },
    {
      "epoch": 0.8436701933600875,
      "grad_norm": 0.1579790860414505,
      "learning_rate": 0.00022699459148078213,
      "loss": 0.0033,
      "step": 9250
    },
    {
      "epoch": 0.8482305727836555,
      "grad_norm": 0.4207400381565094,
      "learning_rate": 0.00022651454539635802,
      "loss": 0.003,
      "step": 9300
    },
    {
      "epoch": 0.8527909522072237,
      "grad_norm": 16288.947265625,
      "learning_rate": 0.00022603449931193393,
      "loss": 0.005,
      "step": 9350
    },
    {
      "epoch": 0.8573513316307917,
      "grad_norm": 15083.19140625,
      "learning_rate": 0.00022555445322750985,
      "loss": 0.0036,
      "step": 9400
    },
    {
      "epoch": 0.8619117110543597,
      "grad_norm": 5.15146017074585,
      "learning_rate": 0.0002250744071430857,
      "loss": 0.0042,
      "step": 9450
    },
    {
      "epoch": 0.8664720904779277,
      "grad_norm": 0.22516004741191864,
      "learning_rate": 0.00022459436105866162,
      "loss": 0.0057,
      "step": 9500
    },
    {
      "epoch": 0.8710324699014959,
      "grad_norm": 0.6659408211708069,
      "learning_rate": 0.0002241143149742375,
      "loss": 0.0034,
      "step": 9550
    },
    {
      "epoch": 0.8755928493250639,
      "grad_norm": 5.212887287139893,
      "learning_rate": 0.00022363426888981342,
      "loss": 0.0036,
      "step": 9600
    },
    {
      "epoch": 0.8801532287486319,
      "grad_norm": 667.3731079101562,
      "learning_rate": 0.00022315422280538928,
      "loss": 0.0039,
      "step": 9650
    },
    {
      "epoch": 0.8847136081721999,
      "grad_norm": 0.7396458387374878,
      "learning_rate": 0.0002226741767209652,
      "loss": 0.0055,
      "step": 9700
    },
    {
      "epoch": 0.8892739875957679,
      "grad_norm": 5766.755859375,
      "learning_rate": 0.0002221941306365411,
      "loss": 0.0047,
      "step": 9750
    },
    {
      "epoch": 0.893834367019336,
      "grad_norm": 524.7015991210938,
      "learning_rate": 0.000221714084552117,
      "loss": 0.0041,
      "step": 9800
    },
    {
      "epoch": 0.8983947464429041,
      "grad_norm": 1570.28515625,
      "learning_rate": 0.00022123403846769287,
      "loss": 0.0065,
      "step": 9850
    },
    {
      "epoch": 0.9029551258664721,
      "grad_norm": 24235.71484375,
      "learning_rate": 0.00022075399238326876,
      "loss": 0.0035,
      "step": 9900
    },
    {
      "epoch": 0.9075155052900401,
      "grad_norm": 266.35626220703125,
      "learning_rate": 0.00022027394629884467,
      "loss": 0.0037,
      "step": 9950
    },
    {
      "epoch": 0.9120758847136081,
      "grad_norm": 15455.5244140625,
      "learning_rate": 0.00021979390021442056,
      "loss": 0.0048,
      "step": 10000
    },
    {
      "epoch": 0.9166362641371762,
      "grad_norm": 6553.109375,
      "learning_rate": 0.00021931385412999645,
      "loss": 0.0045,
      "step": 10050
    },
    {
      "epoch": 0.9211966435607443,
      "grad_norm": 25130.296875,
      "learning_rate": 0.00021883380804557236,
      "loss": 0.0038,
      "step": 10100
    },
    {
      "epoch": 0.9257570229843123,
      "grad_norm": 204.2277374267578,
      "learning_rate": 0.00021835376196114824,
      "loss": 0.0038,
      "step": 10150
    },
    {
      "epoch": 0.9303174024078803,
      "grad_norm": 27197.99609375,
      "learning_rate": 0.00021787371587672416,
      "loss": 0.0047,
      "step": 10200
    },
    {
      "epoch": 0.9348777818314484,
      "grad_norm": 5847.94873046875,
      "learning_rate": 0.00021739366979230002,
      "loss": 0.0051,
      "step": 10250
    },
    {
      "epoch": 0.9394381612550164,
      "grad_norm": 21443.193359375,
      "learning_rate": 0.00021691362370787593,
      "loss": 0.0023,
      "step": 10300
    },
    {
      "epoch": 0.9439985406785845,
      "grad_norm": 26163.48828125,
      "learning_rate": 0.00021643357762345184,
      "loss": 0.004,
      "step": 10350
    },
    {
      "epoch": 0.9485589201021525,
      "grad_norm": 3516.400390625,
      "learning_rate": 0.00021595353153902773,
      "loss": 0.0046,
      "step": 10400
    },
    {
      "epoch": 0.9531192995257205,
      "grad_norm": 1693.5291748046875,
      "learning_rate": 0.00021547348545460364,
      "loss": 0.0037,
      "step": 10450
    },
    {
      "epoch": 0.9576796789492886,
      "grad_norm": 19739.90234375,
      "learning_rate": 0.0002149934393701795,
      "loss": 0.003,
      "step": 10500
    },
    {
      "epoch": 0.9622400583728566,
      "grad_norm": 16610.03515625,
      "learning_rate": 0.00021451339328575541,
      "loss": 0.0044,
      "step": 10550
    },
    {
      "epoch": 0.9668004377964247,
      "grad_norm": 5892.9306640625,
      "learning_rate": 0.0002140333472013313,
      "loss": 0.0045,
      "step": 10600
    },
    {
      "epoch": 0.9713608172199927,
      "grad_norm": 19880.96484375,
      "learning_rate": 0.0002135533011169072,
      "loss": 0.0038,
      "step": 10650
    },
    {
      "epoch": 0.9759211966435607,
      "grad_norm": 3581.54638671875,
      "learning_rate": 0.00021307325503248313,
      "loss": 0.0027,
      "step": 10700
    },
    {
      "epoch": 0.9804815760671288,
      "grad_norm": 10603.330078125,
      "learning_rate": 0.00021259320894805898,
      "loss": 0.0037,
      "step": 10750
    },
    {
      "epoch": 0.9850419554906968,
      "grad_norm": 0.20070327818393707,
      "learning_rate": 0.0002121131628636349,
      "loss": 0.005,
      "step": 10800
    },
    {
      "epoch": 0.9896023349142649,
      "grad_norm": 13806.07421875,
      "learning_rate": 0.00021163311677921078,
      "loss": 0.0059,
      "step": 10850
    },
    {
      "epoch": 0.9941627143378329,
      "grad_norm": 2716.009521484375,
      "learning_rate": 0.0002111530706947867,
      "loss": 0.0043,
      "step": 10900
    },
    {
      "epoch": 0.998723093761401,
      "grad_norm": 32567.5234375,
      "learning_rate": 0.00021067302461036256,
      "loss": 0.0036,
      "step": 10950
    },
    {
      "epoch": 1.003283473184969,
      "grad_norm": 0.3876640200614929,
      "learning_rate": 0.00021019297852593847,
      "loss": 0.0027,
      "step": 11000
    },
    {
      "epoch": 1.007843852608537,
      "grad_norm": 3878.865234375,
      "learning_rate": 0.00020971293244151438,
      "loss": 0.0027,
      "step": 11050
    },
    {
      "epoch": 1.0124042320321052,
      "grad_norm": 0.08259088546037674,
      "learning_rate": 0.00020923288635709027,
      "loss": 0.0021,
      "step": 11100
    },
    {
      "epoch": 1.0169646114556732,
      "grad_norm": 0.36752867698669434,
      "learning_rate": 0.00020875284027266615,
      "loss": 0.0036,
      "step": 11150
    },
    {
      "epoch": 1.0215249908792412,
      "grad_norm": 4521.21142578125,
      "learning_rate": 0.00020827279418824204,
      "loss": 0.0037,
      "step": 11200
    },
    {
      "epoch": 1.0260853703028092,
      "grad_norm": 0.05947845056653023,
      "learning_rate": 0.00020779274810381795,
      "loss": 0.0022,
      "step": 11250
    },
    {
      "epoch": 1.0306457497263772,
      "grad_norm": 18651.173828125,
      "learning_rate": 0.00020731270201939384,
      "loss": 0.0048,
      "step": 11300
    },
    {
      "epoch": 1.0352061291499453,
      "grad_norm": 296.7940368652344,
      "learning_rate": 0.00020683265593496973,
      "loss": 0.0027,
      "step": 11350
    },
    {
      "epoch": 1.0397665085735133,
      "grad_norm": 5519.53369140625,
      "learning_rate": 0.00020635260985054564,
      "loss": 0.0041,
      "step": 11400
    },
    {
      "epoch": 1.0443268879970813,
      "grad_norm": 0.01650325395166874,
      "learning_rate": 0.00020587256376612152,
      "loss": 0.0024,
      "step": 11450
    },
    {
      "epoch": 1.0488872674206493,
      "grad_norm": 2.1907479763031006,
      "learning_rate": 0.00020539251768169744,
      "loss": 0.0045,
      "step": 11500
    },
    {
      "epoch": 1.0534476468442175,
      "grad_norm": 28325.482421875,
      "learning_rate": 0.0002049124715972733,
      "loss": 0.0035,
      "step": 11550
    },
    {
      "epoch": 1.0580080262677856,
      "grad_norm": 21161.70703125,
      "learning_rate": 0.0002044324255128492,
      "loss": 0.0051,
      "step": 11600
    },
    {
      "epoch": 1.0625684056913536,
      "grad_norm": 2.255159854888916,
      "learning_rate": 0.0002039523794284251,
      "loss": 0.0036,
      "step": 11650
    },
    {
      "epoch": 1.0671287851149216,
      "grad_norm": 50875.046875,
      "learning_rate": 0.000203472333344001,
      "loss": 0.0033,
      "step": 11700
    },
    {
      "epoch": 1.0716891645384896,
      "grad_norm": 409.81341552734375,
      "learning_rate": 0.00020299228725957692,
      "loss": 0.0034,
      "step": 11750
    },
    {
      "epoch": 1.0762495439620576,
      "grad_norm": 0.8593921065330505,
      "learning_rate": 0.00020251224117515278,
      "loss": 0.0027,
      "step": 11800
    },
    {
      "epoch": 1.0808099233856256,
      "grad_norm": 6495.0283203125,
      "learning_rate": 0.0002020321950907287,
      "loss": 0.0021,
      "step": 11850
    },
    {
      "epoch": 1.0853703028091937,
      "grad_norm": 20630.86328125,
      "learning_rate": 0.00020155214900630458,
      "loss": 0.0027,
      "step": 11900
    },
    {
      "epoch": 1.0899306822327617,
      "grad_norm": 0.11792295426130295,
      "learning_rate": 0.0002010721029218805,
      "loss": 0.0037,
      "step": 11950
    },
    {
      "epoch": 1.09449106165633,
      "grad_norm": 27092.080078125,
      "learning_rate": 0.0002005920568374564,
      "loss": 0.0032,
      "step": 12000
    },
    {
      "epoch": 1.099051441079898,
      "grad_norm": 32605.34375,
      "learning_rate": 0.00020011201075303226,
      "loss": 0.0033,
      "step": 12050
    },
    {
      "epoch": 1.103611820503466,
      "grad_norm": 6557.986328125,
      "learning_rate": 0.00019963196466860818,
      "loss": 0.003,
      "step": 12100
    },
    {
      "epoch": 1.108172199927034,
      "grad_norm": 5305.4091796875,
      "learning_rate": 0.00019915191858418406,
      "loss": 0.0038,
      "step": 12150
    },
    {
      "epoch": 1.112732579350602,
      "grad_norm": 57.02448272705078,
      "learning_rate": 0.00019867187249975998,
      "loss": 0.0043,
      "step": 12200
    },
    {
      "epoch": 1.11729295877417,
      "grad_norm": 0.2315942645072937,
      "learning_rate": 0.00019819182641533584,
      "loss": 0.0025,
      "step": 12250
    },
    {
      "epoch": 1.121853338197738,
      "grad_norm": 0.05439047887921333,
      "learning_rate": 0.00019771178033091175,
      "loss": 0.0037,
      "step": 12300
    },
    {
      "epoch": 1.126413717621306,
      "grad_norm": 6.691662311553955,
      "learning_rate": 0.00019723173424648766,
      "loss": 0.0041,
      "step": 12350
    },
    {
      "epoch": 1.130974097044874,
      "grad_norm": 5.269130229949951,
      "learning_rate": 0.00019675168816206355,
      "loss": 0.003,
      "step": 12400
    },
    {
      "epoch": 1.135534476468442,
      "grad_norm": 2.628464937210083,
      "learning_rate": 0.00019627164207763943,
      "loss": 0.0035,
      "step": 12450
    },
    {
      "epoch": 1.14009485589201,
      "grad_norm": 56568.78515625,
      "learning_rate": 0.00019579159599321532,
      "loss": 0.0028,
      "step": 12500
    },
    {
      "epoch": 1.1446552353155783,
      "grad_norm": 0.0697721540927887,
      "learning_rate": 0.00019531154990879123,
      "loss": 0.0015,
      "step": 12550
    },
    {
      "epoch": 1.1492156147391464,
      "grad_norm": 0.8621010184288025,
      "learning_rate": 0.00019483150382436712,
      "loss": 0.0043,
      "step": 12600
    },
    {
      "epoch": 1.1537759941627144,
      "grad_norm": 0.015488985925912857,
      "learning_rate": 0.000194351457739943,
      "loss": 0.0033,
      "step": 12650
    },
    {
      "epoch": 1.1583363735862824,
      "grad_norm": 17.353872299194336,
      "learning_rate": 0.00019387141165551892,
      "loss": 0.0051,
      "step": 12700
    },
    {
      "epoch": 1.1628967530098504,
      "grad_norm": 0.30894264578819275,
      "learning_rate": 0.0001933913655710948,
      "loss": 0.0021,
      "step": 12750
    },
    {
      "epoch": 1.1674571324334184,
      "grad_norm": 23046.12109375,
      "learning_rate": 0.00019291131948667072,
      "loss": 0.0041,
      "step": 12800
    },
    {
      "epoch": 1.1720175118569864,
      "grad_norm": 0.33786970376968384,
      "learning_rate": 0.00019243127340224658,
      "loss": 0.0042,
      "step": 12850
    },
    {
      "epoch": 1.1765778912805545,
      "grad_norm": 0.11727647483348846,
      "learning_rate": 0.0001919512273178225,
      "loss": 0.0033,
      "step": 12900
    },
    {
      "epoch": 1.1811382707041225,
      "grad_norm": 0.060193415731191635,
      "learning_rate": 0.00019147118123339838,
      "loss": 0.0033,
      "step": 12950
    },
    {
      "epoch": 1.1856986501276907,
      "grad_norm": 0.5368185043334961,
      "learning_rate": 0.0001909911351489743,
      "loss": 0.0019,
      "step": 13000
    },
    {
      "epoch": 1.1902590295512587,
      "grad_norm": 0.5243939161300659,
      "learning_rate": 0.0001905110890645502,
      "loss": 0.0051,
      "step": 13050
    },
    {
      "epoch": 1.1948194089748267,
      "grad_norm": 2681.46240234375,
      "learning_rate": 0.00019003104298012606,
      "loss": 0.0029,
      "step": 13100
    },
    {
      "epoch": 1.1993797883983948,
      "grad_norm": 17223.119140625,
      "learning_rate": 0.00018955099689570197,
      "loss": 0.0029,
      "step": 13150
    },
    {
      "epoch": 1.2039401678219628,
      "grad_norm": 1.8947744369506836,
      "learning_rate": 0.00018907095081127786,
      "loss": 0.0031,
      "step": 13200
    },
    {
      "epoch": 1.2085005472455308,
      "grad_norm": 15059.07421875,
      "learning_rate": 0.00018859090472685377,
      "loss": 0.0044,
      "step": 13250
    },
    {
      "epoch": 1.2130609266690988,
      "grad_norm": 0.03491253778338432,
      "learning_rate": 0.00018811085864242963,
      "loss": 0.0031,
      "step": 13300
    },
    {
      "epoch": 1.2176213060926668,
      "grad_norm": 385.53314208984375,
      "learning_rate": 0.00018763081255800554,
      "loss": 0.004,
      "step": 13350
    },
    {
      "epoch": 1.2221816855162349,
      "grad_norm": 0.25259941816329956,
      "learning_rate": 0.00018715076647358146,
      "loss": 0.0034,
      "step": 13400
    },
    {
      "epoch": 1.226742064939803,
      "grad_norm": 0.6439884901046753,
      "learning_rate": 0.00018667072038915734,
      "loss": 0.0023,
      "step": 13450
    },
    {
      "epoch": 1.231302444363371,
      "grad_norm": 0.5103828310966492,
      "learning_rate": 0.00018619067430473326,
      "loss": 0.002,
      "step": 13500
    },
    {
      "epoch": 1.2358628237869391,
      "grad_norm": 4278.16259765625,
      "learning_rate": 0.00018571062822030912,
      "loss": 0.005,
      "step": 13550
    },
    {
      "epoch": 1.2404232032105071,
      "grad_norm": 7101.34375,
      "learning_rate": 0.00018523058213588503,
      "loss": 0.003,
      "step": 13600
    },
    {
      "epoch": 1.2449835826340752,
      "grad_norm": 0.0789095088839531,
      "learning_rate": 0.00018475053605146094,
      "loss": 0.0038,
      "step": 13650
    },
    {
      "epoch": 1.2495439620576432,
      "grad_norm": 4.674648761749268,
      "learning_rate": 0.00018427048996703683,
      "loss": 0.0035,
      "step": 13700
    },
    {
      "epoch": 1.2541043414812112,
      "grad_norm": 238.70126342773438,
      "learning_rate": 0.00018379044388261271,
      "loss": 0.0025,
      "step": 13750
    },
    {
      "epoch": 1.2586647209047792,
      "grad_norm": 19936.51953125,
      "learning_rate": 0.0001833103977981886,
      "loss": 0.0032,
      "step": 13800
    },
    {
      "epoch": 1.2632251003283472,
      "grad_norm": 0.05251063406467438,
      "learning_rate": 0.0001828303517137645,
      "loss": 0.0029,
      "step": 13850
    },
    {
      "epoch": 1.2677854797519155,
      "grad_norm": 9255.0849609375,
      "learning_rate": 0.0001823503056293404,
      "loss": 0.0036,
      "step": 13900
    },
    {
      "epoch": 1.2723458591754833,
      "grad_norm": 0.7503611445426941,
      "learning_rate": 0.00018187025954491629,
      "loss": 0.003,
      "step": 13950
    },
    {
      "epoch": 1.2769062385990515,
      "grad_norm": 3907.708251953125,
      "learning_rate": 0.0001813902134604922,
      "loss": 0.0032,
      "step": 14000
    },
    {
      "epoch": 1.2814666180226195,
      "grad_norm": 6162.57568359375,
      "learning_rate": 0.00018091016737606808,
      "loss": 0.0048,
      "step": 14050
    },
    {
      "epoch": 1.2860269974461875,
      "grad_norm": 0.07699794322252274,
      "learning_rate": 0.000180430121291644,
      "loss": 0.0035,
      "step": 14100
    },
    {
      "epoch": 1.2905873768697556,
      "grad_norm": 23825.8671875,
      "learning_rate": 0.00017995007520721986,
      "loss": 0.0043,
      "step": 14150
    },
    {
      "epoch": 1.2951477562933236,
      "grad_norm": 1476.234619140625,
      "learning_rate": 0.00017947002912279577,
      "loss": 0.0035,
      "step": 14200
    },
    {
      "epoch": 1.2997081357168916,
      "grad_norm": 0.16046550869941711,
      "learning_rate": 0.00017898998303837166,
      "loss": 0.0021,
      "step": 14250
    },
    {
      "epoch": 1.3042685151404596,
      "grad_norm": 0.6982142925262451,
      "learning_rate": 0.00017850993695394757,
      "loss": 0.0031,
      "step": 14300
    },
    {
      "epoch": 1.3088288945640278,
      "grad_norm": 170.8339080810547,
      "learning_rate": 0.00017802989086952348,
      "loss": 0.004,
      "step": 14350
    },
    {
      "epoch": 1.3133892739875956,
      "grad_norm": 0.8098143935203552,
      "learning_rate": 0.00017754984478509934,
      "loss": 0.0031,
      "step": 14400
    },
    {
      "epoch": 1.3179496534111639,
      "grad_norm": 4232.1572265625,
      "learning_rate": 0.00017706979870067525,
      "loss": 0.0034,
      "step": 14450
    },
    {
      "epoch": 1.322510032834732,
      "grad_norm": 0.1110318973660469,
      "learning_rate": 0.00017658975261625114,
      "loss": 0.0036,
      "step": 14500
    },
    {
      "epoch": 1.3270704122583,
      "grad_norm": 0.04229433089494705,
      "learning_rate": 0.00017610970653182705,
      "loss": 0.0028,
      "step": 14550
    },
    {
      "epoch": 1.331630791681868,
      "grad_norm": 0.2250213623046875,
      "learning_rate": 0.0001756296604474029,
      "loss": 0.0034,
      "step": 14600
    },
    {
      "epoch": 1.336191171105436,
      "grad_norm": 0.026883434504270554,
      "learning_rate": 0.00017514961436297882,
      "loss": 0.0027,
      "step": 14650
    },
    {
      "epoch": 1.340751550529004,
      "grad_norm": 7930.41015625,
      "learning_rate": 0.00017466956827855474,
      "loss": 0.0045,
      "step": 14700
    },
    {
      "epoch": 1.345311929952572,
      "grad_norm": 2728.629638671875,
      "learning_rate": 0.00017418952219413062,
      "loss": 0.004,
      "step": 14750
    },
    {
      "epoch": 1.3498723093761402,
      "grad_norm": 8508.71875,
      "learning_rate": 0.00017370947610970654,
      "loss": 0.0023,
      "step": 14800
    },
    {
      "epoch": 1.354432688799708,
      "grad_norm": 38566.49609375,
      "learning_rate": 0.0001732294300252824,
      "loss": 0.0041,
      "step": 14850
    },
    {
      "epoch": 1.3589930682232763,
      "grad_norm": 2.113966703414917,
      "learning_rate": 0.0001727493839408583,
      "loss": 0.0032,
      "step": 14900
    },
    {
      "epoch": 1.3635534476468443,
      "grad_norm": 0.562562108039856,
      "learning_rate": 0.0001722693378564342,
      "loss": 0.0052,
      "step": 14950
    },
    {
      "epoch": 1.3681138270704123,
      "grad_norm": 4245.60791015625,
      "learning_rate": 0.0001717892917720101,
      "loss": 0.0032,
      "step": 15000
    },
    {
      "epoch": 1.3726742064939803,
      "grad_norm": 28937.55859375,
      "learning_rate": 0.000171309245687586,
      "loss": 0.0035,
      "step": 15050
    },
    {
      "epoch": 1.3772345859175483,
      "grad_norm": 1.2576286792755127,
      "learning_rate": 0.00017082919960316188,
      "loss": 0.0045,
      "step": 15100
    },
    {
      "epoch": 1.3817949653411163,
      "grad_norm": 29613.03125,
      "learning_rate": 0.0001703491535187378,
      "loss": 0.0028,
      "step": 15150
    },
    {
      "epoch": 1.3863553447646844,
      "grad_norm": 1444.068115234375,
      "learning_rate": 0.00016986910743431368,
      "loss": 0.0037,
      "step": 15200
    },
    {
      "epoch": 1.3909157241882526,
      "grad_norm": 18829.705078125,
      "learning_rate": 0.00016938906134988957,
      "loss": 0.0032,
      "step": 15250
    },
    {
      "epoch": 1.3954761036118204,
      "grad_norm": 0.3990632891654968,
      "learning_rate": 0.00016890901526546548,
      "loss": 0.0027,
      "step": 15300
    },
    {
      "epoch": 1.4000364830353886,
      "grad_norm": 11589.033203125,
      "learning_rate": 0.00016842896918104136,
      "loss": 0.0041,
      "step": 15350
    },
    {
      "epoch": 1.4045968624589567,
      "grad_norm": 0.37232938408851624,
      "learning_rate": 0.00016794892309661728,
      "loss": 0.0034,
      "step": 15400
    },
    {
      "epoch": 1.4091572418825247,
      "grad_norm": 0.3410988450050354,
      "learning_rate": 0.00016746887701219314,
      "loss": 0.0047,
      "step": 15450
    },
    {
      "epoch": 1.4137176213060927,
      "grad_norm": 31498.185546875,
      "learning_rate": 0.00016698883092776905,
      "loss": 0.0042,
      "step": 15500
    },
    {
      "epoch": 1.4182780007296607,
      "grad_norm": 27162.18359375,
      "learning_rate": 0.00016650878484334494,
      "loss": 0.0031,
      "step": 15550
    },
    {
      "epoch": 1.4228383801532287,
      "grad_norm": 0.0800720602273941,
      "learning_rate": 0.00016602873875892085,
      "loss": 0.003,
      "step": 15600
    },
    {
      "epoch": 1.4273987595767967,
      "grad_norm": 2521.1865234375,
      "learning_rate": 0.00016554869267449676,
      "loss": 0.0047,
      "step": 15650
    },
    {
      "epoch": 1.4319591390003648,
      "grad_norm": 0.04565214365720749,
      "learning_rate": 0.00016506864659007262,
      "loss": 0.0028,
      "step": 15700
    },
    {
      "epoch": 1.4365195184239328,
      "grad_norm": 0.026535307988524437,
      "learning_rate": 0.00016458860050564853,
      "loss": 0.0029,
      "step": 15750
    },
    {
      "epoch": 1.441079897847501,
      "grad_norm": 3634.86572265625,
      "learning_rate": 0.00016410855442122442,
      "loss": 0.0047,
      "step": 15800
    },
    {
      "epoch": 1.445640277271069,
      "grad_norm": 0.8684106469154358,
      "learning_rate": 0.00016362850833680033,
      "loss": 0.0037,
      "step": 15850
    },
    {
      "epoch": 1.450200656694637,
      "grad_norm": 0.2747885286808014,
      "learning_rate": 0.0001631484622523762,
      "loss": 0.004,
      "step": 15900
    },
    {
      "epoch": 1.454761036118205,
      "grad_norm": 24.457054138183594,
      "learning_rate": 0.0001626684161679521,
      "loss": 0.0037,
      "step": 15950
    },
    {
      "epoch": 1.459321415541773,
      "grad_norm": 8261.6796875,
      "learning_rate": 0.00016218837008352802,
      "loss": 0.0042,
      "step": 16000
    },
    {
      "epoch": 1.463881794965341,
      "grad_norm": 19835.951171875,
      "learning_rate": 0.0001617083239991039,
      "loss": 0.0032,
      "step": 16050
    },
    {
      "epoch": 1.4684421743889091,
      "grad_norm": 0.1319260448217392,
      "learning_rate": 0.00016122827791467982,
      "loss": 0.0027,
      "step": 16100
    },
    {
      "epoch": 1.4730025538124771,
      "grad_norm": 30241.6796875,
      "learning_rate": 0.00016074823183025568,
      "loss": 0.003,
      "step": 16150
    },
    {
      "epoch": 1.4775629332360452,
      "grad_norm": 25.72944450378418,
      "learning_rate": 0.0001602681857458316,
      "loss": 0.0036,
      "step": 16200
    },
    {
      "epoch": 1.4821233126596134,
      "grad_norm": 0.016656262800097466,
      "learning_rate": 0.00015978813966140748,
      "loss": 0.003,
      "step": 16250
    },
    {
      "epoch": 1.4866836920831812,
      "grad_norm": 18779.494140625,
      "learning_rate": 0.0001593080935769834,
      "loss": 0.0038,
      "step": 16300
    },
    {
      "epoch": 1.4912440715067494,
      "grad_norm": 20460.123046875,
      "learning_rate": 0.00015882804749255927,
      "loss": 0.0038,
      "step": 16350
    },
    {
      "epoch": 1.4958044509303174,
      "grad_norm": 0.9366283416748047,
      "learning_rate": 0.00015834800140813516,
      "loss": 0.0034,
      "step": 16400
    },
    {
      "epoch": 1.5003648303538855,
      "grad_norm": 0.020961785688996315,
      "learning_rate": 0.00015786795532371107,
      "loss": 0.002,
      "step": 16450
    },
    {
      "epoch": 1.5049252097774535,
      "grad_norm": 0.029149208217859268,
      "learning_rate": 0.00015738790923928696,
      "loss": 0.0034,
      "step": 16500
    },
    {
      "epoch": 1.5094855892010215,
      "grad_norm": 0.70670086145401,
      "learning_rate": 0.00015690786315486285,
      "loss": 0.0036,
      "step": 16550
    },
    {
      "epoch": 1.5140459686245895,
      "grad_norm": 0.11508173495531082,
      "learning_rate": 0.00015642781707043873,
      "loss": 0.0029,
      "step": 16600
    },
    {
      "epoch": 1.5186063480481575,
      "grad_norm": 0.1751098483800888,
      "learning_rate": 0.00015594777098601464,
      "loss": 0.0025,
      "step": 16650
    },
    {
      "epoch": 1.5231667274717258,
      "grad_norm": 0.02450675703585148,
      "learning_rate": 0.00015546772490159056,
      "loss": 0.0021,
      "step": 16700
    },
    {
      "epoch": 1.5277271068952936,
      "grad_norm": 0.032794125378131866,
      "learning_rate": 0.00015498767881716642,
      "loss": 0.0035,
      "step": 16750
    },
    {
      "epoch": 1.5322874863188618,
      "grad_norm": 0.01664322428405285,
      "learning_rate": 0.00015450763273274233,
      "loss": 0.0029,
      "step": 16800
    },
    {
      "epoch": 1.5368478657424298,
      "grad_norm": 0.13811558485031128,
      "learning_rate": 0.00015402758664831822,
      "loss": 0.0026,
      "step": 16850
    },
    {
      "epoch": 1.5414082451659978,
      "grad_norm": 11.69924259185791,
      "learning_rate": 0.00015354754056389413,
      "loss": 0.0035,
      "step": 16900
    },
    {
      "epoch": 1.5459686245895659,
      "grad_norm": 17029.1015625,
      "learning_rate": 0.00015306749447947004,
      "loss": 0.0023,
      "step": 16950
    },
    {
      "epoch": 1.5505290040131339,
      "grad_norm": 0.041776735335588455,
      "learning_rate": 0.0001525874483950459,
      "loss": 0.0033,
      "step": 17000
    },
    {
      "epoch": 1.5550893834367019,
      "grad_norm": 7442.642578125,
      "learning_rate": 0.00015210740231062181,
      "loss": 0.0027,
      "step": 17050
    },
    {
      "epoch": 1.55964976286027,
      "grad_norm": 45076.40234375,
      "learning_rate": 0.0001516273562261977,
      "loss": 0.0017,
      "step": 17100
    },
    {
      "epoch": 1.5642101422838381,
      "grad_norm": 3636.59521484375,
      "learning_rate": 0.0001511473101417736,
      "loss": 0.0042,
      "step": 17150
    },
    {
      "epoch": 1.568770521707406,
      "grad_norm": 2.850878953933716,
      "learning_rate": 0.00015066726405734947,
      "loss": 0.0024,
      "step": 17200
    },
    {
      "epoch": 1.5733309011309742,
      "grad_norm": 3882.6943359375,
      "learning_rate": 0.00015018721797292538,
      "loss": 0.0035,
      "step": 17250
    },
    {
      "epoch": 1.577891280554542,
      "grad_norm": 0.06388580054044724,
      "learning_rate": 0.00014970717188850127,
      "loss": 0.0031,
      "step": 17300
    },
    {
      "epoch": 1.5824516599781102,
      "grad_norm": 0.6590938568115234,
      "learning_rate": 0.00014922712580407718,
      "loss": 0.0027,
      "step": 17350
    },
    {
      "epoch": 1.5870120394016782,
      "grad_norm": 0.2934914827346802,
      "learning_rate": 0.00014874707971965307,
      "loss": 0.0033,
      "step": 17400
    },
    {
      "epoch": 1.5915724188252462,
      "grad_norm": 3608.79443359375,
      "learning_rate": 0.00014826703363522898,
      "loss": 0.0028,
      "step": 17450
    },
    {
      "epoch": 1.5961327982488143,
      "grad_norm": 62365.04296875,
      "learning_rate": 0.00014778698755080487,
      "loss": 0.0035,
      "step": 17500
    },
    {
      "epoch": 1.6006931776723823,
      "grad_norm": 36616.21875,
      "learning_rate": 0.00014730694146638076,
      "loss": 0.0032,
      "step": 17550
    },
    {
      "epoch": 1.6052535570959505,
      "grad_norm": 0.045033521950244904,
      "learning_rate": 0.00014682689538195667,
      "loss": 0.004,
      "step": 17600
    },
    {
      "epoch": 1.6098139365195183,
      "grad_norm": 0.19047175347805023,
      "learning_rate": 0.00014634684929753255,
      "loss": 0.0035,
      "step": 17650
    },
    {
      "epoch": 1.6143743159430866,
      "grad_norm": 8285.3671875,
      "learning_rate": 0.00014586680321310844,
      "loss": 0.0032,
      "step": 17700
    },
    {
      "epoch": 1.6189346953666544,
      "grad_norm": 0.05372479185461998,
      "learning_rate": 0.00014538675712868433,
      "loss": 0.0037,
      "step": 17750
    },
    {
      "epoch": 1.6234950747902226,
      "grad_norm": 0.06894957274198532,
      "learning_rate": 0.00014490671104426024,
      "loss": 0.0031,
      "step": 17800
    },
    {
      "epoch": 1.6280554542137906,
      "grad_norm": 22.193649291992188,
      "learning_rate": 0.00014442666495983613,
      "loss": 0.0028,
      "step": 17850
    },
    {
      "epoch": 1.6326158336373586,
      "grad_norm": 0.040129657834768295,
      "learning_rate": 0.00014394661887541204,
      "loss": 0.0023,
      "step": 17900
    },
    {
      "epoch": 1.6371762130609266,
      "grad_norm": 0.86887127161026,
      "learning_rate": 0.00014346657279098792,
      "loss": 0.0028,
      "step": 17950
    },
    {
      "epoch": 1.6417365924844947,
      "grad_norm": 0.03646788373589516,
      "learning_rate": 0.0001429865267065638,
      "loss": 0.0045,
      "step": 18000
    },
    {
      "epoch": 1.646296971908063,
      "grad_norm": 24146.234375,
      "learning_rate": 0.0001425064806221397,
      "loss": 0.0029,
      "step": 18050
    },
    {
      "epoch": 1.6508573513316307,
      "grad_norm": 7433.06103515625,
      "learning_rate": 0.0001420264345377156,
      "loss": 0.0025,
      "step": 18100
    },
    {
      "epoch": 1.655417730755199,
      "grad_norm": 0.10100560635328293,
      "learning_rate": 0.00014154638845329152,
      "loss": 0.0016,
      "step": 18150
    },
    {
      "epoch": 1.6599781101787667,
      "grad_norm": 15236.38671875,
      "learning_rate": 0.0001410663423688674,
      "loss": 0.0049,
      "step": 18200
    },
    {
      "epoch": 1.664538489602335,
      "grad_norm": 3761.83984375,
      "learning_rate": 0.0001405862962844433,
      "loss": 0.0031,
      "step": 18250
    },
    {
      "epoch": 1.669098869025903,
      "grad_norm": 0.026660175994038582,
      "learning_rate": 0.00014010625020001918,
      "loss": 0.0028,
      "step": 18300
    },
    {
      "epoch": 1.673659248449471,
      "grad_norm": 43.5379524230957,
      "learning_rate": 0.0001396262041155951,
      "loss": 0.0029,
      "step": 18350
    },
    {
      "epoch": 1.678219627873039,
      "grad_norm": 4485.64599609375,
      "learning_rate": 0.00013914615803117098,
      "loss": 0.0021,
      "step": 18400
    },
    {
      "epoch": 1.682780007296607,
      "grad_norm": 20.341028213500977,
      "learning_rate": 0.0001386661119467469,
      "loss": 0.0018,
      "step": 18450
    },
    {
      "epoch": 1.6873403867201753,
      "grad_norm": 0.05637165904045105,
      "learning_rate": 0.00013818606586232278,
      "loss": 0.0032,
      "step": 18500
    },
    {
      "epoch": 1.691900766143743,
      "grad_norm": 0.07918210327625275,
      "learning_rate": 0.00013770601977789866,
      "loss": 0.0021,
      "step": 18550
    },
    {
      "epoch": 1.6964611455673113,
      "grad_norm": 5.140122413635254,
      "learning_rate": 0.00013722597369347455,
      "loss": 0.0013,
      "step": 18600
    },
    {
      "epoch": 1.701021524990879,
      "grad_norm": 0.4726804494857788,
      "learning_rate": 0.00013674592760905046,
      "loss": 0.0025,
      "step": 18650
    },
    {
      "epoch": 1.7055819044144473,
      "grad_norm": 0.006519020069390535,
      "learning_rate": 0.00013626588152462635,
      "loss": 0.0023,
      "step": 18700
    },
    {
      "epoch": 1.7101422838380154,
      "grad_norm": 16141.5556640625,
      "learning_rate": 0.00013578583544020224,
      "loss": 0.0033,
      "step": 18750
    },
    {
      "epoch": 1.7147026632615834,
      "grad_norm": 0.34066829085350037,
      "learning_rate": 0.00013530578935577815,
      "loss": 0.0038,
      "step": 18800
    },
    {
      "epoch": 1.7192630426851514,
      "grad_norm": 0.03895588591694832,
      "learning_rate": 0.00013482574327135404,
      "loss": 0.0036,
      "step": 18850
    },
    {
      "epoch": 1.7238234221087194,
      "grad_norm": 0.0550091415643692,
      "learning_rate": 0.00013434569718692995,
      "loss": 0.0029,
      "step": 18900
    },
    {
      "epoch": 1.7283838015322874,
      "grad_norm": 0.7973347902297974,
      "learning_rate": 0.00013386565110250583,
      "loss": 0.0025,
      "step": 18950
    },
    {
      "epoch": 1.7329441809558555,
      "grad_norm": 35276.36328125,
      "learning_rate": 0.00013338560501808172,
      "loss": 0.004,
      "step": 19000
    },
    {
      "epoch": 1.7375045603794237,
      "grad_norm": 1.1493160724639893,
      "learning_rate": 0.0001329055589336576,
      "loss": 0.0027,
      "step": 19050
    },
    {
      "epoch": 1.7420649398029915,
      "grad_norm": 0.051426179707050323,
      "learning_rate": 0.00013242551284923352,
      "loss": 0.0038,
      "step": 19100
    },
    {
      "epoch": 1.7466253192265597,
      "grad_norm": 4459.54638671875,
      "learning_rate": 0.0001319454667648094,
      "loss": 0.003,
      "step": 19150
    },
    {
      "epoch": 1.7511856986501277,
      "grad_norm": 4538.22802734375,
      "learning_rate": 0.00013146542068038532,
      "loss": 0.0026,
      "step": 19200
    },
    {
      "epoch": 1.7557460780736958,
      "grad_norm": 1.5835508108139038,
      "learning_rate": 0.0001309853745959612,
      "loss": 0.0038,
      "step": 19250
    },
    {
      "epoch": 1.7603064574972638,
      "grad_norm": 0.1683024913072586,
      "learning_rate": 0.0001305053285115371,
      "loss": 0.004,
      "step": 19300
    },
    {
      "epoch": 1.7648668369208318,
      "grad_norm": 0.14233067631721497,
      "learning_rate": 0.00013002528242711298,
      "loss": 0.0034,
      "step": 19350
    },
    {
      "epoch": 1.7694272163443998,
      "grad_norm": 36686.96484375,
      "learning_rate": 0.0001295452363426889,
      "loss": 0.0032,
      "step": 19400
    },
    {
      "epoch": 1.7739875957679678,
      "grad_norm": 0.30023568868637085,
      "learning_rate": 0.00012906519025826478,
      "loss": 0.0035,
      "step": 19450
    },
    {
      "epoch": 1.778547975191536,
      "grad_norm": 2.329025983810425,
      "learning_rate": 0.0001285851441738407,
      "loss": 0.0038,
      "step": 19500
    },
    {
      "epoch": 1.7831083546151039,
      "grad_norm": 0.04244843125343323,
      "learning_rate": 0.00012810509808941657,
      "loss": 0.004,
      "step": 19550
    },
    {
      "epoch": 1.787668734038672,
      "grad_norm": 44883.90234375,
      "learning_rate": 0.00012762505200499246,
      "loss": 0.0039,
      "step": 19600
    },
    {
      "epoch": 1.79222911346224,
      "grad_norm": 18669.72265625,
      "learning_rate": 0.00012714500592056837,
      "loss": 0.0038,
      "step": 19650
    },
    {
      "epoch": 1.7967894928858081,
      "grad_norm": 10.40857219696045,
      "learning_rate": 0.00012666495983614426,
      "loss": 0.0019,
      "step": 19700
    },
    {
      "epoch": 1.8013498723093762,
      "grad_norm": 0.16267651319503784,
      "learning_rate": 0.00012618491375172015,
      "loss": 0.002,
      "step": 19750
    },
    {
      "epoch": 1.8059102517329442,
      "grad_norm": 6609.6640625,
      "learning_rate": 0.00012570486766729606,
      "loss": 0.0024,
      "step": 19800
    },
    {
      "epoch": 1.8104706311565122,
      "grad_norm": 0.3650516867637634,
      "learning_rate": 0.00012522482158287194,
      "loss": 0.0039,
      "step": 19850
    },
    {
      "epoch": 1.8150310105800802,
      "grad_norm": 3865.26708984375,
      "learning_rate": 0.00012474477549844783,
      "loss": 0.0023,
      "step": 19900
    },
    {
      "epoch": 1.8195913900036484,
      "grad_norm": 4614.8798828125,
      "learning_rate": 0.00012426472941402374,
      "loss": 0.0036,
      "step": 19950
    },
    {
      "epoch": 1.8241517694272162,
      "grad_norm": 38064.40234375,
      "learning_rate": 0.00012378468332959963,
      "loss": 0.0041,
      "step": 20000
    },
    {
      "epoch": 1.8287121488507845,
      "grad_norm": 2.6146039962768555,
      "learning_rate": 0.00012330463724517552,
      "loss": 0.0026,
      "step": 20050
    },
    {
      "epoch": 1.8332725282743523,
      "grad_norm": 0.08844021707773209,
      "learning_rate": 0.00012282459116075143,
      "loss": 0.0016,
      "step": 20100
    },
    {
      "epoch": 1.8378329076979205,
      "grad_norm": 0.3553329408168793,
      "learning_rate": 0.00012234454507632731,
      "loss": 0.003,
      "step": 20150
    },
    {
      "epoch": 1.8423932871214885,
      "grad_norm": 0.7090244293212891,
      "learning_rate": 0.00012186449899190321,
      "loss": 0.002,
      "step": 20200
    },
    {
      "epoch": 1.8469536665450565,
      "grad_norm": 4231.20458984375,
      "learning_rate": 0.0001213844529074791,
      "loss": 0.0028,
      "step": 20250
    },
    {
      "epoch": 1.8515140459686246,
      "grad_norm": 0.053012121468782425,
      "learning_rate": 0.000120904406823055,
      "loss": 0.0028,
      "step": 20300
    },
    {
      "epoch": 1.8560744253921926,
      "grad_norm": 0.34640631079673767,
      "learning_rate": 0.00012042436073863089,
      "loss": 0.0025,
      "step": 20350
    },
    {
      "epoch": 1.8606348048157608,
      "grad_norm": 0.5880560278892517,
      "learning_rate": 0.00011994431465420679,
      "loss": 0.0035,
      "step": 20400
    },
    {
      "epoch": 1.8651951842393286,
      "grad_norm": 14996.9990234375,
      "learning_rate": 0.0001194642685697827,
      "loss": 0.0035,
      "step": 20450
    },
    {
      "epoch": 1.8697555636628969,
      "grad_norm": 11031.048828125,
      "learning_rate": 0.00011898422248535858,
      "loss": 0.0027,
      "step": 20500
    },
    {
      "epoch": 1.8743159430864647,
      "grad_norm": 75223.4765625,
      "learning_rate": 0.00011850417640093448,
      "loss": 0.0033,
      "step": 20550
    },
    {
      "epoch": 1.878876322510033,
      "grad_norm": 604.8280029296875,
      "learning_rate": 0.00011802413031651037,
      "loss": 0.0033,
      "step": 20600
    },
    {
      "epoch": 1.883436701933601,
      "grad_norm": 0.1404799222946167,
      "learning_rate": 0.00011754408423208627,
      "loss": 0.0027,
      "step": 20650
    },
    {
      "epoch": 1.887997081357169,
      "grad_norm": 3707.977294921875,
      "learning_rate": 0.00011706403814766216,
      "loss": 0.0039,
      "step": 20700
    },
    {
      "epoch": 1.892557460780737,
      "grad_norm": 0.1688435971736908,
      "learning_rate": 0.00011658399206323807,
      "loss": 0.0027,
      "step": 20750
    },
    {
      "epoch": 1.897117840204305,
      "grad_norm": 0.2943316698074341,
      "learning_rate": 0.00011610394597881395,
      "loss": 0.0017,
      "step": 20800
    },
    {
      "epoch": 1.9016782196278732,
      "grad_norm": 0.19117026031017303,
      "learning_rate": 0.00011562389989438985,
      "loss": 0.0033,
      "step": 20850
    },
    {
      "epoch": 1.906238599051441,
      "grad_norm": 0.05698743835091591,
      "learning_rate": 0.00011514385380996574,
      "loss": 0.0019,
      "step": 20900
    },
    {
      "epoch": 1.9107989784750092,
      "grad_norm": 5.793420314788818,
      "learning_rate": 0.00011466380772554164,
      "loss": 0.0037,
      "step": 20950
    },
    {
      "epoch": 1.915359357898577,
      "grad_norm": 6254.2705078125,
      "learning_rate": 0.00011418376164111753,
      "loss": 0.0037,
      "step": 21000
    },
    {
      "epoch": 1.9199197373221453,
      "grad_norm": 0.05002039298415184,
      "learning_rate": 0.00011370371555669343,
      "loss": 0.0021,
      "step": 21050
    },
    {
      "epoch": 1.9244801167457133,
      "grad_norm": 1.3335723876953125,
      "learning_rate": 0.00011322366947226934,
      "loss": 0.0032,
      "step": 21100
    },
    {
      "epoch": 1.9290404961692813,
      "grad_norm": 0.07933121174573898,
      "learning_rate": 0.00011274362338784522,
      "loss": 0.0027,
      "step": 21150
    },
    {
      "epoch": 1.9336008755928493,
      "grad_norm": 6266.72314453125,
      "learning_rate": 0.00011226357730342112,
      "loss": 0.0027,
      "step": 21200
    },
    {
      "epoch": 1.9381612550164173,
      "grad_norm": 0.01566566713154316,
      "learning_rate": 0.00011178353121899701,
      "loss": 0.003,
      "step": 21250
    },
    {
      "epoch": 1.9427216344399854,
      "grad_norm": 3384.154541015625,
      "learning_rate": 0.00011130348513457291,
      "loss": 0.0028,
      "step": 21300
    },
    {
      "epoch": 1.9472820138635534,
      "grad_norm": 0.01317903958261013,
      "learning_rate": 0.0001108234390501488,
      "loss": 0.0038,
      "step": 21350
    },
    {
      "epoch": 1.9518423932871216,
      "grad_norm": 8.292830467224121,
      "learning_rate": 0.0001103433929657247,
      "loss": 0.0027,
      "step": 21400
    },
    {
      "epoch": 1.9564027727106894,
      "grad_norm": 0.1222776547074318,
      "learning_rate": 0.0001098633468813006,
      "loss": 0.0021,
      "step": 21450
    },
    {
      "epoch": 1.9609631521342576,
      "grad_norm": 0.02265009470283985,
      "learning_rate": 0.0001093833007968765,
      "loss": 0.0017,
      "step": 21500
    },
    {
      "epoch": 1.9655235315578257,
      "grad_norm": 0.1876453459262848,
      "learning_rate": 0.00010890325471245238,
      "loss": 0.003,
      "step": 21550
    },
    {
      "epoch": 1.9700839109813937,
      "grad_norm": 0.1137092337012291,
      "learning_rate": 0.00010842320862802828,
      "loss": 0.0043,
      "step": 21600
    },
    {
      "epoch": 1.9746442904049617,
      "grad_norm": 38731.265625,
      "learning_rate": 0.00010794316254360417,
      "loss": 0.0028,
      "step": 21650
    },
    {
      "epoch": 1.9792046698285297,
      "grad_norm": 0.6806749105453491,
      "learning_rate": 0.00010746311645918007,
      "loss": 0.003,
      "step": 21700
    },
    {
      "epoch": 1.9837650492520977,
      "grad_norm": 0.05217931419610977,
      "learning_rate": 0.00010698307037475598,
      "loss": 0.0017,
      "step": 21750
    },
    {
      "epoch": 1.9883254286756658,
      "grad_norm": 0.06577259302139282,
      "learning_rate": 0.00010650302429033186,
      "loss": 0.0029,
      "step": 21800
    },
    {
      "epoch": 1.992885808099234,
      "grad_norm": 19563.8671875,
      "learning_rate": 0.00010602297820590776,
      "loss": 0.0031,
      "step": 21850
    },
    {
      "epoch": 1.9974461875228018,
      "grad_norm": 4579.1337890625,
      "learning_rate": 0.00010554293212148365,
      "loss": 0.0041,
      "step": 21900
    }
  ],
  "logging_steps": 50,
  "max_steps": 32892,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.2609098225680384e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
