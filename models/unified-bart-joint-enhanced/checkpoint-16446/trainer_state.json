{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 16446,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009120758847136081,
      "grad_norm": 5936224.0,
      "learning_rate": 4.4687642498860014e-07,
      "loss": 37.961,
      "step": 50
    },
    {
      "epoch": 0.018241517694272163,
      "grad_norm": 3609146.0,
      "learning_rate": 9.028727770177839e-07,
      "loss": 34.6913,
      "step": 100
    },
    {
      "epoch": 0.027362276541408246,
      "grad_norm": 3565987.75,
      "learning_rate": 1.3588691290469677e-06,
      "loss": 31.0193,
      "step": 150
    },
    {
      "epoch": 0.036483035388544326,
      "grad_norm": 6580893.0,
      "learning_rate": 1.8148654810761515e-06,
      "loss": 27.2476,
      "step": 200
    },
    {
      "epoch": 0.04560379423568041,
      "grad_norm": 7123704.5,
      "learning_rate": 2.2708618331053354e-06,
      "loss": 20.2083,
      "step": 250
    },
    {
      "epoch": 0.05472455308281649,
      "grad_norm": 7122810.0,
      "learning_rate": 2.7268581851345193e-06,
      "loss": 16.0538,
      "step": 300
    },
    {
      "epoch": 0.06384531192995258,
      "grad_norm": 7168265.0,
      "learning_rate": 3.1828545371637033e-06,
      "loss": 13.6815,
      "step": 350
    },
    {
      "epoch": 0.07296607077708865,
      "grad_norm": 7192185.5,
      "learning_rate": 3.638850889192887e-06,
      "loss": 11.8997,
      "step": 400
    },
    {
      "epoch": 0.08208682962422474,
      "grad_norm": 7115527.0,
      "learning_rate": 4.094847241222071e-06,
      "loss": 10.2057,
      "step": 450
    },
    {
      "epoch": 0.09120758847136082,
      "grad_norm": 6981652.0,
      "learning_rate": 4.550843593251254e-06,
      "loss": 8.6633,
      "step": 500
    },
    {
      "epoch": 0.1003283473184969,
      "grad_norm": 6811356.5,
      "learning_rate": 5.006839945280439e-06,
      "loss": 7.278,
      "step": 550
    },
    {
      "epoch": 0.10944910616563298,
      "grad_norm": 5803515.5,
      "learning_rate": 5.462836297309621e-06,
      "loss": 5.8033,
      "step": 600
    },
    {
      "epoch": 0.11856986501276906,
      "grad_norm": 4586944.0,
      "learning_rate": 5.918832649338806e-06,
      "loss": 4.4876,
      "step": 650
    },
    {
      "epoch": 0.12769062385990515,
      "grad_norm": 2988665.0,
      "learning_rate": 6.374829001367989e-06,
      "loss": 3.4683,
      "step": 700
    },
    {
      "epoch": 0.1368113827070412,
      "grad_norm": 1596873.125,
      "learning_rate": 6.830825353397174e-06,
      "loss": 2.6278,
      "step": 750
    },
    {
      "epoch": 0.1459321415541773,
      "grad_norm": 823695.4375,
      "learning_rate": 7.286821705426357e-06,
      "loss": 2.1076,
      "step": 800
    },
    {
      "epoch": 0.1550529004013134,
      "grad_norm": 687583.375,
      "learning_rate": 7.742818057455541e-06,
      "loss": 1.3217,
      "step": 850
    },
    {
      "epoch": 0.16417365924844948,
      "grad_norm": 387442.53125,
      "learning_rate": 8.198814409484724e-06,
      "loss": 0.8887,
      "step": 900
    },
    {
      "epoch": 0.17329441809558555,
      "grad_norm": 807059.5,
      "learning_rate": 8.654810761513909e-06,
      "loss": 0.7782,
      "step": 950
    },
    {
      "epoch": 0.18241517694272164,
      "grad_norm": 841452.3125,
      "learning_rate": 9.110807113543091e-06,
      "loss": 0.5813,
      "step": 1000
    },
    {
      "epoch": 0.19153593578985773,
      "grad_norm": 442154.78125,
      "learning_rate": 9.566803465572276e-06,
      "loss": 0.5491,
      "step": 1050
    },
    {
      "epoch": 0.2006566946369938,
      "grad_norm": 597384.6875,
      "learning_rate": 1.002279981760146e-05,
      "loss": 0.5095,
      "step": 1100
    },
    {
      "epoch": 0.20977745348412988,
      "grad_norm": 700358.75,
      "learning_rate": 1.0478796169630643e-05,
      "loss": 0.5135,
      "step": 1150
    },
    {
      "epoch": 0.21889821233126597,
      "grad_norm": 762592.875,
      "learning_rate": 1.0934792521659827e-05,
      "loss": 0.4396,
      "step": 1200
    },
    {
      "epoch": 0.22801897117840203,
      "grad_norm": 711147.125,
      "learning_rate": 1.1390788873689011e-05,
      "loss": 0.4072,
      "step": 1250
    },
    {
      "epoch": 0.23713973002553812,
      "grad_norm": 471668.34375,
      "learning_rate": 1.1846785225718196e-05,
      "loss": 0.4217,
      "step": 1300
    },
    {
      "epoch": 0.2462604888726742,
      "grad_norm": 353904.5625,
      "learning_rate": 1.2302781577747378e-05,
      "loss": 0.4368,
      "step": 1350
    },
    {
      "epoch": 0.2553812477198103,
      "grad_norm": 612999.625,
      "learning_rate": 1.2758777929776563e-05,
      "loss": 0.3692,
      "step": 1400
    },
    {
      "epoch": 0.2645020065669464,
      "grad_norm": 400021.6875,
      "learning_rate": 1.3214774281805747e-05,
      "loss": 0.381,
      "step": 1450
    },
    {
      "epoch": 0.2736227654140824,
      "grad_norm": 655072.375,
      "learning_rate": 1.3670770633834932e-05,
      "loss": 0.3679,
      "step": 1500
    },
    {
      "epoch": 0.2827435242612185,
      "grad_norm": 198947.984375,
      "learning_rate": 1.4126766985864113e-05,
      "loss": 0.4183,
      "step": 1550
    },
    {
      "epoch": 0.2918642831083546,
      "grad_norm": 545327.5,
      "learning_rate": 1.4582763337893297e-05,
      "loss": 0.3756,
      "step": 1600
    },
    {
      "epoch": 0.3009850419554907,
      "grad_norm": 476617.8125,
      "learning_rate": 1.5038759689922481e-05,
      "loss": 0.3379,
      "step": 1650
    },
    {
      "epoch": 0.3101058008026268,
      "grad_norm": 638888.375,
      "learning_rate": 1.5494756041951667e-05,
      "loss": 0.3547,
      "step": 1700
    },
    {
      "epoch": 0.3192265596497629,
      "grad_norm": 437538.875,
      "learning_rate": 1.595075239398085e-05,
      "loss": 0.3613,
      "step": 1750
    },
    {
      "epoch": 0.32834731849689897,
      "grad_norm": 565889.4375,
      "learning_rate": 1.6406748746010033e-05,
      "loss": 0.3084,
      "step": 1800
    },
    {
      "epoch": 0.337468077344035,
      "grad_norm": 689763.625,
      "learning_rate": 1.686274509803922e-05,
      "loss": 0.3319,
      "step": 1850
    },
    {
      "epoch": 0.3465888361911711,
      "grad_norm": 659720.75,
      "learning_rate": 1.73187414500684e-05,
      "loss": 0.2794,
      "step": 1900
    },
    {
      "epoch": 0.3557095950383072,
      "grad_norm": 410950.34375,
      "learning_rate": 1.7774737802097584e-05,
      "loss": 0.31,
      "step": 1950
    },
    {
      "epoch": 0.36483035388544327,
      "grad_norm": 377901.875,
      "learning_rate": 1.8230734154126767e-05,
      "loss": 0.2641,
      "step": 2000
    },
    {
      "epoch": 0.37395111273257936,
      "grad_norm": 69726.6484375,
      "learning_rate": 1.8686730506155953e-05,
      "loss": 0.3077,
      "step": 2050
    },
    {
      "epoch": 0.38307187157971545,
      "grad_norm": 512561.8125,
      "learning_rate": 1.9142726858185135e-05,
      "loss": 0.2944,
      "step": 2100
    },
    {
      "epoch": 0.39219263042685154,
      "grad_norm": 537200.125,
      "learning_rate": 1.9598723210214318e-05,
      "loss": 0.2898,
      "step": 2150
    },
    {
      "epoch": 0.4013133892739876,
      "grad_norm": 501728.40625,
      "learning_rate": 1.9993919432480366e-05,
      "loss": 0.2421,
      "step": 2200
    },
    {
      "epoch": 0.41043414812112367,
      "grad_norm": 499459.9375,
      "learning_rate": 1.9943248036483407e-05,
      "loss": 0.2521,
      "step": 2250
    },
    {
      "epoch": 0.41955490696825976,
      "grad_norm": 439026.53125,
      "learning_rate": 1.9892576640486448e-05,
      "loss": 0.2482,
      "step": 2300
    },
    {
      "epoch": 0.42867566581539585,
      "grad_norm": 508896.6875,
      "learning_rate": 1.9841905244489488e-05,
      "loss": 0.2579,
      "step": 2350
    },
    {
      "epoch": 0.43779642466253194,
      "grad_norm": 1120810.125,
      "learning_rate": 1.979123384849253e-05,
      "loss": 0.2382,
      "step": 2400
    },
    {
      "epoch": 0.446917183509668,
      "grad_norm": 468299.5,
      "learning_rate": 1.974056245249557e-05,
      "loss": 0.2531,
      "step": 2450
    },
    {
      "epoch": 0.45603794235680406,
      "grad_norm": 878376.875,
      "learning_rate": 1.968989105649861e-05,
      "loss": 0.2357,
      "step": 2500
    },
    {
      "epoch": 0.46515870120394015,
      "grad_norm": 135160.609375,
      "learning_rate": 1.963921966050165e-05,
      "loss": 0.2343,
      "step": 2550
    },
    {
      "epoch": 0.47427946005107624,
      "grad_norm": 270697.0,
      "learning_rate": 1.9588548264504688e-05,
      "loss": 0.2641,
      "step": 2600
    },
    {
      "epoch": 0.48340021889821233,
      "grad_norm": 697387.4375,
      "learning_rate": 1.953787686850773e-05,
      "loss": 0.2206,
      "step": 2650
    },
    {
      "epoch": 0.4925209777453484,
      "grad_norm": 374661.125,
      "learning_rate": 1.948720547251077e-05,
      "loss": 0.2651,
      "step": 2700
    },
    {
      "epoch": 0.5016417365924845,
      "grad_norm": 370686.71875,
      "learning_rate": 1.943653407651381e-05,
      "loss": 0.2287,
      "step": 2750
    },
    {
      "epoch": 0.5107624954396206,
      "grad_norm": 906469.8125,
      "learning_rate": 1.938586268051685e-05,
      "loss": 0.2224,
      "step": 2800
    },
    {
      "epoch": 0.5198832542867566,
      "grad_norm": 351443.28125,
      "learning_rate": 1.933519128451989e-05,
      "loss": 0.2629,
      "step": 2850
    },
    {
      "epoch": 0.5290040131338928,
      "grad_norm": 925489.625,
      "learning_rate": 1.9284519888522932e-05,
      "loss": 0.2387,
      "step": 2900
    },
    {
      "epoch": 0.5381247719810288,
      "grad_norm": 662768.375,
      "learning_rate": 1.923384849252597e-05,
      "loss": 0.2902,
      "step": 2950
    },
    {
      "epoch": 0.5472455308281649,
      "grad_norm": 27612.37109375,
      "learning_rate": 1.918317709652901e-05,
      "loss": 0.2399,
      "step": 3000
    },
    {
      "epoch": 0.556366289675301,
      "grad_norm": 465642.96875,
      "learning_rate": 1.913250570053205e-05,
      "loss": 0.2917,
      "step": 3050
    },
    {
      "epoch": 0.565487048522437,
      "grad_norm": 110167.90625,
      "learning_rate": 1.908183430453509e-05,
      "loss": 0.2652,
      "step": 3100
    },
    {
      "epoch": 0.5746078073695732,
      "grad_norm": 926643.0625,
      "learning_rate": 1.903116290853813e-05,
      "loss": 0.2123,
      "step": 3150
    },
    {
      "epoch": 0.5837285662167092,
      "grad_norm": 237950.4375,
      "learning_rate": 1.8980491512541172e-05,
      "loss": 0.2494,
      "step": 3200
    },
    {
      "epoch": 0.5928493250638454,
      "grad_norm": 573800.4375,
      "learning_rate": 1.8929820116544213e-05,
      "loss": 0.1816,
      "step": 3250
    },
    {
      "epoch": 0.6019700839109814,
      "grad_norm": 499455.40625,
      "learning_rate": 1.8879148720547254e-05,
      "loss": 0.2127,
      "step": 3300
    },
    {
      "epoch": 0.6110908427581174,
      "grad_norm": 645332.0625,
      "learning_rate": 1.8828477324550294e-05,
      "loss": 0.2139,
      "step": 3350
    },
    {
      "epoch": 0.6202116016052536,
      "grad_norm": 570036.8125,
      "learning_rate": 1.8777805928553335e-05,
      "loss": 0.2495,
      "step": 3400
    },
    {
      "epoch": 0.6293323604523896,
      "grad_norm": 571552.0625,
      "learning_rate": 1.8727134532556372e-05,
      "loss": 0.2361,
      "step": 3450
    },
    {
      "epoch": 0.6384531192995258,
      "grad_norm": 685822.875,
      "learning_rate": 1.8676463136559413e-05,
      "loss": 0.2217,
      "step": 3500
    },
    {
      "epoch": 0.6475738781466618,
      "grad_norm": 450229.59375,
      "learning_rate": 1.8625791740562453e-05,
      "loss": 0.2005,
      "step": 3550
    },
    {
      "epoch": 0.6566946369937979,
      "grad_norm": 291205.15625,
      "learning_rate": 1.8575120344565494e-05,
      "loss": 0.1855,
      "step": 3600
    },
    {
      "epoch": 0.665815395840934,
      "grad_norm": 568229.8125,
      "learning_rate": 1.8524448948568535e-05,
      "loss": 0.1757,
      "step": 3650
    },
    {
      "epoch": 0.67493615468807,
      "grad_norm": 418715.6875,
      "learning_rate": 1.8473777552571575e-05,
      "loss": 0.2017,
      "step": 3700
    },
    {
      "epoch": 0.6840569135352061,
      "grad_norm": 127682.7265625,
      "learning_rate": 1.8423106156574616e-05,
      "loss": 0.186,
      "step": 3750
    },
    {
      "epoch": 0.6931776723823422,
      "grad_norm": 795673.5,
      "learning_rate": 1.8372434760577657e-05,
      "loss": 0.2412,
      "step": 3800
    },
    {
      "epoch": 0.7022984312294783,
      "grad_norm": 65337.98828125,
      "learning_rate": 1.8321763364580697e-05,
      "loss": 0.2104,
      "step": 3850
    },
    {
      "epoch": 0.7114191900766144,
      "grad_norm": 618753.0625,
      "learning_rate": 1.8271091968583738e-05,
      "loss": 0.2262,
      "step": 3900
    },
    {
      "epoch": 0.7205399489237505,
      "grad_norm": 297139.34375,
      "learning_rate": 1.8220420572586775e-05,
      "loss": 0.2023,
      "step": 3950
    },
    {
      "epoch": 0.7296607077708865,
      "grad_norm": 860036.0625,
      "learning_rate": 1.8169749176589816e-05,
      "loss": 0.2153,
      "step": 4000
    },
    {
      "epoch": 0.7387814666180226,
      "grad_norm": 385338.96875,
      "learning_rate": 1.8119077780592856e-05,
      "loss": 0.202,
      "step": 4050
    },
    {
      "epoch": 0.7479022254651587,
      "grad_norm": 175294.28125,
      "learning_rate": 1.8068406384595897e-05,
      "loss": 0.1806,
      "step": 4100
    },
    {
      "epoch": 0.7570229843122948,
      "grad_norm": 92467.28125,
      "learning_rate": 1.8017734988598938e-05,
      "loss": 0.1844,
      "step": 4150
    },
    {
      "epoch": 0.7661437431594309,
      "grad_norm": 462566.28125,
      "learning_rate": 1.7967063592601978e-05,
      "loss": 0.1634,
      "step": 4200
    },
    {
      "epoch": 0.7752645020065669,
      "grad_norm": 131794.96875,
      "learning_rate": 1.7916392196605015e-05,
      "loss": 0.2244,
      "step": 4250
    },
    {
      "epoch": 0.7843852608537031,
      "grad_norm": 96948.9453125,
      "learning_rate": 1.7865720800608056e-05,
      "loss": 0.2306,
      "step": 4300
    },
    {
      "epoch": 0.7935060197008391,
      "grad_norm": 149633.703125,
      "learning_rate": 1.7815049404611097e-05,
      "loss": 0.1741,
      "step": 4350
    },
    {
      "epoch": 0.8026267785479752,
      "grad_norm": 111322.9765625,
      "learning_rate": 1.7764378008614137e-05,
      "loss": 0.222,
      "step": 4400
    },
    {
      "epoch": 0.8117475373951113,
      "grad_norm": 729147.625,
      "learning_rate": 1.7713706612617178e-05,
      "loss": 0.2101,
      "step": 4450
    },
    {
      "epoch": 0.8208682962422473,
      "grad_norm": 703945.0,
      "learning_rate": 1.766303521662022e-05,
      "loss": 0.1854,
      "step": 4500
    },
    {
      "epoch": 0.8299890550893835,
      "grad_norm": 168290.296875,
      "learning_rate": 1.761236382062326e-05,
      "loss": 0.1955,
      "step": 4550
    },
    {
      "epoch": 0.8391098139365195,
      "grad_norm": 283606.25,
      "learning_rate": 1.75616924246263e-05,
      "loss": 0.2009,
      "step": 4600
    },
    {
      "epoch": 0.8482305727836555,
      "grad_norm": 268640.03125,
      "learning_rate": 1.751102102862934e-05,
      "loss": 0.1936,
      "step": 4650
    },
    {
      "epoch": 0.8573513316307917,
      "grad_norm": 524306.125,
      "learning_rate": 1.746034963263238e-05,
      "loss": 0.1827,
      "step": 4700
    },
    {
      "epoch": 0.8664720904779277,
      "grad_norm": 254491.09375,
      "learning_rate": 1.7409678236635422e-05,
      "loss": 0.2057,
      "step": 4750
    },
    {
      "epoch": 0.8755928493250639,
      "grad_norm": 924322.5625,
      "learning_rate": 1.7359006840638462e-05,
      "loss": 0.1701,
      "step": 4800
    },
    {
      "epoch": 0.8847136081721999,
      "grad_norm": 167627.0625,
      "learning_rate": 1.7308335444641503e-05,
      "loss": 0.2277,
      "step": 4850
    },
    {
      "epoch": 0.893834367019336,
      "grad_norm": 96051.21875,
      "learning_rate": 1.7257664048644544e-05,
      "loss": 0.1808,
      "step": 4900
    },
    {
      "epoch": 0.9029551258664721,
      "grad_norm": 991511.0,
      "learning_rate": 1.7206992652647584e-05,
      "loss": 0.2206,
      "step": 4950
    },
    {
      "epoch": 0.9120758847136081,
      "grad_norm": 285384.96875,
      "learning_rate": 1.715632125665062e-05,
      "loss": 0.2018,
      "step": 5000
    },
    {
      "epoch": 0.9211966435607443,
      "grad_norm": 22526.392578125,
      "learning_rate": 1.7105649860653662e-05,
      "loss": 0.1588,
      "step": 5050
    },
    {
      "epoch": 0.9303174024078803,
      "grad_norm": 1007282.75,
      "learning_rate": 1.7054978464656703e-05,
      "loss": 0.2094,
      "step": 5100
    },
    {
      "epoch": 0.9394381612550164,
      "grad_norm": 586393.3125,
      "learning_rate": 1.7004307068659744e-05,
      "loss": 0.2068,
      "step": 5150
    },
    {
      "epoch": 0.9485589201021525,
      "grad_norm": 523450.8125,
      "learning_rate": 1.6953635672662784e-05,
      "loss": 0.2073,
      "step": 5200
    },
    {
      "epoch": 0.9576796789492886,
      "grad_norm": 929890.1875,
      "learning_rate": 1.690296427666582e-05,
      "loss": 0.1751,
      "step": 5250
    },
    {
      "epoch": 0.9668004377964247,
      "grad_norm": 1103105.25,
      "learning_rate": 1.6852292880668862e-05,
      "loss": 0.1675,
      "step": 5300
    },
    {
      "epoch": 0.9759211966435607,
      "grad_norm": 198764.5625,
      "learning_rate": 1.6801621484671903e-05,
      "loss": 0.1951,
      "step": 5350
    },
    {
      "epoch": 0.9850419554906968,
      "grad_norm": 31200.3125,
      "learning_rate": 1.6750950088674943e-05,
      "loss": 0.216,
      "step": 5400
    },
    {
      "epoch": 0.9941627143378329,
      "grad_norm": 561323.5625,
      "learning_rate": 1.6700278692677984e-05,
      "loss": 0.1999,
      "step": 5450
    },
    {
      "epoch": 1.003283473184969,
      "grad_norm": 757494.1875,
      "learning_rate": 1.6649607296681025e-05,
      "loss": 0.159,
      "step": 5500
    },
    {
      "epoch": 1.0124042320321052,
      "grad_norm": 590224.625,
      "learning_rate": 1.6598935900684065e-05,
      "loss": 0.1656,
      "step": 5550
    },
    {
      "epoch": 1.0215249908792412,
      "grad_norm": 119205.25,
      "learning_rate": 1.6548264504687106e-05,
      "loss": 0.1813,
      "step": 5600
    },
    {
      "epoch": 1.0306457497263772,
      "grad_norm": 98343.6953125,
      "learning_rate": 1.6497593108690147e-05,
      "loss": 0.1775,
      "step": 5650
    },
    {
      "epoch": 1.0397665085735133,
      "grad_norm": 437897.34375,
      "learning_rate": 1.6446921712693187e-05,
      "loss": 0.1385,
      "step": 5700
    },
    {
      "epoch": 1.0488872674206493,
      "grad_norm": 41403.91015625,
      "learning_rate": 1.6396250316696228e-05,
      "loss": 0.1422,
      "step": 5750
    },
    {
      "epoch": 1.0580080262677856,
      "grad_norm": 416996.53125,
      "learning_rate": 1.634557892069927e-05,
      "loss": 0.2186,
      "step": 5800
    },
    {
      "epoch": 1.0671287851149216,
      "grad_norm": 524399.5625,
      "learning_rate": 1.6294907524702306e-05,
      "loss": 0.1505,
      "step": 5850
    },
    {
      "epoch": 1.0762495439620576,
      "grad_norm": 465325.0625,
      "learning_rate": 1.6244236128705346e-05,
      "loss": 0.1445,
      "step": 5900
    },
    {
      "epoch": 1.0853703028091937,
      "grad_norm": 75256.359375,
      "learning_rate": 1.6193564732708387e-05,
      "loss": 0.1292,
      "step": 5950
    },
    {
      "epoch": 1.09449106165633,
      "grad_norm": 337780.5,
      "learning_rate": 1.6142893336711428e-05,
      "loss": 0.1295,
      "step": 6000
    },
    {
      "epoch": 1.103611820503466,
      "grad_norm": 947174.5,
      "learning_rate": 1.6092221940714468e-05,
      "loss": 0.1642,
      "step": 6050
    },
    {
      "epoch": 1.112732579350602,
      "grad_norm": 800064.6875,
      "learning_rate": 1.604155054471751e-05,
      "loss": 0.1734,
      "step": 6100
    },
    {
      "epoch": 1.121853338197738,
      "grad_norm": 478807.5,
      "learning_rate": 1.599087914872055e-05,
      "loss": 0.1573,
      "step": 6150
    },
    {
      "epoch": 1.130974097044874,
      "grad_norm": 288940.875,
      "learning_rate": 1.594020775272359e-05,
      "loss": 0.1948,
      "step": 6200
    },
    {
      "epoch": 1.14009485589201,
      "grad_norm": 28141.294921875,
      "learning_rate": 1.588953635672663e-05,
      "loss": 0.1509,
      "step": 6250
    },
    {
      "epoch": 1.1492156147391464,
      "grad_norm": 1682346.125,
      "learning_rate": 1.5838864960729668e-05,
      "loss": 0.1471,
      "step": 6300
    },
    {
      "epoch": 1.1583363735862824,
      "grad_norm": 1039749.25,
      "learning_rate": 1.578819356473271e-05,
      "loss": 0.2062,
      "step": 6350
    },
    {
      "epoch": 1.1674571324334184,
      "grad_norm": 260467.796875,
      "learning_rate": 1.573752216873575e-05,
      "loss": 0.1461,
      "step": 6400
    },
    {
      "epoch": 1.1765778912805545,
      "grad_norm": 1509705.25,
      "learning_rate": 1.568685077273879e-05,
      "loss": 0.167,
      "step": 6450
    },
    {
      "epoch": 1.1856986501276907,
      "grad_norm": 82328.40625,
      "learning_rate": 1.563617937674183e-05,
      "loss": 0.1225,
      "step": 6500
    },
    {
      "epoch": 1.1948194089748267,
      "grad_norm": 19242.591796875,
      "learning_rate": 1.558550798074487e-05,
      "loss": 0.1785,
      "step": 6550
    },
    {
      "epoch": 1.2039401678219628,
      "grad_norm": 78780.625,
      "learning_rate": 1.5534836584747912e-05,
      "loss": 0.1669,
      "step": 6600
    },
    {
      "epoch": 1.2130609266690988,
      "grad_norm": 5044.451171875,
      "learning_rate": 1.548416518875095e-05,
      "loss": 0.166,
      "step": 6650
    },
    {
      "epoch": 1.2221816855162349,
      "grad_norm": 22234.224609375,
      "learning_rate": 1.543349379275399e-05,
      "loss": 0.1588,
      "step": 6700
    },
    {
      "epoch": 1.231302444363371,
      "grad_norm": 280961.125,
      "learning_rate": 1.538282239675703e-05,
      "loss": 0.1424,
      "step": 6750
    },
    {
      "epoch": 1.2404232032105071,
      "grad_norm": 549833.8125,
      "learning_rate": 1.533215100076007e-05,
      "loss": 0.167,
      "step": 6800
    },
    {
      "epoch": 1.2495439620576432,
      "grad_norm": 23013.283203125,
      "learning_rate": 1.5281479604763112e-05,
      "loss": 0.151,
      "step": 6850
    },
    {
      "epoch": 1.2586647209047792,
      "grad_norm": 573590.625,
      "learning_rate": 1.5230808208766152e-05,
      "loss": 0.159,
      "step": 6900
    },
    {
      "epoch": 1.2677854797519155,
      "grad_norm": 526045.125,
      "learning_rate": 1.5180136812769193e-05,
      "loss": 0.1913,
      "step": 6950
    },
    {
      "epoch": 1.2769062385990515,
      "grad_norm": 945480.25,
      "learning_rate": 1.5129465416772234e-05,
      "loss": 0.1628,
      "step": 7000
    },
    {
      "epoch": 1.2860269974461875,
      "grad_norm": 671979.6875,
      "learning_rate": 1.5078794020775274e-05,
      "loss": 0.2049,
      "step": 7050
    },
    {
      "epoch": 1.2951477562933236,
      "grad_norm": 473662.40625,
      "learning_rate": 1.5028122624778313e-05,
      "loss": 0.1791,
      "step": 7100
    },
    {
      "epoch": 1.3042685151404596,
      "grad_norm": 824720.25,
      "learning_rate": 1.4977451228781354e-05,
      "loss": 0.128,
      "step": 7150
    },
    {
      "epoch": 1.3133892739875956,
      "grad_norm": 133404.390625,
      "learning_rate": 1.4926779832784395e-05,
      "loss": 0.1659,
      "step": 7200
    },
    {
      "epoch": 1.322510032834732,
      "grad_norm": 439313.0,
      "learning_rate": 1.4876108436787435e-05,
      "loss": 0.1822,
      "step": 7250
    },
    {
      "epoch": 1.331630791681868,
      "grad_norm": 13203.1181640625,
      "learning_rate": 1.4825437040790476e-05,
      "loss": 0.1593,
      "step": 7300
    },
    {
      "epoch": 1.340751550529004,
      "grad_norm": 280689.34375,
      "learning_rate": 1.4774765644793516e-05,
      "loss": 0.1432,
      "step": 7350
    },
    {
      "epoch": 1.3498723093761402,
      "grad_norm": 1033610.0625,
      "learning_rate": 1.4724094248796557e-05,
      "loss": 0.1918,
      "step": 7400
    },
    {
      "epoch": 1.3589930682232763,
      "grad_norm": 495282.96875,
      "learning_rate": 1.4673422852799594e-05,
      "loss": 0.1778,
      "step": 7450
    },
    {
      "epoch": 1.3681138270704123,
      "grad_norm": 353377.9375,
      "learning_rate": 1.4622751456802635e-05,
      "loss": 0.1821,
      "step": 7500
    },
    {
      "epoch": 1.3772345859175483,
      "grad_norm": 32913.53125,
      "learning_rate": 1.4572080060805676e-05,
      "loss": 0.1832,
      "step": 7550
    },
    {
      "epoch": 1.3863553447646844,
      "grad_norm": 132365.265625,
      "learning_rate": 1.4521408664808716e-05,
      "loss": 0.1665,
      "step": 7600
    },
    {
      "epoch": 1.3954761036118204,
      "grad_norm": 82107.2421875,
      "learning_rate": 1.4470737268811757e-05,
      "loss": 0.167,
      "step": 7650
    },
    {
      "epoch": 1.4045968624589567,
      "grad_norm": 15999.71875,
      "learning_rate": 1.4420065872814798e-05,
      "loss": 0.1441,
      "step": 7700
    },
    {
      "epoch": 1.4137176213060927,
      "grad_norm": 901016.9375,
      "learning_rate": 1.4369394476817836e-05,
      "loss": 0.1701,
      "step": 7750
    },
    {
      "epoch": 1.4228383801532287,
      "grad_norm": 138710.6875,
      "learning_rate": 1.4318723080820877e-05,
      "loss": 0.1634,
      "step": 7800
    },
    {
      "epoch": 1.4319591390003648,
      "grad_norm": 520854.4375,
      "learning_rate": 1.4268051684823918e-05,
      "loss": 0.1893,
      "step": 7850
    },
    {
      "epoch": 1.441079897847501,
      "grad_norm": 173379.4375,
      "learning_rate": 1.4217380288826958e-05,
      "loss": 0.1396,
      "step": 7900
    },
    {
      "epoch": 1.450200656694637,
      "grad_norm": 62904.48046875,
      "learning_rate": 1.4166708892829999e-05,
      "loss": 0.1375,
      "step": 7950
    },
    {
      "epoch": 1.459321415541773,
      "grad_norm": 1227663.25,
      "learning_rate": 1.411603749683304e-05,
      "loss": 0.2117,
      "step": 8000
    },
    {
      "epoch": 1.4684421743889091,
      "grad_norm": 1724358.375,
      "learning_rate": 1.406536610083608e-05,
      "loss": 0.1412,
      "step": 8050
    },
    {
      "epoch": 1.4775629332360452,
      "grad_norm": 545782.875,
      "learning_rate": 1.4014694704839121e-05,
      "loss": 0.1394,
      "step": 8100
    },
    {
      "epoch": 1.4866836920831812,
      "grad_norm": 20171.197265625,
      "learning_rate": 1.396402330884216e-05,
      "loss": 0.1936,
      "step": 8150
    },
    {
      "epoch": 1.4958044509303174,
      "grad_norm": 364950.53125,
      "learning_rate": 1.39133519128452e-05,
      "loss": 0.1783,
      "step": 8200
    },
    {
      "epoch": 1.5049252097774535,
      "grad_norm": 7368.8564453125,
      "learning_rate": 1.386268051684824e-05,
      "loss": 0.1576,
      "step": 8250
    },
    {
      "epoch": 1.5140459686245895,
      "grad_norm": 55676.6640625,
      "learning_rate": 1.381200912085128e-05,
      "loss": 0.162,
      "step": 8300
    },
    {
      "epoch": 1.5231667274717258,
      "grad_norm": 9863.220703125,
      "learning_rate": 1.376133772485432e-05,
      "loss": 0.1343,
      "step": 8350
    },
    {
      "epoch": 1.5322874863188618,
      "grad_norm": 15558.63671875,
      "learning_rate": 1.371066632885736e-05,
      "loss": 0.1452,
      "step": 8400
    },
    {
      "epoch": 1.5414082451659978,
      "grad_norm": 216820.484375,
      "learning_rate": 1.36599949328604e-05,
      "loss": 0.1496,
      "step": 8450
    },
    {
      "epoch": 1.5505290040131339,
      "grad_norm": 865911.5625,
      "learning_rate": 1.3609323536863441e-05,
      "loss": 0.158,
      "step": 8500
    },
    {
      "epoch": 1.55964976286027,
      "grad_norm": 949935.875,
      "learning_rate": 1.3558652140866482e-05,
      "loss": 0.124,
      "step": 8550
    },
    {
      "epoch": 1.568770521707406,
      "grad_norm": 17069.2265625,
      "learning_rate": 1.3507980744869522e-05,
      "loss": 0.1756,
      "step": 8600
    },
    {
      "epoch": 1.577891280554542,
      "grad_norm": 8642.16796875,
      "learning_rate": 1.3457309348872563e-05,
      "loss": 0.1726,
      "step": 8650
    },
    {
      "epoch": 1.5870120394016782,
      "grad_norm": 17426.0703125,
      "learning_rate": 1.3406637952875603e-05,
      "loss": 0.1555,
      "step": 8700
    },
    {
      "epoch": 1.5961327982488143,
      "grad_norm": 1078521.75,
      "learning_rate": 1.3355966556878644e-05,
      "loss": 0.1452,
      "step": 8750
    },
    {
      "epoch": 1.6052535570959505,
      "grad_norm": 280474.6875,
      "learning_rate": 1.3305295160881683e-05,
      "loss": 0.1912,
      "step": 8800
    },
    {
      "epoch": 1.6143743159430866,
      "grad_norm": 837893.5625,
      "learning_rate": 1.3254623764884724e-05,
      "loss": 0.1308,
      "step": 8850
    },
    {
      "epoch": 1.6234950747902226,
      "grad_norm": 288069.34375,
      "learning_rate": 1.3203952368887764e-05,
      "loss": 0.1662,
      "step": 8900
    },
    {
      "epoch": 1.6326158336373586,
      "grad_norm": 235165.078125,
      "learning_rate": 1.3153280972890805e-05,
      "loss": 0.1406,
      "step": 8950
    },
    {
      "epoch": 1.6417365924844947,
      "grad_norm": 9408.2802734375,
      "learning_rate": 1.3102609576893846e-05,
      "loss": 0.1805,
      "step": 9000
    },
    {
      "epoch": 1.6508573513316307,
      "grad_norm": 814492.625,
      "learning_rate": 1.3051938180896883e-05,
      "loss": 0.134,
      "step": 9050
    },
    {
      "epoch": 1.6599781101787667,
      "grad_norm": 843453.4375,
      "learning_rate": 1.3001266784899924e-05,
      "loss": 0.1474,
      "step": 9100
    },
    {
      "epoch": 1.669098869025903,
      "grad_norm": 8126.31005859375,
      "learning_rate": 1.2950595388902964e-05,
      "loss": 0.1493,
      "step": 9150
    },
    {
      "epoch": 1.678219627873039,
      "grad_norm": 720370.3125,
      "learning_rate": 1.2899923992906005e-05,
      "loss": 0.1354,
      "step": 9200
    },
    {
      "epoch": 1.6873403867201753,
      "grad_norm": 412899.96875,
      "learning_rate": 1.2849252596909045e-05,
      "loss": 0.1669,
      "step": 9250
    },
    {
      "epoch": 1.6964611455673113,
      "grad_norm": 530867.625,
      "learning_rate": 1.2798581200912086e-05,
      "loss": 0.1028,
      "step": 9300
    },
    {
      "epoch": 1.7055819044144473,
      "grad_norm": 9509.8017578125,
      "learning_rate": 1.2747909804915127e-05,
      "loss": 0.1213,
      "step": 9350
    },
    {
      "epoch": 1.7147026632615834,
      "grad_norm": 117835.7265625,
      "learning_rate": 1.2697238408918167e-05,
      "loss": 0.157,
      "step": 9400
    },
    {
      "epoch": 1.7238234221087194,
      "grad_norm": 9566.6044921875,
      "learning_rate": 1.2646567012921206e-05,
      "loss": 0.1434,
      "step": 9450
    },
    {
      "epoch": 1.7329441809558555,
      "grad_norm": 122376.7578125,
      "learning_rate": 1.2595895616924247e-05,
      "loss": 0.1489,
      "step": 9500
    },
    {
      "epoch": 1.7420649398029915,
      "grad_norm": 12444.4609375,
      "learning_rate": 1.2545224220927288e-05,
      "loss": 0.1827,
      "step": 9550
    },
    {
      "epoch": 1.7511856986501277,
      "grad_norm": 209636.859375,
      "learning_rate": 1.2494552824930328e-05,
      "loss": 0.1334,
      "step": 9600
    },
    {
      "epoch": 1.7603064574972638,
      "grad_norm": 138764.125,
      "learning_rate": 1.2443881428933369e-05,
      "loss": 0.176,
      "step": 9650
    },
    {
      "epoch": 1.7694272163443998,
      "grad_norm": 1398038.125,
      "learning_rate": 1.239321003293641e-05,
      "loss": 0.1199,
      "step": 9700
    },
    {
      "epoch": 1.778547975191536,
      "grad_norm": 4111.6298828125,
      "learning_rate": 1.234253863693945e-05,
      "loss": 0.1465,
      "step": 9750
    },
    {
      "epoch": 1.787668734038672,
      "grad_norm": 672973.0625,
      "learning_rate": 1.229186724094249e-05,
      "loss": 0.1564,
      "step": 9800
    },
    {
      "epoch": 1.7967894928858081,
      "grad_norm": 875977.3125,
      "learning_rate": 1.2241195844945528e-05,
      "loss": 0.1393,
      "step": 9850
    },
    {
      "epoch": 1.8059102517329442,
      "grad_norm": 508638.21875,
      "learning_rate": 1.2190524448948569e-05,
      "loss": 0.1443,
      "step": 9900
    },
    {
      "epoch": 1.8150310105800802,
      "grad_norm": 311908.78125,
      "learning_rate": 1.213985305295161e-05,
      "loss": 0.1519,
      "step": 9950
    },
    {
      "epoch": 1.8241517694272162,
      "grad_norm": 31275.94921875,
      "learning_rate": 1.208918165695465e-05,
      "loss": 0.1562,
      "step": 10000
    },
    {
      "epoch": 1.8332725282743523,
      "grad_norm": 576478.3125,
      "learning_rate": 1.203851026095769e-05,
      "loss": 0.0994,
      "step": 10050
    },
    {
      "epoch": 1.8423932871214885,
      "grad_norm": 72607.15625,
      "learning_rate": 1.198783886496073e-05,
      "loss": 0.1282,
      "step": 10100
    },
    {
      "epoch": 1.8515140459686246,
      "grad_norm": 9223.2470703125,
      "learning_rate": 1.193716746896377e-05,
      "loss": 0.1114,
      "step": 10150
    },
    {
      "epoch": 1.8606348048157608,
      "grad_norm": 942542.6875,
      "learning_rate": 1.188649607296681e-05,
      "loss": 0.1546,
      "step": 10200
    },
    {
      "epoch": 1.8697555636628969,
      "grad_norm": 509781.625,
      "learning_rate": 1.1835824676969851e-05,
      "loss": 0.1712,
      "step": 10250
    },
    {
      "epoch": 1.878876322510033,
      "grad_norm": 24648.212890625,
      "learning_rate": 1.1785153280972892e-05,
      "loss": 0.1773,
      "step": 10300
    },
    {
      "epoch": 1.887997081357169,
      "grad_norm": 345658.4375,
      "learning_rate": 1.1734481884975933e-05,
      "loss": 0.1519,
      "step": 10350
    },
    {
      "epoch": 1.897117840204305,
      "grad_norm": 389879.65625,
      "learning_rate": 1.1683810488978973e-05,
      "loss": 0.1342,
      "step": 10400
    },
    {
      "epoch": 1.906238599051441,
      "grad_norm": 26194.060546875,
      "learning_rate": 1.1633139092982014e-05,
      "loss": 0.1263,
      "step": 10450
    },
    {
      "epoch": 1.915359357898577,
      "grad_norm": 475314.625,
      "learning_rate": 1.1582467696985053e-05,
      "loss": 0.152,
      "step": 10500
    },
    {
      "epoch": 1.9244801167457133,
      "grad_norm": 377996.40625,
      "learning_rate": 1.1531796300988094e-05,
      "loss": 0.1471,
      "step": 10550
    },
    {
      "epoch": 1.9336008755928493,
      "grad_norm": 193409.953125,
      "learning_rate": 1.1481124904991134e-05,
      "loss": 0.1287,
      "step": 10600
    },
    {
      "epoch": 1.9427216344399854,
      "grad_norm": 151753.75,
      "learning_rate": 1.1430453508994175e-05,
      "loss": 0.1657,
      "step": 10650
    },
    {
      "epoch": 1.9518423932871216,
      "grad_norm": 88547.9296875,
      "learning_rate": 1.1379782112997214e-05,
      "loss": 0.1398,
      "step": 10700
    },
    {
      "epoch": 1.9609631521342576,
      "grad_norm": 533953.75,
      "learning_rate": 1.1329110717000253e-05,
      "loss": 0.1369,
      "step": 10750
    },
    {
      "epoch": 1.9700839109813937,
      "grad_norm": 280480.03125,
      "learning_rate": 1.1278439321003293e-05,
      "loss": 0.1371,
      "step": 10800
    },
    {
      "epoch": 1.9792046698285297,
      "grad_norm": 1142595.625,
      "learning_rate": 1.1227767925006334e-05,
      "loss": 0.156,
      "step": 10850
    },
    {
      "epoch": 1.9883254286756658,
      "grad_norm": 63976.15234375,
      "learning_rate": 1.1177096529009375e-05,
      "loss": 0.1144,
      "step": 10900
    },
    {
      "epoch": 1.9974461875228018,
      "grad_norm": 315432.125,
      "learning_rate": 1.1126425133012415e-05,
      "loss": 0.177,
      "step": 10950
    },
    {
      "epoch": 2.006566946369938,
      "grad_norm": 518792.28125,
      "learning_rate": 1.1075753737015456e-05,
      "loss": 0.0956,
      "step": 11000
    },
    {
      "epoch": 2.015687705217074,
      "grad_norm": 35010.015625,
      "learning_rate": 1.1025082341018497e-05,
      "loss": 0.1294,
      "step": 11050
    },
    {
      "epoch": 2.0248084640642103,
      "grad_norm": 463834.90625,
      "learning_rate": 1.0974410945021537e-05,
      "loss": 0.1236,
      "step": 11100
    },
    {
      "epoch": 2.0339292229113464,
      "grad_norm": 612922.125,
      "learning_rate": 1.0923739549024576e-05,
      "loss": 0.1488,
      "step": 11150
    },
    {
      "epoch": 2.0430499817584824,
      "grad_norm": 6927.66552734375,
      "learning_rate": 1.0873068153027617e-05,
      "loss": 0.0844,
      "step": 11200
    },
    {
      "epoch": 2.0521707406056184,
      "grad_norm": 16208.74609375,
      "learning_rate": 1.0822396757030657e-05,
      "loss": 0.0862,
      "step": 11250
    },
    {
      "epoch": 2.0612914994527545,
      "grad_norm": 12963.083984375,
      "learning_rate": 1.0771725361033698e-05,
      "loss": 0.1205,
      "step": 11300
    },
    {
      "epoch": 2.0704122582998905,
      "grad_norm": 823055.8125,
      "learning_rate": 1.0721053965036739e-05,
      "loss": 0.1339,
      "step": 11350
    },
    {
      "epoch": 2.0795330171470265,
      "grad_norm": 13406.169921875,
      "learning_rate": 1.067038256903978e-05,
      "loss": 0.0968,
      "step": 11400
    },
    {
      "epoch": 2.0886537759941626,
      "grad_norm": 1094761.5,
      "learning_rate": 1.061971117304282e-05,
      "loss": 0.117,
      "step": 11450
    },
    {
      "epoch": 2.0977745348412986,
      "grad_norm": 346153.0,
      "learning_rate": 1.0569039777045857e-05,
      "loss": 0.126,
      "step": 11500
    },
    {
      "epoch": 2.106895293688435,
      "grad_norm": 29862.87109375,
      "learning_rate": 1.0518368381048898e-05,
      "loss": 0.0707,
      "step": 11550
    },
    {
      "epoch": 2.116016052535571,
      "grad_norm": 17524.248046875,
      "learning_rate": 1.0467696985051939e-05,
      "loss": 0.1058,
      "step": 11600
    },
    {
      "epoch": 2.125136811382707,
      "grad_norm": 27132.626953125,
      "learning_rate": 1.0417025589054979e-05,
      "loss": 0.1221,
      "step": 11650
    },
    {
      "epoch": 2.134257570229843,
      "grad_norm": 18611.271484375,
      "learning_rate": 1.036635419305802e-05,
      "loss": 0.1069,
      "step": 11700
    },
    {
      "epoch": 2.1433783290769792,
      "grad_norm": 6232.9599609375,
      "learning_rate": 1.031568279706106e-05,
      "loss": 0.1302,
      "step": 11750
    },
    {
      "epoch": 2.1524990879241153,
      "grad_norm": 25218.12109375,
      "learning_rate": 1.02650114010641e-05,
      "loss": 0.1079,
      "step": 11800
    },
    {
      "epoch": 2.1616198467712513,
      "grad_norm": 1335923.625,
      "learning_rate": 1.021434000506714e-05,
      "loss": 0.1322,
      "step": 11850
    },
    {
      "epoch": 2.1707406056183873,
      "grad_norm": 1676924.375,
      "learning_rate": 1.016366860907018e-05,
      "loss": 0.11,
      "step": 11900
    },
    {
      "epoch": 2.1798613644655234,
      "grad_norm": 756576.25,
      "learning_rate": 1.0112997213073221e-05,
      "loss": 0.1003,
      "step": 11950
    },
    {
      "epoch": 2.18898212331266,
      "grad_norm": 6653.81494140625,
      "learning_rate": 1.0062325817076262e-05,
      "loss": 0.1046,
      "step": 12000
    },
    {
      "epoch": 2.198102882159796,
      "grad_norm": 810981.125,
      "learning_rate": 1.0011654421079303e-05,
      "loss": 0.0843,
      "step": 12050
    },
    {
      "epoch": 2.207223641006932,
      "grad_norm": 178732.953125,
      "learning_rate": 9.960983025082341e-06,
      "loss": 0.0967,
      "step": 12100
    },
    {
      "epoch": 2.216344399854068,
      "grad_norm": 1141952.5,
      "learning_rate": 9.910311629085382e-06,
      "loss": 0.0823,
      "step": 12150
    },
    {
      "epoch": 2.225465158701204,
      "grad_norm": 5572.48046875,
      "learning_rate": 9.859640233088423e-06,
      "loss": 0.1006,
      "step": 12200
    },
    {
      "epoch": 2.23458591754834,
      "grad_norm": 1683108.75,
      "learning_rate": 9.808968837091463e-06,
      "loss": 0.1176,
      "step": 12250
    },
    {
      "epoch": 2.243706676395476,
      "grad_norm": 104974.5390625,
      "learning_rate": 9.758297441094502e-06,
      "loss": 0.1132,
      "step": 12300
    },
    {
      "epoch": 2.252827435242612,
      "grad_norm": 617514.1875,
      "learning_rate": 9.707626045097543e-06,
      "loss": 0.0899,
      "step": 12350
    },
    {
      "epoch": 2.261948194089748,
      "grad_norm": 73502.5234375,
      "learning_rate": 9.656954649100584e-06,
      "loss": 0.1556,
      "step": 12400
    },
    {
      "epoch": 2.271068952936884,
      "grad_norm": 191302.25,
      "learning_rate": 9.606283253103624e-06,
      "loss": 0.1235,
      "step": 12450
    },
    {
      "epoch": 2.28018971178402,
      "grad_norm": 18955.9296875,
      "learning_rate": 9.555611857106663e-06,
      "loss": 0.0971,
      "step": 12500
    },
    {
      "epoch": 2.2893104706311567,
      "grad_norm": 609338.9375,
      "learning_rate": 9.504940461109704e-06,
      "loss": 0.0764,
      "step": 12550
    },
    {
      "epoch": 2.2984312294782927,
      "grad_norm": 49400.29296875,
      "learning_rate": 9.454269065112744e-06,
      "loss": 0.116,
      "step": 12600
    },
    {
      "epoch": 2.3075519883254287,
      "grad_norm": 5707.58447265625,
      "learning_rate": 9.403597669115785e-06,
      "loss": 0.1071,
      "step": 12650
    },
    {
      "epoch": 2.3166727471725648,
      "grad_norm": 8319.708984375,
      "learning_rate": 9.352926273118826e-06,
      "loss": 0.1108,
      "step": 12700
    },
    {
      "epoch": 2.325793506019701,
      "grad_norm": 1522824.25,
      "learning_rate": 9.302254877121866e-06,
      "loss": 0.1228,
      "step": 12750
    },
    {
      "epoch": 2.334914264866837,
      "grad_norm": 8568.81640625,
      "learning_rate": 9.251583481124905e-06,
      "loss": 0.1304,
      "step": 12800
    },
    {
      "epoch": 2.344035023713973,
      "grad_norm": 409862.28125,
      "learning_rate": 9.200912085127946e-06,
      "loss": 0.0932,
      "step": 12850
    },
    {
      "epoch": 2.353155782561109,
      "grad_norm": 809333.6875,
      "learning_rate": 9.150240689130987e-06,
      "loss": 0.1201,
      "step": 12900
    },
    {
      "epoch": 2.362276541408245,
      "grad_norm": 5091.81201171875,
      "learning_rate": 9.099569293134026e-06,
      "loss": 0.1079,
      "step": 12950
    },
    {
      "epoch": 2.3713973002553814,
      "grad_norm": 12275.2744140625,
      "learning_rate": 9.048897897137066e-06,
      "loss": 0.0808,
      "step": 13000
    },
    {
      "epoch": 2.3805180591025175,
      "grad_norm": 778364.9375,
      "learning_rate": 8.998226501140107e-06,
      "loss": 0.1252,
      "step": 13050
    },
    {
      "epoch": 2.3896388179496535,
      "grad_norm": 1392218.875,
      "learning_rate": 8.947555105143147e-06,
      "loss": 0.1573,
      "step": 13100
    },
    {
      "epoch": 2.3987595767967895,
      "grad_norm": 91139.6796875,
      "learning_rate": 8.896883709146188e-06,
      "loss": 0.1527,
      "step": 13150
    },
    {
      "epoch": 2.4078803356439256,
      "grad_norm": 24613.296875,
      "learning_rate": 8.846212313149229e-06,
      "loss": 0.1082,
      "step": 13200
    },
    {
      "epoch": 2.4170010944910616,
      "grad_norm": 5775.83984375,
      "learning_rate": 8.79554091715227e-06,
      "loss": 0.1051,
      "step": 13250
    },
    {
      "epoch": 2.4261218533381976,
      "grad_norm": 137943.359375,
      "learning_rate": 8.744869521155308e-06,
      "loss": 0.0708,
      "step": 13300
    },
    {
      "epoch": 2.4352426121853337,
      "grad_norm": 10633.3671875,
      "learning_rate": 8.694198125158349e-06,
      "loss": 0.1113,
      "step": 13350
    },
    {
      "epoch": 2.4443633710324697,
      "grad_norm": 8726.71484375,
      "learning_rate": 8.64352672916139e-06,
      "loss": 0.0938,
      "step": 13400
    },
    {
      "epoch": 2.453484129879606,
      "grad_norm": 13200.255859375,
      "learning_rate": 8.592855333164429e-06,
      "loss": 0.1603,
      "step": 13450
    },
    {
      "epoch": 2.462604888726742,
      "grad_norm": 11513.1064453125,
      "learning_rate": 8.54218393716747e-06,
      "loss": 0.1038,
      "step": 13500
    },
    {
      "epoch": 2.4717256475738782,
      "grad_norm": 1077261.5,
      "learning_rate": 8.49151254117051e-06,
      "loss": 0.1487,
      "step": 13550
    },
    {
      "epoch": 2.4808464064210143,
      "grad_norm": 102814.7734375,
      "learning_rate": 8.44084114517355e-06,
      "loss": 0.0871,
      "step": 13600
    },
    {
      "epoch": 2.4899671652681503,
      "grad_norm": 8294.140625,
      "learning_rate": 8.390169749176591e-06,
      "loss": 0.0677,
      "step": 13650
    },
    {
      "epoch": 2.4990879241152864,
      "grad_norm": 171011.953125,
      "learning_rate": 8.33949835317963e-06,
      "loss": 0.1383,
      "step": 13700
    },
    {
      "epoch": 2.5082086829624224,
      "grad_norm": 948796.125,
      "learning_rate": 8.28882695718267e-06,
      "loss": 0.1327,
      "step": 13750
    },
    {
      "epoch": 2.5173294418095584,
      "grad_norm": 108077.9296875,
      "learning_rate": 8.238155561185711e-06,
      "loss": 0.1194,
      "step": 13800
    },
    {
      "epoch": 2.5264502006566945,
      "grad_norm": 891588.625,
      "learning_rate": 8.187484165188752e-06,
      "loss": 0.1191,
      "step": 13850
    },
    {
      "epoch": 2.535570959503831,
      "grad_norm": 231775.171875,
      "learning_rate": 8.136812769191793e-06,
      "loss": 0.1023,
      "step": 13900
    },
    {
      "epoch": 2.5446917183509665,
      "grad_norm": 326468.28125,
      "learning_rate": 8.086141373194832e-06,
      "loss": 0.1128,
      "step": 13950
    },
    {
      "epoch": 2.553812477198103,
      "grad_norm": 7085.73828125,
      "learning_rate": 8.035469977197872e-06,
      "loss": 0.0896,
      "step": 14000
    },
    {
      "epoch": 2.562933236045239,
      "grad_norm": 11163.0478515625,
      "learning_rate": 7.984798581200913e-06,
      "loss": 0.088,
      "step": 14050
    },
    {
      "epoch": 2.572053994892375,
      "grad_norm": 870912.75,
      "learning_rate": 7.934127185203952e-06,
      "loss": 0.1218,
      "step": 14100
    },
    {
      "epoch": 2.581174753739511,
      "grad_norm": 2076912.875,
      "learning_rate": 7.883455789206992e-06,
      "loss": 0.0741,
      "step": 14150
    },
    {
      "epoch": 2.590295512586647,
      "grad_norm": 560899.625,
      "learning_rate": 7.832784393210033e-06,
      "loss": 0.1038,
      "step": 14200
    },
    {
      "epoch": 2.599416271433783,
      "grad_norm": 145081.390625,
      "learning_rate": 7.782112997213074e-06,
      "loss": 0.1033,
      "step": 14250
    },
    {
      "epoch": 2.608537030280919,
      "grad_norm": 13475.7421875,
      "learning_rate": 7.731441601216114e-06,
      "loss": 0.076,
      "step": 14300
    },
    {
      "epoch": 2.6176577891280557,
      "grad_norm": 8091.7548828125,
      "learning_rate": 7.680770205219155e-06,
      "loss": 0.1139,
      "step": 14350
    },
    {
      "epoch": 2.6267785479751913,
      "grad_norm": 7125.2431640625,
      "learning_rate": 7.630098809222196e-06,
      "loss": 0.1011,
      "step": 14400
    },
    {
      "epoch": 2.6358993068223278,
      "grad_norm": 418235.0,
      "learning_rate": 7.579427413225235e-06,
      "loss": 0.0998,
      "step": 14450
    },
    {
      "epoch": 2.645020065669464,
      "grad_norm": 6137.4609375,
      "learning_rate": 7.528756017228274e-06,
      "loss": 0.1203,
      "step": 14500
    },
    {
      "epoch": 2.6541408245166,
      "grad_norm": 895619.5625,
      "learning_rate": 7.478084621231315e-06,
      "loss": 0.0989,
      "step": 14550
    },
    {
      "epoch": 2.663261583363736,
      "grad_norm": 1271397.875,
      "learning_rate": 7.427413225234356e-06,
      "loss": 0.1066,
      "step": 14600
    },
    {
      "epoch": 2.672382342210872,
      "grad_norm": 97996.046875,
      "learning_rate": 7.376741829237396e-06,
      "loss": 0.0888,
      "step": 14650
    },
    {
      "epoch": 2.681503101058008,
      "grad_norm": 47474.50390625,
      "learning_rate": 7.326070433240436e-06,
      "loss": 0.1255,
      "step": 14700
    },
    {
      "epoch": 2.690623859905144,
      "grad_norm": 1970422.5,
      "learning_rate": 7.275399037243477e-06,
      "loss": 0.1303,
      "step": 14750
    },
    {
      "epoch": 2.6997446187522804,
      "grad_norm": 12816.1962890625,
      "learning_rate": 7.224727641246517e-06,
      "loss": 0.0986,
      "step": 14800
    },
    {
      "epoch": 2.708865377599416,
      "grad_norm": 11599.1953125,
      "learning_rate": 7.174056245249558e-06,
      "loss": 0.0904,
      "step": 14850
    },
    {
      "epoch": 2.7179861364465525,
      "grad_norm": 1421936.0,
      "learning_rate": 7.123384849252597e-06,
      "loss": 0.1277,
      "step": 14900
    },
    {
      "epoch": 2.7271068952936885,
      "grad_norm": 357933.8125,
      "learning_rate": 7.0727134532556376e-06,
      "loss": 0.139,
      "step": 14950
    },
    {
      "epoch": 2.7362276541408246,
      "grad_norm": 2287124.25,
      "learning_rate": 7.022042057258678e-06,
      "loss": 0.1035,
      "step": 15000
    },
    {
      "epoch": 2.7453484129879606,
      "grad_norm": 289179.34375,
      "learning_rate": 6.971370661261718e-06,
      "loss": 0.1376,
      "step": 15050
    },
    {
      "epoch": 2.7544691718350967,
      "grad_norm": 29445.27734375,
      "learning_rate": 6.920699265264759e-06,
      "loss": 0.07,
      "step": 15100
    },
    {
      "epoch": 2.7635899306822327,
      "grad_norm": 551219.25,
      "learning_rate": 6.870027869267799e-06,
      "loss": 0.1334,
      "step": 15150
    },
    {
      "epoch": 2.7727106895293687,
      "grad_norm": 662608.8125,
      "learning_rate": 6.81935647327084e-06,
      "loss": 0.0984,
      "step": 15200
    },
    {
      "epoch": 2.781831448376505,
      "grad_norm": 529108.75,
      "learning_rate": 6.76868507727388e-06,
      "loss": 0.1354,
      "step": 15250
    },
    {
      "epoch": 2.790952207223641,
      "grad_norm": 104224.609375,
      "learning_rate": 6.7180136812769195e-06,
      "loss": 0.1214,
      "step": 15300
    },
    {
      "epoch": 2.8000729660707773,
      "grad_norm": 195596.28125,
      "learning_rate": 6.667342285279959e-06,
      "loss": 0.1553,
      "step": 15350
    },
    {
      "epoch": 2.8091937249179133,
      "grad_norm": 22802.654296875,
      "learning_rate": 6.616670889283e-06,
      "loss": 0.0658,
      "step": 15400
    },
    {
      "epoch": 2.8183144837650493,
      "grad_norm": 4139.34912109375,
      "learning_rate": 6.5659994932860406e-06,
      "loss": 0.0782,
      "step": 15450
    },
    {
      "epoch": 2.8274352426121854,
      "grad_norm": 45684.21875,
      "learning_rate": 6.515328097289081e-06,
      "loss": 0.1125,
      "step": 15500
    },
    {
      "epoch": 2.8365560014593214,
      "grad_norm": 1223668.75,
      "learning_rate": 6.464656701292121e-06,
      "loss": 0.0861,
      "step": 15550
    },
    {
      "epoch": 2.8456767603064574,
      "grad_norm": 383293.34375,
      "learning_rate": 6.413985305295162e-06,
      "loss": 0.1208,
      "step": 15600
    },
    {
      "epoch": 2.8547975191535935,
      "grad_norm": 567774.3125,
      "learning_rate": 6.363313909298202e-06,
      "loss": 0.112,
      "step": 15650
    },
    {
      "epoch": 2.8639182780007295,
      "grad_norm": 1285287.25,
      "learning_rate": 6.312642513301241e-06,
      "loss": 0.09,
      "step": 15700
    },
    {
      "epoch": 2.8730390368478655,
      "grad_norm": 6639.6904296875,
      "learning_rate": 6.261971117304282e-06,
      "loss": 0.13,
      "step": 15750
    },
    {
      "epoch": 2.882159795695002,
      "grad_norm": 215088.921875,
      "learning_rate": 6.2112997213073225e-06,
      "loss": 0.1047,
      "step": 15800
    },
    {
      "epoch": 2.891280554542138,
      "grad_norm": 428886.59375,
      "learning_rate": 6.160628325310363e-06,
      "loss": 0.1207,
      "step": 15850
    },
    {
      "epoch": 2.900401313389274,
      "grad_norm": 13772.1787109375,
      "learning_rate": 6.109956929313403e-06,
      "loss": 0.1194,
      "step": 15900
    },
    {
      "epoch": 2.90952207223641,
      "grad_norm": 61497.54296875,
      "learning_rate": 6.0592855333164436e-06,
      "loss": 0.1387,
      "step": 15950
    },
    {
      "epoch": 2.918642831083546,
      "grad_norm": 939115.8125,
      "learning_rate": 6.008614137319484e-06,
      "loss": 0.0962,
      "step": 16000
    },
    {
      "epoch": 2.927763589930682,
      "grad_norm": 299580.875,
      "learning_rate": 5.957942741322524e-06,
      "loss": 0.1015,
      "step": 16050
    },
    {
      "epoch": 2.9368843487778182,
      "grad_norm": 4868.5166015625,
      "learning_rate": 5.907271345325564e-06,
      "loss": 0.0702,
      "step": 16100
    },
    {
      "epoch": 2.9460051076249543,
      "grad_norm": 6620.400390625,
      "learning_rate": 5.856599949328604e-06,
      "loss": 0.0972,
      "step": 16150
    },
    {
      "epoch": 2.9551258664720903,
      "grad_norm": 29583.2890625,
      "learning_rate": 5.805928553331644e-06,
      "loss": 0.1455,
      "step": 16200
    },
    {
      "epoch": 2.964246625319227,
      "grad_norm": 137434.390625,
      "learning_rate": 5.755257157334685e-06,
      "loss": 0.1209,
      "step": 16250
    },
    {
      "epoch": 2.9733673841663624,
      "grad_norm": 413871.0625,
      "learning_rate": 5.7045857613377255e-06,
      "loss": 0.1305,
      "step": 16300
    },
    {
      "epoch": 2.982488143013499,
      "grad_norm": 355659.0,
      "learning_rate": 5.653914365340766e-06,
      "loss": 0.1505,
      "step": 16350
    },
    {
      "epoch": 2.991608901860635,
      "grad_norm": 763238.1875,
      "learning_rate": 5.603242969343806e-06,
      "loss": 0.1158,
      "step": 16400
    }
  ],
  "logging_steps": 50,
  "max_steps": 21928,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.02260289516078e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
