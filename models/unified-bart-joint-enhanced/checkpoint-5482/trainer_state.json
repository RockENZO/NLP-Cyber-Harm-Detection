{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 5482,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009120758847136081,
      "grad_norm": 5936224.0,
      "learning_rate": 4.4687642498860014e-07,
      "loss": 37.961,
      "step": 50
    },
    {
      "epoch": 0.018241517694272163,
      "grad_norm": 3609146.0,
      "learning_rate": 9.028727770177839e-07,
      "loss": 34.6913,
      "step": 100
    },
    {
      "epoch": 0.027362276541408246,
      "grad_norm": 3565987.75,
      "learning_rate": 1.3588691290469677e-06,
      "loss": 31.0193,
      "step": 150
    },
    {
      "epoch": 0.036483035388544326,
      "grad_norm": 6580893.0,
      "learning_rate": 1.8148654810761515e-06,
      "loss": 27.2476,
      "step": 200
    },
    {
      "epoch": 0.04560379423568041,
      "grad_norm": 7123704.5,
      "learning_rate": 2.2708618331053354e-06,
      "loss": 20.2083,
      "step": 250
    },
    {
      "epoch": 0.05472455308281649,
      "grad_norm": 7122810.0,
      "learning_rate": 2.7268581851345193e-06,
      "loss": 16.0538,
      "step": 300
    },
    {
      "epoch": 0.06384531192995258,
      "grad_norm": 7168265.0,
      "learning_rate": 3.1828545371637033e-06,
      "loss": 13.6815,
      "step": 350
    },
    {
      "epoch": 0.07296607077708865,
      "grad_norm": 7192185.5,
      "learning_rate": 3.638850889192887e-06,
      "loss": 11.8997,
      "step": 400
    },
    {
      "epoch": 0.08208682962422474,
      "grad_norm": 7115527.0,
      "learning_rate": 4.094847241222071e-06,
      "loss": 10.2057,
      "step": 450
    },
    {
      "epoch": 0.09120758847136082,
      "grad_norm": 6981652.0,
      "learning_rate": 4.550843593251254e-06,
      "loss": 8.6633,
      "step": 500
    },
    {
      "epoch": 0.1003283473184969,
      "grad_norm": 6811356.5,
      "learning_rate": 5.006839945280439e-06,
      "loss": 7.278,
      "step": 550
    },
    {
      "epoch": 0.10944910616563298,
      "grad_norm": 5803515.5,
      "learning_rate": 5.462836297309621e-06,
      "loss": 5.8033,
      "step": 600
    },
    {
      "epoch": 0.11856986501276906,
      "grad_norm": 4586944.0,
      "learning_rate": 5.918832649338806e-06,
      "loss": 4.4876,
      "step": 650
    },
    {
      "epoch": 0.12769062385990515,
      "grad_norm": 2988665.0,
      "learning_rate": 6.374829001367989e-06,
      "loss": 3.4683,
      "step": 700
    },
    {
      "epoch": 0.1368113827070412,
      "grad_norm": 1596873.125,
      "learning_rate": 6.830825353397174e-06,
      "loss": 2.6278,
      "step": 750
    },
    {
      "epoch": 0.1459321415541773,
      "grad_norm": 823695.4375,
      "learning_rate": 7.286821705426357e-06,
      "loss": 2.1076,
      "step": 800
    },
    {
      "epoch": 0.1550529004013134,
      "grad_norm": 687583.375,
      "learning_rate": 7.742818057455541e-06,
      "loss": 1.3217,
      "step": 850
    },
    {
      "epoch": 0.16417365924844948,
      "grad_norm": 387442.53125,
      "learning_rate": 8.198814409484724e-06,
      "loss": 0.8887,
      "step": 900
    },
    {
      "epoch": 0.17329441809558555,
      "grad_norm": 807059.5,
      "learning_rate": 8.654810761513909e-06,
      "loss": 0.7782,
      "step": 950
    },
    {
      "epoch": 0.18241517694272164,
      "grad_norm": 841452.3125,
      "learning_rate": 9.110807113543091e-06,
      "loss": 0.5813,
      "step": 1000
    },
    {
      "epoch": 0.19153593578985773,
      "grad_norm": 442154.78125,
      "learning_rate": 9.566803465572276e-06,
      "loss": 0.5491,
      "step": 1050
    },
    {
      "epoch": 0.2006566946369938,
      "grad_norm": 597384.6875,
      "learning_rate": 1.002279981760146e-05,
      "loss": 0.5095,
      "step": 1100
    },
    {
      "epoch": 0.20977745348412988,
      "grad_norm": 700358.75,
      "learning_rate": 1.0478796169630643e-05,
      "loss": 0.5135,
      "step": 1150
    },
    {
      "epoch": 0.21889821233126597,
      "grad_norm": 762592.875,
      "learning_rate": 1.0934792521659827e-05,
      "loss": 0.4396,
      "step": 1200
    },
    {
      "epoch": 0.22801897117840203,
      "grad_norm": 711147.125,
      "learning_rate": 1.1390788873689011e-05,
      "loss": 0.4072,
      "step": 1250
    },
    {
      "epoch": 0.23713973002553812,
      "grad_norm": 471668.34375,
      "learning_rate": 1.1846785225718196e-05,
      "loss": 0.4217,
      "step": 1300
    },
    {
      "epoch": 0.2462604888726742,
      "grad_norm": 353904.5625,
      "learning_rate": 1.2302781577747378e-05,
      "loss": 0.4368,
      "step": 1350
    },
    {
      "epoch": 0.2553812477198103,
      "grad_norm": 612999.625,
      "learning_rate": 1.2758777929776563e-05,
      "loss": 0.3692,
      "step": 1400
    },
    {
      "epoch": 0.2645020065669464,
      "grad_norm": 400021.6875,
      "learning_rate": 1.3214774281805747e-05,
      "loss": 0.381,
      "step": 1450
    },
    {
      "epoch": 0.2736227654140824,
      "grad_norm": 655072.375,
      "learning_rate": 1.3670770633834932e-05,
      "loss": 0.3679,
      "step": 1500
    },
    {
      "epoch": 0.2827435242612185,
      "grad_norm": 198947.984375,
      "learning_rate": 1.4126766985864113e-05,
      "loss": 0.4183,
      "step": 1550
    },
    {
      "epoch": 0.2918642831083546,
      "grad_norm": 545327.5,
      "learning_rate": 1.4582763337893297e-05,
      "loss": 0.3756,
      "step": 1600
    },
    {
      "epoch": 0.3009850419554907,
      "grad_norm": 476617.8125,
      "learning_rate": 1.5038759689922481e-05,
      "loss": 0.3379,
      "step": 1650
    },
    {
      "epoch": 0.3101058008026268,
      "grad_norm": 638888.375,
      "learning_rate": 1.5494756041951667e-05,
      "loss": 0.3547,
      "step": 1700
    },
    {
      "epoch": 0.3192265596497629,
      "grad_norm": 437538.875,
      "learning_rate": 1.595075239398085e-05,
      "loss": 0.3613,
      "step": 1750
    },
    {
      "epoch": 0.32834731849689897,
      "grad_norm": 565889.4375,
      "learning_rate": 1.6406748746010033e-05,
      "loss": 0.3084,
      "step": 1800
    },
    {
      "epoch": 0.337468077344035,
      "grad_norm": 689763.625,
      "learning_rate": 1.686274509803922e-05,
      "loss": 0.3319,
      "step": 1850
    },
    {
      "epoch": 0.3465888361911711,
      "grad_norm": 659720.75,
      "learning_rate": 1.73187414500684e-05,
      "loss": 0.2794,
      "step": 1900
    },
    {
      "epoch": 0.3557095950383072,
      "grad_norm": 410950.34375,
      "learning_rate": 1.7774737802097584e-05,
      "loss": 0.31,
      "step": 1950
    },
    {
      "epoch": 0.36483035388544327,
      "grad_norm": 377901.875,
      "learning_rate": 1.8230734154126767e-05,
      "loss": 0.2641,
      "step": 2000
    },
    {
      "epoch": 0.37395111273257936,
      "grad_norm": 69726.6484375,
      "learning_rate": 1.8686730506155953e-05,
      "loss": 0.3077,
      "step": 2050
    },
    {
      "epoch": 0.38307187157971545,
      "grad_norm": 512561.8125,
      "learning_rate": 1.9142726858185135e-05,
      "loss": 0.2944,
      "step": 2100
    },
    {
      "epoch": 0.39219263042685154,
      "grad_norm": 537200.125,
      "learning_rate": 1.9598723210214318e-05,
      "loss": 0.2898,
      "step": 2150
    },
    {
      "epoch": 0.4013133892739876,
      "grad_norm": 501728.40625,
      "learning_rate": 1.9993919432480366e-05,
      "loss": 0.2421,
      "step": 2200
    },
    {
      "epoch": 0.41043414812112367,
      "grad_norm": 499459.9375,
      "learning_rate": 1.9943248036483407e-05,
      "loss": 0.2521,
      "step": 2250
    },
    {
      "epoch": 0.41955490696825976,
      "grad_norm": 439026.53125,
      "learning_rate": 1.9892576640486448e-05,
      "loss": 0.2482,
      "step": 2300
    },
    {
      "epoch": 0.42867566581539585,
      "grad_norm": 508896.6875,
      "learning_rate": 1.9841905244489488e-05,
      "loss": 0.2579,
      "step": 2350
    },
    {
      "epoch": 0.43779642466253194,
      "grad_norm": 1120810.125,
      "learning_rate": 1.979123384849253e-05,
      "loss": 0.2382,
      "step": 2400
    },
    {
      "epoch": 0.446917183509668,
      "grad_norm": 468299.5,
      "learning_rate": 1.974056245249557e-05,
      "loss": 0.2531,
      "step": 2450
    },
    {
      "epoch": 0.45603794235680406,
      "grad_norm": 878376.875,
      "learning_rate": 1.968989105649861e-05,
      "loss": 0.2357,
      "step": 2500
    },
    {
      "epoch": 0.46515870120394015,
      "grad_norm": 135160.609375,
      "learning_rate": 1.963921966050165e-05,
      "loss": 0.2343,
      "step": 2550
    },
    {
      "epoch": 0.47427946005107624,
      "grad_norm": 270697.0,
      "learning_rate": 1.9588548264504688e-05,
      "loss": 0.2641,
      "step": 2600
    },
    {
      "epoch": 0.48340021889821233,
      "grad_norm": 697387.4375,
      "learning_rate": 1.953787686850773e-05,
      "loss": 0.2206,
      "step": 2650
    },
    {
      "epoch": 0.4925209777453484,
      "grad_norm": 374661.125,
      "learning_rate": 1.948720547251077e-05,
      "loss": 0.2651,
      "step": 2700
    },
    {
      "epoch": 0.5016417365924845,
      "grad_norm": 370686.71875,
      "learning_rate": 1.943653407651381e-05,
      "loss": 0.2287,
      "step": 2750
    },
    {
      "epoch": 0.5107624954396206,
      "grad_norm": 906469.8125,
      "learning_rate": 1.938586268051685e-05,
      "loss": 0.2224,
      "step": 2800
    },
    {
      "epoch": 0.5198832542867566,
      "grad_norm": 351443.28125,
      "learning_rate": 1.933519128451989e-05,
      "loss": 0.2629,
      "step": 2850
    },
    {
      "epoch": 0.5290040131338928,
      "grad_norm": 925489.625,
      "learning_rate": 1.9284519888522932e-05,
      "loss": 0.2387,
      "step": 2900
    },
    {
      "epoch": 0.5381247719810288,
      "grad_norm": 662768.375,
      "learning_rate": 1.923384849252597e-05,
      "loss": 0.2902,
      "step": 2950
    },
    {
      "epoch": 0.5472455308281649,
      "grad_norm": 27612.37109375,
      "learning_rate": 1.918317709652901e-05,
      "loss": 0.2399,
      "step": 3000
    },
    {
      "epoch": 0.556366289675301,
      "grad_norm": 465642.96875,
      "learning_rate": 1.913250570053205e-05,
      "loss": 0.2917,
      "step": 3050
    },
    {
      "epoch": 0.565487048522437,
      "grad_norm": 110167.90625,
      "learning_rate": 1.908183430453509e-05,
      "loss": 0.2652,
      "step": 3100
    },
    {
      "epoch": 0.5746078073695732,
      "grad_norm": 926643.0625,
      "learning_rate": 1.903116290853813e-05,
      "loss": 0.2123,
      "step": 3150
    },
    {
      "epoch": 0.5837285662167092,
      "grad_norm": 237950.4375,
      "learning_rate": 1.8980491512541172e-05,
      "loss": 0.2494,
      "step": 3200
    },
    {
      "epoch": 0.5928493250638454,
      "grad_norm": 573800.4375,
      "learning_rate": 1.8929820116544213e-05,
      "loss": 0.1816,
      "step": 3250
    },
    {
      "epoch": 0.6019700839109814,
      "grad_norm": 499455.40625,
      "learning_rate": 1.8879148720547254e-05,
      "loss": 0.2127,
      "step": 3300
    },
    {
      "epoch": 0.6110908427581174,
      "grad_norm": 645332.0625,
      "learning_rate": 1.8828477324550294e-05,
      "loss": 0.2139,
      "step": 3350
    },
    {
      "epoch": 0.6202116016052536,
      "grad_norm": 570036.8125,
      "learning_rate": 1.8777805928553335e-05,
      "loss": 0.2495,
      "step": 3400
    },
    {
      "epoch": 0.6293323604523896,
      "grad_norm": 571552.0625,
      "learning_rate": 1.8727134532556372e-05,
      "loss": 0.2361,
      "step": 3450
    },
    {
      "epoch": 0.6384531192995258,
      "grad_norm": 685822.875,
      "learning_rate": 1.8676463136559413e-05,
      "loss": 0.2217,
      "step": 3500
    },
    {
      "epoch": 0.6475738781466618,
      "grad_norm": 450229.59375,
      "learning_rate": 1.8625791740562453e-05,
      "loss": 0.2005,
      "step": 3550
    },
    {
      "epoch": 0.6566946369937979,
      "grad_norm": 291205.15625,
      "learning_rate": 1.8575120344565494e-05,
      "loss": 0.1855,
      "step": 3600
    },
    {
      "epoch": 0.665815395840934,
      "grad_norm": 568229.8125,
      "learning_rate": 1.8524448948568535e-05,
      "loss": 0.1757,
      "step": 3650
    },
    {
      "epoch": 0.67493615468807,
      "grad_norm": 418715.6875,
      "learning_rate": 1.8473777552571575e-05,
      "loss": 0.2017,
      "step": 3700
    },
    {
      "epoch": 0.6840569135352061,
      "grad_norm": 127682.7265625,
      "learning_rate": 1.8423106156574616e-05,
      "loss": 0.186,
      "step": 3750
    },
    {
      "epoch": 0.6931776723823422,
      "grad_norm": 795673.5,
      "learning_rate": 1.8372434760577657e-05,
      "loss": 0.2412,
      "step": 3800
    },
    {
      "epoch": 0.7022984312294783,
      "grad_norm": 65337.98828125,
      "learning_rate": 1.8321763364580697e-05,
      "loss": 0.2104,
      "step": 3850
    },
    {
      "epoch": 0.7114191900766144,
      "grad_norm": 618753.0625,
      "learning_rate": 1.8271091968583738e-05,
      "loss": 0.2262,
      "step": 3900
    },
    {
      "epoch": 0.7205399489237505,
      "grad_norm": 297139.34375,
      "learning_rate": 1.8220420572586775e-05,
      "loss": 0.2023,
      "step": 3950
    },
    {
      "epoch": 0.7296607077708865,
      "grad_norm": 860036.0625,
      "learning_rate": 1.8169749176589816e-05,
      "loss": 0.2153,
      "step": 4000
    },
    {
      "epoch": 0.7387814666180226,
      "grad_norm": 385338.96875,
      "learning_rate": 1.8119077780592856e-05,
      "loss": 0.202,
      "step": 4050
    },
    {
      "epoch": 0.7479022254651587,
      "grad_norm": 175294.28125,
      "learning_rate": 1.8068406384595897e-05,
      "loss": 0.1806,
      "step": 4100
    },
    {
      "epoch": 0.7570229843122948,
      "grad_norm": 92467.28125,
      "learning_rate": 1.8017734988598938e-05,
      "loss": 0.1844,
      "step": 4150
    },
    {
      "epoch": 0.7661437431594309,
      "grad_norm": 462566.28125,
      "learning_rate": 1.7967063592601978e-05,
      "loss": 0.1634,
      "step": 4200
    },
    {
      "epoch": 0.7752645020065669,
      "grad_norm": 131794.96875,
      "learning_rate": 1.7916392196605015e-05,
      "loss": 0.2244,
      "step": 4250
    },
    {
      "epoch": 0.7843852608537031,
      "grad_norm": 96948.9453125,
      "learning_rate": 1.7865720800608056e-05,
      "loss": 0.2306,
      "step": 4300
    },
    {
      "epoch": 0.7935060197008391,
      "grad_norm": 149633.703125,
      "learning_rate": 1.7815049404611097e-05,
      "loss": 0.1741,
      "step": 4350
    },
    {
      "epoch": 0.8026267785479752,
      "grad_norm": 111322.9765625,
      "learning_rate": 1.7764378008614137e-05,
      "loss": 0.222,
      "step": 4400
    },
    {
      "epoch": 0.8117475373951113,
      "grad_norm": 729147.625,
      "learning_rate": 1.7713706612617178e-05,
      "loss": 0.2101,
      "step": 4450
    },
    {
      "epoch": 0.8208682962422473,
      "grad_norm": 703945.0,
      "learning_rate": 1.766303521662022e-05,
      "loss": 0.1854,
      "step": 4500
    },
    {
      "epoch": 0.8299890550893835,
      "grad_norm": 168290.296875,
      "learning_rate": 1.761236382062326e-05,
      "loss": 0.1955,
      "step": 4550
    },
    {
      "epoch": 0.8391098139365195,
      "grad_norm": 283606.25,
      "learning_rate": 1.75616924246263e-05,
      "loss": 0.2009,
      "step": 4600
    },
    {
      "epoch": 0.8482305727836555,
      "grad_norm": 268640.03125,
      "learning_rate": 1.751102102862934e-05,
      "loss": 0.1936,
      "step": 4650
    },
    {
      "epoch": 0.8573513316307917,
      "grad_norm": 524306.125,
      "learning_rate": 1.746034963263238e-05,
      "loss": 0.1827,
      "step": 4700
    },
    {
      "epoch": 0.8664720904779277,
      "grad_norm": 254491.09375,
      "learning_rate": 1.7409678236635422e-05,
      "loss": 0.2057,
      "step": 4750
    },
    {
      "epoch": 0.8755928493250639,
      "grad_norm": 924322.5625,
      "learning_rate": 1.7359006840638462e-05,
      "loss": 0.1701,
      "step": 4800
    },
    {
      "epoch": 0.8847136081721999,
      "grad_norm": 167627.0625,
      "learning_rate": 1.7308335444641503e-05,
      "loss": 0.2277,
      "step": 4850
    },
    {
      "epoch": 0.893834367019336,
      "grad_norm": 96051.21875,
      "learning_rate": 1.7257664048644544e-05,
      "loss": 0.1808,
      "step": 4900
    },
    {
      "epoch": 0.9029551258664721,
      "grad_norm": 991511.0,
      "learning_rate": 1.7206992652647584e-05,
      "loss": 0.2206,
      "step": 4950
    },
    {
      "epoch": 0.9120758847136081,
      "grad_norm": 285384.96875,
      "learning_rate": 1.715632125665062e-05,
      "loss": 0.2018,
      "step": 5000
    },
    {
      "epoch": 0.9211966435607443,
      "grad_norm": 22526.392578125,
      "learning_rate": 1.7105649860653662e-05,
      "loss": 0.1588,
      "step": 5050
    },
    {
      "epoch": 0.9303174024078803,
      "grad_norm": 1007282.75,
      "learning_rate": 1.7054978464656703e-05,
      "loss": 0.2094,
      "step": 5100
    },
    {
      "epoch": 0.9394381612550164,
      "grad_norm": 586393.3125,
      "learning_rate": 1.7004307068659744e-05,
      "loss": 0.2068,
      "step": 5150
    },
    {
      "epoch": 0.9485589201021525,
      "grad_norm": 523450.8125,
      "learning_rate": 1.6953635672662784e-05,
      "loss": 0.2073,
      "step": 5200
    },
    {
      "epoch": 0.9576796789492886,
      "grad_norm": 929890.1875,
      "learning_rate": 1.690296427666582e-05,
      "loss": 0.1751,
      "step": 5250
    },
    {
      "epoch": 0.9668004377964247,
      "grad_norm": 1103105.25,
      "learning_rate": 1.6852292880668862e-05,
      "loss": 0.1675,
      "step": 5300
    },
    {
      "epoch": 0.9759211966435607,
      "grad_norm": 198764.5625,
      "learning_rate": 1.6801621484671903e-05,
      "loss": 0.1951,
      "step": 5350
    },
    {
      "epoch": 0.9850419554906968,
      "grad_norm": 31200.3125,
      "learning_rate": 1.6750950088674943e-05,
      "loss": 0.216,
      "step": 5400
    },
    {
      "epoch": 0.9941627143378329,
      "grad_norm": 561323.5625,
      "learning_rate": 1.6700278692677984e-05,
      "loss": 0.1999,
      "step": 5450
    }
  ],
  "logging_steps": 50,
  "max_steps": 21928,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.6742009650535936e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
