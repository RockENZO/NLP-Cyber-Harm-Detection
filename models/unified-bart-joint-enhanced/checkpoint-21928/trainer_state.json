{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 21928,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009120758847136081,
      "grad_norm": 5936224.0,
      "learning_rate": 4.4687642498860014e-07,
      "loss": 37.961,
      "step": 50
    },
    {
      "epoch": 0.018241517694272163,
      "grad_norm": 3609146.0,
      "learning_rate": 9.028727770177839e-07,
      "loss": 34.6913,
      "step": 100
    },
    {
      "epoch": 0.027362276541408246,
      "grad_norm": 3565987.75,
      "learning_rate": 1.3588691290469677e-06,
      "loss": 31.0193,
      "step": 150
    },
    {
      "epoch": 0.036483035388544326,
      "grad_norm": 6580893.0,
      "learning_rate": 1.8148654810761515e-06,
      "loss": 27.2476,
      "step": 200
    },
    {
      "epoch": 0.04560379423568041,
      "grad_norm": 7123704.5,
      "learning_rate": 2.2708618331053354e-06,
      "loss": 20.2083,
      "step": 250
    },
    {
      "epoch": 0.05472455308281649,
      "grad_norm": 7122810.0,
      "learning_rate": 2.7268581851345193e-06,
      "loss": 16.0538,
      "step": 300
    },
    {
      "epoch": 0.06384531192995258,
      "grad_norm": 7168265.0,
      "learning_rate": 3.1828545371637033e-06,
      "loss": 13.6815,
      "step": 350
    },
    {
      "epoch": 0.07296607077708865,
      "grad_norm": 7192185.5,
      "learning_rate": 3.638850889192887e-06,
      "loss": 11.8997,
      "step": 400
    },
    {
      "epoch": 0.08208682962422474,
      "grad_norm": 7115527.0,
      "learning_rate": 4.094847241222071e-06,
      "loss": 10.2057,
      "step": 450
    },
    {
      "epoch": 0.09120758847136082,
      "grad_norm": 6981652.0,
      "learning_rate": 4.550843593251254e-06,
      "loss": 8.6633,
      "step": 500
    },
    {
      "epoch": 0.1003283473184969,
      "grad_norm": 6811356.5,
      "learning_rate": 5.006839945280439e-06,
      "loss": 7.278,
      "step": 550
    },
    {
      "epoch": 0.10944910616563298,
      "grad_norm": 5803515.5,
      "learning_rate": 5.462836297309621e-06,
      "loss": 5.8033,
      "step": 600
    },
    {
      "epoch": 0.11856986501276906,
      "grad_norm": 4586944.0,
      "learning_rate": 5.918832649338806e-06,
      "loss": 4.4876,
      "step": 650
    },
    {
      "epoch": 0.12769062385990515,
      "grad_norm": 2988665.0,
      "learning_rate": 6.374829001367989e-06,
      "loss": 3.4683,
      "step": 700
    },
    {
      "epoch": 0.1368113827070412,
      "grad_norm": 1596873.125,
      "learning_rate": 6.830825353397174e-06,
      "loss": 2.6278,
      "step": 750
    },
    {
      "epoch": 0.1459321415541773,
      "grad_norm": 823695.4375,
      "learning_rate": 7.286821705426357e-06,
      "loss": 2.1076,
      "step": 800
    },
    {
      "epoch": 0.1550529004013134,
      "grad_norm": 687583.375,
      "learning_rate": 7.742818057455541e-06,
      "loss": 1.3217,
      "step": 850
    },
    {
      "epoch": 0.16417365924844948,
      "grad_norm": 387442.53125,
      "learning_rate": 8.198814409484724e-06,
      "loss": 0.8887,
      "step": 900
    },
    {
      "epoch": 0.17329441809558555,
      "grad_norm": 807059.5,
      "learning_rate": 8.654810761513909e-06,
      "loss": 0.7782,
      "step": 950
    },
    {
      "epoch": 0.18241517694272164,
      "grad_norm": 841452.3125,
      "learning_rate": 9.110807113543091e-06,
      "loss": 0.5813,
      "step": 1000
    },
    {
      "epoch": 0.19153593578985773,
      "grad_norm": 442154.78125,
      "learning_rate": 9.566803465572276e-06,
      "loss": 0.5491,
      "step": 1050
    },
    {
      "epoch": 0.2006566946369938,
      "grad_norm": 597384.6875,
      "learning_rate": 1.002279981760146e-05,
      "loss": 0.5095,
      "step": 1100
    },
    {
      "epoch": 0.20977745348412988,
      "grad_norm": 700358.75,
      "learning_rate": 1.0478796169630643e-05,
      "loss": 0.5135,
      "step": 1150
    },
    {
      "epoch": 0.21889821233126597,
      "grad_norm": 762592.875,
      "learning_rate": 1.0934792521659827e-05,
      "loss": 0.4396,
      "step": 1200
    },
    {
      "epoch": 0.22801897117840203,
      "grad_norm": 711147.125,
      "learning_rate": 1.1390788873689011e-05,
      "loss": 0.4072,
      "step": 1250
    },
    {
      "epoch": 0.23713973002553812,
      "grad_norm": 471668.34375,
      "learning_rate": 1.1846785225718196e-05,
      "loss": 0.4217,
      "step": 1300
    },
    {
      "epoch": 0.2462604888726742,
      "grad_norm": 353904.5625,
      "learning_rate": 1.2302781577747378e-05,
      "loss": 0.4368,
      "step": 1350
    },
    {
      "epoch": 0.2553812477198103,
      "grad_norm": 612999.625,
      "learning_rate": 1.2758777929776563e-05,
      "loss": 0.3692,
      "step": 1400
    },
    {
      "epoch": 0.2645020065669464,
      "grad_norm": 400021.6875,
      "learning_rate": 1.3214774281805747e-05,
      "loss": 0.381,
      "step": 1450
    },
    {
      "epoch": 0.2736227654140824,
      "grad_norm": 655072.375,
      "learning_rate": 1.3670770633834932e-05,
      "loss": 0.3679,
      "step": 1500
    },
    {
      "epoch": 0.2827435242612185,
      "grad_norm": 198947.984375,
      "learning_rate": 1.4126766985864113e-05,
      "loss": 0.4183,
      "step": 1550
    },
    {
      "epoch": 0.2918642831083546,
      "grad_norm": 545327.5,
      "learning_rate": 1.4582763337893297e-05,
      "loss": 0.3756,
      "step": 1600
    },
    {
      "epoch": 0.3009850419554907,
      "grad_norm": 476617.8125,
      "learning_rate": 1.5038759689922481e-05,
      "loss": 0.3379,
      "step": 1650
    },
    {
      "epoch": 0.3101058008026268,
      "grad_norm": 638888.375,
      "learning_rate": 1.5494756041951667e-05,
      "loss": 0.3547,
      "step": 1700
    },
    {
      "epoch": 0.3192265596497629,
      "grad_norm": 437538.875,
      "learning_rate": 1.595075239398085e-05,
      "loss": 0.3613,
      "step": 1750
    },
    {
      "epoch": 0.32834731849689897,
      "grad_norm": 565889.4375,
      "learning_rate": 1.6406748746010033e-05,
      "loss": 0.3084,
      "step": 1800
    },
    {
      "epoch": 0.337468077344035,
      "grad_norm": 689763.625,
      "learning_rate": 1.686274509803922e-05,
      "loss": 0.3319,
      "step": 1850
    },
    {
      "epoch": 0.3465888361911711,
      "grad_norm": 659720.75,
      "learning_rate": 1.73187414500684e-05,
      "loss": 0.2794,
      "step": 1900
    },
    {
      "epoch": 0.3557095950383072,
      "grad_norm": 410950.34375,
      "learning_rate": 1.7774737802097584e-05,
      "loss": 0.31,
      "step": 1950
    },
    {
      "epoch": 0.36483035388544327,
      "grad_norm": 377901.875,
      "learning_rate": 1.8230734154126767e-05,
      "loss": 0.2641,
      "step": 2000
    },
    {
      "epoch": 0.37395111273257936,
      "grad_norm": 69726.6484375,
      "learning_rate": 1.8686730506155953e-05,
      "loss": 0.3077,
      "step": 2050
    },
    {
      "epoch": 0.38307187157971545,
      "grad_norm": 512561.8125,
      "learning_rate": 1.9142726858185135e-05,
      "loss": 0.2944,
      "step": 2100
    },
    {
      "epoch": 0.39219263042685154,
      "grad_norm": 537200.125,
      "learning_rate": 1.9598723210214318e-05,
      "loss": 0.2898,
      "step": 2150
    },
    {
      "epoch": 0.4013133892739876,
      "grad_norm": 501728.40625,
      "learning_rate": 1.9993919432480366e-05,
      "loss": 0.2421,
      "step": 2200
    },
    {
      "epoch": 0.41043414812112367,
      "grad_norm": 499459.9375,
      "learning_rate": 1.9943248036483407e-05,
      "loss": 0.2521,
      "step": 2250
    },
    {
      "epoch": 0.41955490696825976,
      "grad_norm": 439026.53125,
      "learning_rate": 1.9892576640486448e-05,
      "loss": 0.2482,
      "step": 2300
    },
    {
      "epoch": 0.42867566581539585,
      "grad_norm": 508896.6875,
      "learning_rate": 1.9841905244489488e-05,
      "loss": 0.2579,
      "step": 2350
    },
    {
      "epoch": 0.43779642466253194,
      "grad_norm": 1120810.125,
      "learning_rate": 1.979123384849253e-05,
      "loss": 0.2382,
      "step": 2400
    },
    {
      "epoch": 0.446917183509668,
      "grad_norm": 468299.5,
      "learning_rate": 1.974056245249557e-05,
      "loss": 0.2531,
      "step": 2450
    },
    {
      "epoch": 0.45603794235680406,
      "grad_norm": 878376.875,
      "learning_rate": 1.968989105649861e-05,
      "loss": 0.2357,
      "step": 2500
    },
    {
      "epoch": 0.46515870120394015,
      "grad_norm": 135160.609375,
      "learning_rate": 1.963921966050165e-05,
      "loss": 0.2343,
      "step": 2550
    },
    {
      "epoch": 0.47427946005107624,
      "grad_norm": 270697.0,
      "learning_rate": 1.9588548264504688e-05,
      "loss": 0.2641,
      "step": 2600
    },
    {
      "epoch": 0.48340021889821233,
      "grad_norm": 697387.4375,
      "learning_rate": 1.953787686850773e-05,
      "loss": 0.2206,
      "step": 2650
    },
    {
      "epoch": 0.4925209777453484,
      "grad_norm": 374661.125,
      "learning_rate": 1.948720547251077e-05,
      "loss": 0.2651,
      "step": 2700
    },
    {
      "epoch": 0.5016417365924845,
      "grad_norm": 370686.71875,
      "learning_rate": 1.943653407651381e-05,
      "loss": 0.2287,
      "step": 2750
    },
    {
      "epoch": 0.5107624954396206,
      "grad_norm": 906469.8125,
      "learning_rate": 1.938586268051685e-05,
      "loss": 0.2224,
      "step": 2800
    },
    {
      "epoch": 0.5198832542867566,
      "grad_norm": 351443.28125,
      "learning_rate": 1.933519128451989e-05,
      "loss": 0.2629,
      "step": 2850
    },
    {
      "epoch": 0.5290040131338928,
      "grad_norm": 925489.625,
      "learning_rate": 1.9284519888522932e-05,
      "loss": 0.2387,
      "step": 2900
    },
    {
      "epoch": 0.5381247719810288,
      "grad_norm": 662768.375,
      "learning_rate": 1.923384849252597e-05,
      "loss": 0.2902,
      "step": 2950
    },
    {
      "epoch": 0.5472455308281649,
      "grad_norm": 27612.37109375,
      "learning_rate": 1.918317709652901e-05,
      "loss": 0.2399,
      "step": 3000
    },
    {
      "epoch": 0.556366289675301,
      "grad_norm": 465642.96875,
      "learning_rate": 1.913250570053205e-05,
      "loss": 0.2917,
      "step": 3050
    },
    {
      "epoch": 0.565487048522437,
      "grad_norm": 110167.90625,
      "learning_rate": 1.908183430453509e-05,
      "loss": 0.2652,
      "step": 3100
    },
    {
      "epoch": 0.5746078073695732,
      "grad_norm": 926643.0625,
      "learning_rate": 1.903116290853813e-05,
      "loss": 0.2123,
      "step": 3150
    },
    {
      "epoch": 0.5837285662167092,
      "grad_norm": 237950.4375,
      "learning_rate": 1.8980491512541172e-05,
      "loss": 0.2494,
      "step": 3200
    },
    {
      "epoch": 0.5928493250638454,
      "grad_norm": 573800.4375,
      "learning_rate": 1.8929820116544213e-05,
      "loss": 0.1816,
      "step": 3250
    },
    {
      "epoch": 0.6019700839109814,
      "grad_norm": 499455.40625,
      "learning_rate": 1.8879148720547254e-05,
      "loss": 0.2127,
      "step": 3300
    },
    {
      "epoch": 0.6110908427581174,
      "grad_norm": 645332.0625,
      "learning_rate": 1.8828477324550294e-05,
      "loss": 0.2139,
      "step": 3350
    },
    {
      "epoch": 0.6202116016052536,
      "grad_norm": 570036.8125,
      "learning_rate": 1.8777805928553335e-05,
      "loss": 0.2495,
      "step": 3400
    },
    {
      "epoch": 0.6293323604523896,
      "grad_norm": 571552.0625,
      "learning_rate": 1.8727134532556372e-05,
      "loss": 0.2361,
      "step": 3450
    },
    {
      "epoch": 0.6384531192995258,
      "grad_norm": 685822.875,
      "learning_rate": 1.8676463136559413e-05,
      "loss": 0.2217,
      "step": 3500
    },
    {
      "epoch": 0.6475738781466618,
      "grad_norm": 450229.59375,
      "learning_rate": 1.8625791740562453e-05,
      "loss": 0.2005,
      "step": 3550
    },
    {
      "epoch": 0.6566946369937979,
      "grad_norm": 291205.15625,
      "learning_rate": 1.8575120344565494e-05,
      "loss": 0.1855,
      "step": 3600
    },
    {
      "epoch": 0.665815395840934,
      "grad_norm": 568229.8125,
      "learning_rate": 1.8524448948568535e-05,
      "loss": 0.1757,
      "step": 3650
    },
    {
      "epoch": 0.67493615468807,
      "grad_norm": 418715.6875,
      "learning_rate": 1.8473777552571575e-05,
      "loss": 0.2017,
      "step": 3700
    },
    {
      "epoch": 0.6840569135352061,
      "grad_norm": 127682.7265625,
      "learning_rate": 1.8423106156574616e-05,
      "loss": 0.186,
      "step": 3750
    },
    {
      "epoch": 0.6931776723823422,
      "grad_norm": 795673.5,
      "learning_rate": 1.8372434760577657e-05,
      "loss": 0.2412,
      "step": 3800
    },
    {
      "epoch": 0.7022984312294783,
      "grad_norm": 65337.98828125,
      "learning_rate": 1.8321763364580697e-05,
      "loss": 0.2104,
      "step": 3850
    },
    {
      "epoch": 0.7114191900766144,
      "grad_norm": 618753.0625,
      "learning_rate": 1.8271091968583738e-05,
      "loss": 0.2262,
      "step": 3900
    },
    {
      "epoch": 0.7205399489237505,
      "grad_norm": 297139.34375,
      "learning_rate": 1.8220420572586775e-05,
      "loss": 0.2023,
      "step": 3950
    },
    {
      "epoch": 0.7296607077708865,
      "grad_norm": 860036.0625,
      "learning_rate": 1.8169749176589816e-05,
      "loss": 0.2153,
      "step": 4000
    },
    {
      "epoch": 0.7387814666180226,
      "grad_norm": 385338.96875,
      "learning_rate": 1.8119077780592856e-05,
      "loss": 0.202,
      "step": 4050
    },
    {
      "epoch": 0.7479022254651587,
      "grad_norm": 175294.28125,
      "learning_rate": 1.8068406384595897e-05,
      "loss": 0.1806,
      "step": 4100
    },
    {
      "epoch": 0.7570229843122948,
      "grad_norm": 92467.28125,
      "learning_rate": 1.8017734988598938e-05,
      "loss": 0.1844,
      "step": 4150
    },
    {
      "epoch": 0.7661437431594309,
      "grad_norm": 462566.28125,
      "learning_rate": 1.7967063592601978e-05,
      "loss": 0.1634,
      "step": 4200
    },
    {
      "epoch": 0.7752645020065669,
      "grad_norm": 131794.96875,
      "learning_rate": 1.7916392196605015e-05,
      "loss": 0.2244,
      "step": 4250
    },
    {
      "epoch": 0.7843852608537031,
      "grad_norm": 96948.9453125,
      "learning_rate": 1.7865720800608056e-05,
      "loss": 0.2306,
      "step": 4300
    },
    {
      "epoch": 0.7935060197008391,
      "grad_norm": 149633.703125,
      "learning_rate": 1.7815049404611097e-05,
      "loss": 0.1741,
      "step": 4350
    },
    {
      "epoch": 0.8026267785479752,
      "grad_norm": 111322.9765625,
      "learning_rate": 1.7764378008614137e-05,
      "loss": 0.222,
      "step": 4400
    },
    {
      "epoch": 0.8117475373951113,
      "grad_norm": 729147.625,
      "learning_rate": 1.7713706612617178e-05,
      "loss": 0.2101,
      "step": 4450
    },
    {
      "epoch": 0.8208682962422473,
      "grad_norm": 703945.0,
      "learning_rate": 1.766303521662022e-05,
      "loss": 0.1854,
      "step": 4500
    },
    {
      "epoch": 0.8299890550893835,
      "grad_norm": 168290.296875,
      "learning_rate": 1.761236382062326e-05,
      "loss": 0.1955,
      "step": 4550
    },
    {
      "epoch": 0.8391098139365195,
      "grad_norm": 283606.25,
      "learning_rate": 1.75616924246263e-05,
      "loss": 0.2009,
      "step": 4600
    },
    {
      "epoch": 0.8482305727836555,
      "grad_norm": 268640.03125,
      "learning_rate": 1.751102102862934e-05,
      "loss": 0.1936,
      "step": 4650
    },
    {
      "epoch": 0.8573513316307917,
      "grad_norm": 524306.125,
      "learning_rate": 1.746034963263238e-05,
      "loss": 0.1827,
      "step": 4700
    },
    {
      "epoch": 0.8664720904779277,
      "grad_norm": 254491.09375,
      "learning_rate": 1.7409678236635422e-05,
      "loss": 0.2057,
      "step": 4750
    },
    {
      "epoch": 0.8755928493250639,
      "grad_norm": 924322.5625,
      "learning_rate": 1.7359006840638462e-05,
      "loss": 0.1701,
      "step": 4800
    },
    {
      "epoch": 0.8847136081721999,
      "grad_norm": 167627.0625,
      "learning_rate": 1.7308335444641503e-05,
      "loss": 0.2277,
      "step": 4850
    },
    {
      "epoch": 0.893834367019336,
      "grad_norm": 96051.21875,
      "learning_rate": 1.7257664048644544e-05,
      "loss": 0.1808,
      "step": 4900
    },
    {
      "epoch": 0.9029551258664721,
      "grad_norm": 991511.0,
      "learning_rate": 1.7206992652647584e-05,
      "loss": 0.2206,
      "step": 4950
    },
    {
      "epoch": 0.9120758847136081,
      "grad_norm": 285384.96875,
      "learning_rate": 1.715632125665062e-05,
      "loss": 0.2018,
      "step": 5000
    },
    {
      "epoch": 0.9211966435607443,
      "grad_norm": 22526.392578125,
      "learning_rate": 1.7105649860653662e-05,
      "loss": 0.1588,
      "step": 5050
    },
    {
      "epoch": 0.9303174024078803,
      "grad_norm": 1007282.75,
      "learning_rate": 1.7054978464656703e-05,
      "loss": 0.2094,
      "step": 5100
    },
    {
      "epoch": 0.9394381612550164,
      "grad_norm": 586393.3125,
      "learning_rate": 1.7004307068659744e-05,
      "loss": 0.2068,
      "step": 5150
    },
    {
      "epoch": 0.9485589201021525,
      "grad_norm": 523450.8125,
      "learning_rate": 1.6953635672662784e-05,
      "loss": 0.2073,
      "step": 5200
    },
    {
      "epoch": 0.9576796789492886,
      "grad_norm": 929890.1875,
      "learning_rate": 1.690296427666582e-05,
      "loss": 0.1751,
      "step": 5250
    },
    {
      "epoch": 0.9668004377964247,
      "grad_norm": 1103105.25,
      "learning_rate": 1.6852292880668862e-05,
      "loss": 0.1675,
      "step": 5300
    },
    {
      "epoch": 0.9759211966435607,
      "grad_norm": 198764.5625,
      "learning_rate": 1.6801621484671903e-05,
      "loss": 0.1951,
      "step": 5350
    },
    {
      "epoch": 0.9850419554906968,
      "grad_norm": 31200.3125,
      "learning_rate": 1.6750950088674943e-05,
      "loss": 0.216,
      "step": 5400
    },
    {
      "epoch": 0.9941627143378329,
      "grad_norm": 561323.5625,
      "learning_rate": 1.6700278692677984e-05,
      "loss": 0.1999,
      "step": 5450
    },
    {
      "epoch": 1.003283473184969,
      "grad_norm": 757494.1875,
      "learning_rate": 1.6649607296681025e-05,
      "loss": 0.159,
      "step": 5500
    },
    {
      "epoch": 1.0124042320321052,
      "grad_norm": 590224.625,
      "learning_rate": 1.6598935900684065e-05,
      "loss": 0.1656,
      "step": 5550
    },
    {
      "epoch": 1.0215249908792412,
      "grad_norm": 119205.25,
      "learning_rate": 1.6548264504687106e-05,
      "loss": 0.1813,
      "step": 5600
    },
    {
      "epoch": 1.0306457497263772,
      "grad_norm": 98343.6953125,
      "learning_rate": 1.6497593108690147e-05,
      "loss": 0.1775,
      "step": 5650
    },
    {
      "epoch": 1.0397665085735133,
      "grad_norm": 437897.34375,
      "learning_rate": 1.6446921712693187e-05,
      "loss": 0.1385,
      "step": 5700
    },
    {
      "epoch": 1.0488872674206493,
      "grad_norm": 41403.91015625,
      "learning_rate": 1.6396250316696228e-05,
      "loss": 0.1422,
      "step": 5750
    },
    {
      "epoch": 1.0580080262677856,
      "grad_norm": 416996.53125,
      "learning_rate": 1.634557892069927e-05,
      "loss": 0.2186,
      "step": 5800
    },
    {
      "epoch": 1.0671287851149216,
      "grad_norm": 524399.5625,
      "learning_rate": 1.6294907524702306e-05,
      "loss": 0.1505,
      "step": 5850
    },
    {
      "epoch": 1.0762495439620576,
      "grad_norm": 465325.0625,
      "learning_rate": 1.6244236128705346e-05,
      "loss": 0.1445,
      "step": 5900
    },
    {
      "epoch": 1.0853703028091937,
      "grad_norm": 75256.359375,
      "learning_rate": 1.6193564732708387e-05,
      "loss": 0.1292,
      "step": 5950
    },
    {
      "epoch": 1.09449106165633,
      "grad_norm": 337780.5,
      "learning_rate": 1.6142893336711428e-05,
      "loss": 0.1295,
      "step": 6000
    },
    {
      "epoch": 1.103611820503466,
      "grad_norm": 947174.5,
      "learning_rate": 1.6092221940714468e-05,
      "loss": 0.1642,
      "step": 6050
    },
    {
      "epoch": 1.112732579350602,
      "grad_norm": 800064.6875,
      "learning_rate": 1.604155054471751e-05,
      "loss": 0.1734,
      "step": 6100
    },
    {
      "epoch": 1.121853338197738,
      "grad_norm": 478807.5,
      "learning_rate": 1.599087914872055e-05,
      "loss": 0.1573,
      "step": 6150
    },
    {
      "epoch": 1.130974097044874,
      "grad_norm": 288940.875,
      "learning_rate": 1.594020775272359e-05,
      "loss": 0.1948,
      "step": 6200
    },
    {
      "epoch": 1.14009485589201,
      "grad_norm": 28141.294921875,
      "learning_rate": 1.588953635672663e-05,
      "loss": 0.1509,
      "step": 6250
    },
    {
      "epoch": 1.1492156147391464,
      "grad_norm": 1682346.125,
      "learning_rate": 1.5838864960729668e-05,
      "loss": 0.1471,
      "step": 6300
    },
    {
      "epoch": 1.1583363735862824,
      "grad_norm": 1039749.25,
      "learning_rate": 1.578819356473271e-05,
      "loss": 0.2062,
      "step": 6350
    },
    {
      "epoch": 1.1674571324334184,
      "grad_norm": 260467.796875,
      "learning_rate": 1.573752216873575e-05,
      "loss": 0.1461,
      "step": 6400
    },
    {
      "epoch": 1.1765778912805545,
      "grad_norm": 1509705.25,
      "learning_rate": 1.568685077273879e-05,
      "loss": 0.167,
      "step": 6450
    },
    {
      "epoch": 1.1856986501276907,
      "grad_norm": 82328.40625,
      "learning_rate": 1.563617937674183e-05,
      "loss": 0.1225,
      "step": 6500
    },
    {
      "epoch": 1.1948194089748267,
      "grad_norm": 19242.591796875,
      "learning_rate": 1.558550798074487e-05,
      "loss": 0.1785,
      "step": 6550
    },
    {
      "epoch": 1.2039401678219628,
      "grad_norm": 78780.625,
      "learning_rate": 1.5534836584747912e-05,
      "loss": 0.1669,
      "step": 6600
    },
    {
      "epoch": 1.2130609266690988,
      "grad_norm": 5044.451171875,
      "learning_rate": 1.548416518875095e-05,
      "loss": 0.166,
      "step": 6650
    },
    {
      "epoch": 1.2221816855162349,
      "grad_norm": 22234.224609375,
      "learning_rate": 1.543349379275399e-05,
      "loss": 0.1588,
      "step": 6700
    },
    {
      "epoch": 1.231302444363371,
      "grad_norm": 280961.125,
      "learning_rate": 1.538282239675703e-05,
      "loss": 0.1424,
      "step": 6750
    },
    {
      "epoch": 1.2404232032105071,
      "grad_norm": 549833.8125,
      "learning_rate": 1.533215100076007e-05,
      "loss": 0.167,
      "step": 6800
    },
    {
      "epoch": 1.2495439620576432,
      "grad_norm": 23013.283203125,
      "learning_rate": 1.5281479604763112e-05,
      "loss": 0.151,
      "step": 6850
    },
    {
      "epoch": 1.2586647209047792,
      "grad_norm": 573590.625,
      "learning_rate": 1.5230808208766152e-05,
      "loss": 0.159,
      "step": 6900
    },
    {
      "epoch": 1.2677854797519155,
      "grad_norm": 526045.125,
      "learning_rate": 1.5180136812769193e-05,
      "loss": 0.1913,
      "step": 6950
    },
    {
      "epoch": 1.2769062385990515,
      "grad_norm": 945480.25,
      "learning_rate": 1.5129465416772234e-05,
      "loss": 0.1628,
      "step": 7000
    },
    {
      "epoch": 1.2860269974461875,
      "grad_norm": 671979.6875,
      "learning_rate": 1.5078794020775274e-05,
      "loss": 0.2049,
      "step": 7050
    },
    {
      "epoch": 1.2951477562933236,
      "grad_norm": 473662.40625,
      "learning_rate": 1.5028122624778313e-05,
      "loss": 0.1791,
      "step": 7100
    },
    {
      "epoch": 1.3042685151404596,
      "grad_norm": 824720.25,
      "learning_rate": 1.4977451228781354e-05,
      "loss": 0.128,
      "step": 7150
    },
    {
      "epoch": 1.3133892739875956,
      "grad_norm": 133404.390625,
      "learning_rate": 1.4926779832784395e-05,
      "loss": 0.1659,
      "step": 7200
    },
    {
      "epoch": 1.322510032834732,
      "grad_norm": 439313.0,
      "learning_rate": 1.4876108436787435e-05,
      "loss": 0.1822,
      "step": 7250
    },
    {
      "epoch": 1.331630791681868,
      "grad_norm": 13203.1181640625,
      "learning_rate": 1.4825437040790476e-05,
      "loss": 0.1593,
      "step": 7300
    },
    {
      "epoch": 1.340751550529004,
      "grad_norm": 280689.34375,
      "learning_rate": 1.4774765644793516e-05,
      "loss": 0.1432,
      "step": 7350
    },
    {
      "epoch": 1.3498723093761402,
      "grad_norm": 1033610.0625,
      "learning_rate": 1.4724094248796557e-05,
      "loss": 0.1918,
      "step": 7400
    },
    {
      "epoch": 1.3589930682232763,
      "grad_norm": 495282.96875,
      "learning_rate": 1.4673422852799594e-05,
      "loss": 0.1778,
      "step": 7450
    },
    {
      "epoch": 1.3681138270704123,
      "grad_norm": 353377.9375,
      "learning_rate": 1.4622751456802635e-05,
      "loss": 0.1821,
      "step": 7500
    },
    {
      "epoch": 1.3772345859175483,
      "grad_norm": 32913.53125,
      "learning_rate": 1.4572080060805676e-05,
      "loss": 0.1832,
      "step": 7550
    },
    {
      "epoch": 1.3863553447646844,
      "grad_norm": 132365.265625,
      "learning_rate": 1.4521408664808716e-05,
      "loss": 0.1665,
      "step": 7600
    },
    {
      "epoch": 1.3954761036118204,
      "grad_norm": 82107.2421875,
      "learning_rate": 1.4470737268811757e-05,
      "loss": 0.167,
      "step": 7650
    },
    {
      "epoch": 1.4045968624589567,
      "grad_norm": 15999.71875,
      "learning_rate": 1.4420065872814798e-05,
      "loss": 0.1441,
      "step": 7700
    },
    {
      "epoch": 1.4137176213060927,
      "grad_norm": 901016.9375,
      "learning_rate": 1.4369394476817836e-05,
      "loss": 0.1701,
      "step": 7750
    },
    {
      "epoch": 1.4228383801532287,
      "grad_norm": 138710.6875,
      "learning_rate": 1.4318723080820877e-05,
      "loss": 0.1634,
      "step": 7800
    },
    {
      "epoch": 1.4319591390003648,
      "grad_norm": 520854.4375,
      "learning_rate": 1.4268051684823918e-05,
      "loss": 0.1893,
      "step": 7850
    },
    {
      "epoch": 1.441079897847501,
      "grad_norm": 173379.4375,
      "learning_rate": 1.4217380288826958e-05,
      "loss": 0.1396,
      "step": 7900
    },
    {
      "epoch": 1.450200656694637,
      "grad_norm": 62904.48046875,
      "learning_rate": 1.4166708892829999e-05,
      "loss": 0.1375,
      "step": 7950
    },
    {
      "epoch": 1.459321415541773,
      "grad_norm": 1227663.25,
      "learning_rate": 1.411603749683304e-05,
      "loss": 0.2117,
      "step": 8000
    },
    {
      "epoch": 1.4684421743889091,
      "grad_norm": 1724358.375,
      "learning_rate": 1.406536610083608e-05,
      "loss": 0.1412,
      "step": 8050
    },
    {
      "epoch": 1.4775629332360452,
      "grad_norm": 545782.875,
      "learning_rate": 1.4014694704839121e-05,
      "loss": 0.1394,
      "step": 8100
    },
    {
      "epoch": 1.4866836920831812,
      "grad_norm": 20171.197265625,
      "learning_rate": 1.396402330884216e-05,
      "loss": 0.1936,
      "step": 8150
    },
    {
      "epoch": 1.4958044509303174,
      "grad_norm": 364950.53125,
      "learning_rate": 1.39133519128452e-05,
      "loss": 0.1783,
      "step": 8200
    },
    {
      "epoch": 1.5049252097774535,
      "grad_norm": 7368.8564453125,
      "learning_rate": 1.386268051684824e-05,
      "loss": 0.1576,
      "step": 8250
    },
    {
      "epoch": 1.5140459686245895,
      "grad_norm": 55676.6640625,
      "learning_rate": 1.381200912085128e-05,
      "loss": 0.162,
      "step": 8300
    },
    {
      "epoch": 1.5231667274717258,
      "grad_norm": 9863.220703125,
      "learning_rate": 1.376133772485432e-05,
      "loss": 0.1343,
      "step": 8350
    },
    {
      "epoch": 1.5322874863188618,
      "grad_norm": 15558.63671875,
      "learning_rate": 1.371066632885736e-05,
      "loss": 0.1452,
      "step": 8400
    },
    {
      "epoch": 1.5414082451659978,
      "grad_norm": 216820.484375,
      "learning_rate": 1.36599949328604e-05,
      "loss": 0.1496,
      "step": 8450
    },
    {
      "epoch": 1.5505290040131339,
      "grad_norm": 865911.5625,
      "learning_rate": 1.3609323536863441e-05,
      "loss": 0.158,
      "step": 8500
    },
    {
      "epoch": 1.55964976286027,
      "grad_norm": 949935.875,
      "learning_rate": 1.3558652140866482e-05,
      "loss": 0.124,
      "step": 8550
    },
    {
      "epoch": 1.568770521707406,
      "grad_norm": 17069.2265625,
      "learning_rate": 1.3507980744869522e-05,
      "loss": 0.1756,
      "step": 8600
    },
    {
      "epoch": 1.577891280554542,
      "grad_norm": 8642.16796875,
      "learning_rate": 1.3457309348872563e-05,
      "loss": 0.1726,
      "step": 8650
    },
    {
      "epoch": 1.5870120394016782,
      "grad_norm": 17426.0703125,
      "learning_rate": 1.3406637952875603e-05,
      "loss": 0.1555,
      "step": 8700
    },
    {
      "epoch": 1.5961327982488143,
      "grad_norm": 1078521.75,
      "learning_rate": 1.3355966556878644e-05,
      "loss": 0.1452,
      "step": 8750
    },
    {
      "epoch": 1.6052535570959505,
      "grad_norm": 280474.6875,
      "learning_rate": 1.3305295160881683e-05,
      "loss": 0.1912,
      "step": 8800
    },
    {
      "epoch": 1.6143743159430866,
      "grad_norm": 837893.5625,
      "learning_rate": 1.3254623764884724e-05,
      "loss": 0.1308,
      "step": 8850
    },
    {
      "epoch": 1.6234950747902226,
      "grad_norm": 288069.34375,
      "learning_rate": 1.3203952368887764e-05,
      "loss": 0.1662,
      "step": 8900
    },
    {
      "epoch": 1.6326158336373586,
      "grad_norm": 235165.078125,
      "learning_rate": 1.3153280972890805e-05,
      "loss": 0.1406,
      "step": 8950
    },
    {
      "epoch": 1.6417365924844947,
      "grad_norm": 9408.2802734375,
      "learning_rate": 1.3102609576893846e-05,
      "loss": 0.1805,
      "step": 9000
    },
    {
      "epoch": 1.6508573513316307,
      "grad_norm": 814492.625,
      "learning_rate": 1.3051938180896883e-05,
      "loss": 0.134,
      "step": 9050
    },
    {
      "epoch": 1.6599781101787667,
      "grad_norm": 843453.4375,
      "learning_rate": 1.3001266784899924e-05,
      "loss": 0.1474,
      "step": 9100
    },
    {
      "epoch": 1.669098869025903,
      "grad_norm": 8126.31005859375,
      "learning_rate": 1.2950595388902964e-05,
      "loss": 0.1493,
      "step": 9150
    },
    {
      "epoch": 1.678219627873039,
      "grad_norm": 720370.3125,
      "learning_rate": 1.2899923992906005e-05,
      "loss": 0.1354,
      "step": 9200
    },
    {
      "epoch": 1.6873403867201753,
      "grad_norm": 412899.96875,
      "learning_rate": 1.2849252596909045e-05,
      "loss": 0.1669,
      "step": 9250
    },
    {
      "epoch": 1.6964611455673113,
      "grad_norm": 530867.625,
      "learning_rate": 1.2798581200912086e-05,
      "loss": 0.1028,
      "step": 9300
    },
    {
      "epoch": 1.7055819044144473,
      "grad_norm": 9509.8017578125,
      "learning_rate": 1.2747909804915127e-05,
      "loss": 0.1213,
      "step": 9350
    },
    {
      "epoch": 1.7147026632615834,
      "grad_norm": 117835.7265625,
      "learning_rate": 1.2697238408918167e-05,
      "loss": 0.157,
      "step": 9400
    },
    {
      "epoch": 1.7238234221087194,
      "grad_norm": 9566.6044921875,
      "learning_rate": 1.2646567012921206e-05,
      "loss": 0.1434,
      "step": 9450
    },
    {
      "epoch": 1.7329441809558555,
      "grad_norm": 122376.7578125,
      "learning_rate": 1.2595895616924247e-05,
      "loss": 0.1489,
      "step": 9500
    },
    {
      "epoch": 1.7420649398029915,
      "grad_norm": 12444.4609375,
      "learning_rate": 1.2545224220927288e-05,
      "loss": 0.1827,
      "step": 9550
    },
    {
      "epoch": 1.7511856986501277,
      "grad_norm": 209636.859375,
      "learning_rate": 1.2494552824930328e-05,
      "loss": 0.1334,
      "step": 9600
    },
    {
      "epoch": 1.7603064574972638,
      "grad_norm": 138764.125,
      "learning_rate": 1.2443881428933369e-05,
      "loss": 0.176,
      "step": 9650
    },
    {
      "epoch": 1.7694272163443998,
      "grad_norm": 1398038.125,
      "learning_rate": 1.239321003293641e-05,
      "loss": 0.1199,
      "step": 9700
    },
    {
      "epoch": 1.778547975191536,
      "grad_norm": 4111.6298828125,
      "learning_rate": 1.234253863693945e-05,
      "loss": 0.1465,
      "step": 9750
    },
    {
      "epoch": 1.787668734038672,
      "grad_norm": 672973.0625,
      "learning_rate": 1.229186724094249e-05,
      "loss": 0.1564,
      "step": 9800
    },
    {
      "epoch": 1.7967894928858081,
      "grad_norm": 875977.3125,
      "learning_rate": 1.2241195844945528e-05,
      "loss": 0.1393,
      "step": 9850
    },
    {
      "epoch": 1.8059102517329442,
      "grad_norm": 508638.21875,
      "learning_rate": 1.2190524448948569e-05,
      "loss": 0.1443,
      "step": 9900
    },
    {
      "epoch": 1.8150310105800802,
      "grad_norm": 311908.78125,
      "learning_rate": 1.213985305295161e-05,
      "loss": 0.1519,
      "step": 9950
    },
    {
      "epoch": 1.8241517694272162,
      "grad_norm": 31275.94921875,
      "learning_rate": 1.208918165695465e-05,
      "loss": 0.1562,
      "step": 10000
    },
    {
      "epoch": 1.8332725282743523,
      "grad_norm": 576478.3125,
      "learning_rate": 1.203851026095769e-05,
      "loss": 0.0994,
      "step": 10050
    },
    {
      "epoch": 1.8423932871214885,
      "grad_norm": 72607.15625,
      "learning_rate": 1.198783886496073e-05,
      "loss": 0.1282,
      "step": 10100
    },
    {
      "epoch": 1.8515140459686246,
      "grad_norm": 9223.2470703125,
      "learning_rate": 1.193716746896377e-05,
      "loss": 0.1114,
      "step": 10150
    },
    {
      "epoch": 1.8606348048157608,
      "grad_norm": 942542.6875,
      "learning_rate": 1.188649607296681e-05,
      "loss": 0.1546,
      "step": 10200
    },
    {
      "epoch": 1.8697555636628969,
      "grad_norm": 509781.625,
      "learning_rate": 1.1835824676969851e-05,
      "loss": 0.1712,
      "step": 10250
    },
    {
      "epoch": 1.878876322510033,
      "grad_norm": 24648.212890625,
      "learning_rate": 1.1785153280972892e-05,
      "loss": 0.1773,
      "step": 10300
    },
    {
      "epoch": 1.887997081357169,
      "grad_norm": 345658.4375,
      "learning_rate": 1.1734481884975933e-05,
      "loss": 0.1519,
      "step": 10350
    },
    {
      "epoch": 1.897117840204305,
      "grad_norm": 389879.65625,
      "learning_rate": 1.1683810488978973e-05,
      "loss": 0.1342,
      "step": 10400
    },
    {
      "epoch": 1.906238599051441,
      "grad_norm": 26194.060546875,
      "learning_rate": 1.1633139092982014e-05,
      "loss": 0.1263,
      "step": 10450
    },
    {
      "epoch": 1.915359357898577,
      "grad_norm": 475314.625,
      "learning_rate": 1.1582467696985053e-05,
      "loss": 0.152,
      "step": 10500
    },
    {
      "epoch": 1.9244801167457133,
      "grad_norm": 377996.40625,
      "learning_rate": 1.1531796300988094e-05,
      "loss": 0.1471,
      "step": 10550
    },
    {
      "epoch": 1.9336008755928493,
      "grad_norm": 193409.953125,
      "learning_rate": 1.1481124904991134e-05,
      "loss": 0.1287,
      "step": 10600
    },
    {
      "epoch": 1.9427216344399854,
      "grad_norm": 151753.75,
      "learning_rate": 1.1430453508994175e-05,
      "loss": 0.1657,
      "step": 10650
    },
    {
      "epoch": 1.9518423932871216,
      "grad_norm": 88547.9296875,
      "learning_rate": 1.1379782112997214e-05,
      "loss": 0.1398,
      "step": 10700
    },
    {
      "epoch": 1.9609631521342576,
      "grad_norm": 533953.75,
      "learning_rate": 1.1329110717000253e-05,
      "loss": 0.1369,
      "step": 10750
    },
    {
      "epoch": 1.9700839109813937,
      "grad_norm": 280480.03125,
      "learning_rate": 1.1278439321003293e-05,
      "loss": 0.1371,
      "step": 10800
    },
    {
      "epoch": 1.9792046698285297,
      "grad_norm": 1142595.625,
      "learning_rate": 1.1227767925006334e-05,
      "loss": 0.156,
      "step": 10850
    },
    {
      "epoch": 1.9883254286756658,
      "grad_norm": 63976.15234375,
      "learning_rate": 1.1177096529009375e-05,
      "loss": 0.1144,
      "step": 10900
    },
    {
      "epoch": 1.9974461875228018,
      "grad_norm": 315432.125,
      "learning_rate": 1.1126425133012415e-05,
      "loss": 0.177,
      "step": 10950
    },
    {
      "epoch": 2.006566946369938,
      "grad_norm": 518792.28125,
      "learning_rate": 1.1075753737015456e-05,
      "loss": 0.0956,
      "step": 11000
    },
    {
      "epoch": 2.015687705217074,
      "grad_norm": 35010.015625,
      "learning_rate": 1.1025082341018497e-05,
      "loss": 0.1294,
      "step": 11050
    },
    {
      "epoch": 2.0248084640642103,
      "grad_norm": 463834.90625,
      "learning_rate": 1.0974410945021537e-05,
      "loss": 0.1236,
      "step": 11100
    },
    {
      "epoch": 2.0339292229113464,
      "grad_norm": 612922.125,
      "learning_rate": 1.0923739549024576e-05,
      "loss": 0.1488,
      "step": 11150
    },
    {
      "epoch": 2.0430499817584824,
      "grad_norm": 6927.66552734375,
      "learning_rate": 1.0873068153027617e-05,
      "loss": 0.0844,
      "step": 11200
    },
    {
      "epoch": 2.0521707406056184,
      "grad_norm": 16208.74609375,
      "learning_rate": 1.0822396757030657e-05,
      "loss": 0.0862,
      "step": 11250
    },
    {
      "epoch": 2.0612914994527545,
      "grad_norm": 12963.083984375,
      "learning_rate": 1.0771725361033698e-05,
      "loss": 0.1205,
      "step": 11300
    },
    {
      "epoch": 2.0704122582998905,
      "grad_norm": 823055.8125,
      "learning_rate": 1.0721053965036739e-05,
      "loss": 0.1339,
      "step": 11350
    },
    {
      "epoch": 2.0795330171470265,
      "grad_norm": 13406.169921875,
      "learning_rate": 1.067038256903978e-05,
      "loss": 0.0968,
      "step": 11400
    },
    {
      "epoch": 2.0886537759941626,
      "grad_norm": 1094761.5,
      "learning_rate": 1.061971117304282e-05,
      "loss": 0.117,
      "step": 11450
    },
    {
      "epoch": 2.0977745348412986,
      "grad_norm": 346153.0,
      "learning_rate": 1.0569039777045857e-05,
      "loss": 0.126,
      "step": 11500
    },
    {
      "epoch": 2.106895293688435,
      "grad_norm": 29862.87109375,
      "learning_rate": 1.0518368381048898e-05,
      "loss": 0.0707,
      "step": 11550
    },
    {
      "epoch": 2.116016052535571,
      "grad_norm": 17524.248046875,
      "learning_rate": 1.0467696985051939e-05,
      "loss": 0.1058,
      "step": 11600
    },
    {
      "epoch": 2.125136811382707,
      "grad_norm": 27132.626953125,
      "learning_rate": 1.0417025589054979e-05,
      "loss": 0.1221,
      "step": 11650
    },
    {
      "epoch": 2.134257570229843,
      "grad_norm": 18611.271484375,
      "learning_rate": 1.036635419305802e-05,
      "loss": 0.1069,
      "step": 11700
    },
    {
      "epoch": 2.1433783290769792,
      "grad_norm": 6232.9599609375,
      "learning_rate": 1.031568279706106e-05,
      "loss": 0.1302,
      "step": 11750
    },
    {
      "epoch": 2.1524990879241153,
      "grad_norm": 25218.12109375,
      "learning_rate": 1.02650114010641e-05,
      "loss": 0.1079,
      "step": 11800
    },
    {
      "epoch": 2.1616198467712513,
      "grad_norm": 1335923.625,
      "learning_rate": 1.021434000506714e-05,
      "loss": 0.1322,
      "step": 11850
    },
    {
      "epoch": 2.1707406056183873,
      "grad_norm": 1676924.375,
      "learning_rate": 1.016366860907018e-05,
      "loss": 0.11,
      "step": 11900
    },
    {
      "epoch": 2.1798613644655234,
      "grad_norm": 756576.25,
      "learning_rate": 1.0112997213073221e-05,
      "loss": 0.1003,
      "step": 11950
    },
    {
      "epoch": 2.18898212331266,
      "grad_norm": 6653.81494140625,
      "learning_rate": 1.0062325817076262e-05,
      "loss": 0.1046,
      "step": 12000
    },
    {
      "epoch": 2.198102882159796,
      "grad_norm": 810981.125,
      "learning_rate": 1.0011654421079303e-05,
      "loss": 0.0843,
      "step": 12050
    },
    {
      "epoch": 2.207223641006932,
      "grad_norm": 178732.953125,
      "learning_rate": 9.960983025082341e-06,
      "loss": 0.0967,
      "step": 12100
    },
    {
      "epoch": 2.216344399854068,
      "grad_norm": 1141952.5,
      "learning_rate": 9.910311629085382e-06,
      "loss": 0.0823,
      "step": 12150
    },
    {
      "epoch": 2.225465158701204,
      "grad_norm": 5572.48046875,
      "learning_rate": 9.859640233088423e-06,
      "loss": 0.1006,
      "step": 12200
    },
    {
      "epoch": 2.23458591754834,
      "grad_norm": 1683108.75,
      "learning_rate": 9.808968837091463e-06,
      "loss": 0.1176,
      "step": 12250
    },
    {
      "epoch": 2.243706676395476,
      "grad_norm": 104974.5390625,
      "learning_rate": 9.758297441094502e-06,
      "loss": 0.1132,
      "step": 12300
    },
    {
      "epoch": 2.252827435242612,
      "grad_norm": 617514.1875,
      "learning_rate": 9.707626045097543e-06,
      "loss": 0.0899,
      "step": 12350
    },
    {
      "epoch": 2.261948194089748,
      "grad_norm": 73502.5234375,
      "learning_rate": 9.656954649100584e-06,
      "loss": 0.1556,
      "step": 12400
    },
    {
      "epoch": 2.271068952936884,
      "grad_norm": 191302.25,
      "learning_rate": 9.606283253103624e-06,
      "loss": 0.1235,
      "step": 12450
    },
    {
      "epoch": 2.28018971178402,
      "grad_norm": 18955.9296875,
      "learning_rate": 9.555611857106663e-06,
      "loss": 0.0971,
      "step": 12500
    },
    {
      "epoch": 2.2893104706311567,
      "grad_norm": 609338.9375,
      "learning_rate": 9.504940461109704e-06,
      "loss": 0.0764,
      "step": 12550
    },
    {
      "epoch": 2.2984312294782927,
      "grad_norm": 49400.29296875,
      "learning_rate": 9.454269065112744e-06,
      "loss": 0.116,
      "step": 12600
    },
    {
      "epoch": 2.3075519883254287,
      "grad_norm": 5707.58447265625,
      "learning_rate": 9.403597669115785e-06,
      "loss": 0.1071,
      "step": 12650
    },
    {
      "epoch": 2.3166727471725648,
      "grad_norm": 8319.708984375,
      "learning_rate": 9.352926273118826e-06,
      "loss": 0.1108,
      "step": 12700
    },
    {
      "epoch": 2.325793506019701,
      "grad_norm": 1522824.25,
      "learning_rate": 9.302254877121866e-06,
      "loss": 0.1228,
      "step": 12750
    },
    {
      "epoch": 2.334914264866837,
      "grad_norm": 8568.81640625,
      "learning_rate": 9.251583481124905e-06,
      "loss": 0.1304,
      "step": 12800
    },
    {
      "epoch": 2.344035023713973,
      "grad_norm": 409862.28125,
      "learning_rate": 9.200912085127946e-06,
      "loss": 0.0932,
      "step": 12850
    },
    {
      "epoch": 2.353155782561109,
      "grad_norm": 809333.6875,
      "learning_rate": 9.150240689130987e-06,
      "loss": 0.1201,
      "step": 12900
    },
    {
      "epoch": 2.362276541408245,
      "grad_norm": 5091.81201171875,
      "learning_rate": 9.099569293134026e-06,
      "loss": 0.1079,
      "step": 12950
    },
    {
      "epoch": 2.3713973002553814,
      "grad_norm": 12275.2744140625,
      "learning_rate": 9.048897897137066e-06,
      "loss": 0.0808,
      "step": 13000
    },
    {
      "epoch": 2.3805180591025175,
      "grad_norm": 778364.9375,
      "learning_rate": 8.998226501140107e-06,
      "loss": 0.1252,
      "step": 13050
    },
    {
      "epoch": 2.3896388179496535,
      "grad_norm": 1392218.875,
      "learning_rate": 8.947555105143147e-06,
      "loss": 0.1573,
      "step": 13100
    },
    {
      "epoch": 2.3987595767967895,
      "grad_norm": 91139.6796875,
      "learning_rate": 8.896883709146188e-06,
      "loss": 0.1527,
      "step": 13150
    },
    {
      "epoch": 2.4078803356439256,
      "grad_norm": 24613.296875,
      "learning_rate": 8.846212313149229e-06,
      "loss": 0.1082,
      "step": 13200
    },
    {
      "epoch": 2.4170010944910616,
      "grad_norm": 5775.83984375,
      "learning_rate": 8.79554091715227e-06,
      "loss": 0.1051,
      "step": 13250
    },
    {
      "epoch": 2.4261218533381976,
      "grad_norm": 137943.359375,
      "learning_rate": 8.744869521155308e-06,
      "loss": 0.0708,
      "step": 13300
    },
    {
      "epoch": 2.4352426121853337,
      "grad_norm": 10633.3671875,
      "learning_rate": 8.694198125158349e-06,
      "loss": 0.1113,
      "step": 13350
    },
    {
      "epoch": 2.4443633710324697,
      "grad_norm": 8726.71484375,
      "learning_rate": 8.64352672916139e-06,
      "loss": 0.0938,
      "step": 13400
    },
    {
      "epoch": 2.453484129879606,
      "grad_norm": 13200.255859375,
      "learning_rate": 8.592855333164429e-06,
      "loss": 0.1603,
      "step": 13450
    },
    {
      "epoch": 2.462604888726742,
      "grad_norm": 11513.1064453125,
      "learning_rate": 8.54218393716747e-06,
      "loss": 0.1038,
      "step": 13500
    },
    {
      "epoch": 2.4717256475738782,
      "grad_norm": 1077261.5,
      "learning_rate": 8.49151254117051e-06,
      "loss": 0.1487,
      "step": 13550
    },
    {
      "epoch": 2.4808464064210143,
      "grad_norm": 102814.7734375,
      "learning_rate": 8.44084114517355e-06,
      "loss": 0.0871,
      "step": 13600
    },
    {
      "epoch": 2.4899671652681503,
      "grad_norm": 8294.140625,
      "learning_rate": 8.390169749176591e-06,
      "loss": 0.0677,
      "step": 13650
    },
    {
      "epoch": 2.4990879241152864,
      "grad_norm": 171011.953125,
      "learning_rate": 8.33949835317963e-06,
      "loss": 0.1383,
      "step": 13700
    },
    {
      "epoch": 2.5082086829624224,
      "grad_norm": 948796.125,
      "learning_rate": 8.28882695718267e-06,
      "loss": 0.1327,
      "step": 13750
    },
    {
      "epoch": 2.5173294418095584,
      "grad_norm": 108077.9296875,
      "learning_rate": 8.238155561185711e-06,
      "loss": 0.1194,
      "step": 13800
    },
    {
      "epoch": 2.5264502006566945,
      "grad_norm": 891588.625,
      "learning_rate": 8.187484165188752e-06,
      "loss": 0.1191,
      "step": 13850
    },
    {
      "epoch": 2.535570959503831,
      "grad_norm": 231775.171875,
      "learning_rate": 8.136812769191793e-06,
      "loss": 0.1023,
      "step": 13900
    },
    {
      "epoch": 2.5446917183509665,
      "grad_norm": 326468.28125,
      "learning_rate": 8.086141373194832e-06,
      "loss": 0.1128,
      "step": 13950
    },
    {
      "epoch": 2.553812477198103,
      "grad_norm": 7085.73828125,
      "learning_rate": 8.035469977197872e-06,
      "loss": 0.0896,
      "step": 14000
    },
    {
      "epoch": 2.562933236045239,
      "grad_norm": 11163.0478515625,
      "learning_rate": 7.984798581200913e-06,
      "loss": 0.088,
      "step": 14050
    },
    {
      "epoch": 2.572053994892375,
      "grad_norm": 870912.75,
      "learning_rate": 7.934127185203952e-06,
      "loss": 0.1218,
      "step": 14100
    },
    {
      "epoch": 2.581174753739511,
      "grad_norm": 2076912.875,
      "learning_rate": 7.883455789206992e-06,
      "loss": 0.0741,
      "step": 14150
    },
    {
      "epoch": 2.590295512586647,
      "grad_norm": 560899.625,
      "learning_rate": 7.832784393210033e-06,
      "loss": 0.1038,
      "step": 14200
    },
    {
      "epoch": 2.599416271433783,
      "grad_norm": 145081.390625,
      "learning_rate": 7.782112997213074e-06,
      "loss": 0.1033,
      "step": 14250
    },
    {
      "epoch": 2.608537030280919,
      "grad_norm": 13475.7421875,
      "learning_rate": 7.731441601216114e-06,
      "loss": 0.076,
      "step": 14300
    },
    {
      "epoch": 2.6176577891280557,
      "grad_norm": 8091.7548828125,
      "learning_rate": 7.680770205219155e-06,
      "loss": 0.1139,
      "step": 14350
    },
    {
      "epoch": 2.6267785479751913,
      "grad_norm": 7125.2431640625,
      "learning_rate": 7.630098809222196e-06,
      "loss": 0.1011,
      "step": 14400
    },
    {
      "epoch": 2.6358993068223278,
      "grad_norm": 418235.0,
      "learning_rate": 7.579427413225235e-06,
      "loss": 0.0998,
      "step": 14450
    },
    {
      "epoch": 2.645020065669464,
      "grad_norm": 6137.4609375,
      "learning_rate": 7.528756017228274e-06,
      "loss": 0.1203,
      "step": 14500
    },
    {
      "epoch": 2.6541408245166,
      "grad_norm": 895619.5625,
      "learning_rate": 7.478084621231315e-06,
      "loss": 0.0989,
      "step": 14550
    },
    {
      "epoch": 2.663261583363736,
      "grad_norm": 1271397.875,
      "learning_rate": 7.427413225234356e-06,
      "loss": 0.1066,
      "step": 14600
    },
    {
      "epoch": 2.672382342210872,
      "grad_norm": 97996.046875,
      "learning_rate": 7.376741829237396e-06,
      "loss": 0.0888,
      "step": 14650
    },
    {
      "epoch": 2.681503101058008,
      "grad_norm": 47474.50390625,
      "learning_rate": 7.326070433240436e-06,
      "loss": 0.1255,
      "step": 14700
    },
    {
      "epoch": 2.690623859905144,
      "grad_norm": 1970422.5,
      "learning_rate": 7.275399037243477e-06,
      "loss": 0.1303,
      "step": 14750
    },
    {
      "epoch": 2.6997446187522804,
      "grad_norm": 12816.1962890625,
      "learning_rate": 7.224727641246517e-06,
      "loss": 0.0986,
      "step": 14800
    },
    {
      "epoch": 2.708865377599416,
      "grad_norm": 11599.1953125,
      "learning_rate": 7.174056245249558e-06,
      "loss": 0.0904,
      "step": 14850
    },
    {
      "epoch": 2.7179861364465525,
      "grad_norm": 1421936.0,
      "learning_rate": 7.123384849252597e-06,
      "loss": 0.1277,
      "step": 14900
    },
    {
      "epoch": 2.7271068952936885,
      "grad_norm": 357933.8125,
      "learning_rate": 7.0727134532556376e-06,
      "loss": 0.139,
      "step": 14950
    },
    {
      "epoch": 2.7362276541408246,
      "grad_norm": 2287124.25,
      "learning_rate": 7.022042057258678e-06,
      "loss": 0.1035,
      "step": 15000
    },
    {
      "epoch": 2.7453484129879606,
      "grad_norm": 289179.34375,
      "learning_rate": 6.971370661261718e-06,
      "loss": 0.1376,
      "step": 15050
    },
    {
      "epoch": 2.7544691718350967,
      "grad_norm": 29445.27734375,
      "learning_rate": 6.920699265264759e-06,
      "loss": 0.07,
      "step": 15100
    },
    {
      "epoch": 2.7635899306822327,
      "grad_norm": 551219.25,
      "learning_rate": 6.870027869267799e-06,
      "loss": 0.1334,
      "step": 15150
    },
    {
      "epoch": 2.7727106895293687,
      "grad_norm": 662608.8125,
      "learning_rate": 6.81935647327084e-06,
      "loss": 0.0984,
      "step": 15200
    },
    {
      "epoch": 2.781831448376505,
      "grad_norm": 529108.75,
      "learning_rate": 6.76868507727388e-06,
      "loss": 0.1354,
      "step": 15250
    },
    {
      "epoch": 2.790952207223641,
      "grad_norm": 104224.609375,
      "learning_rate": 6.7180136812769195e-06,
      "loss": 0.1214,
      "step": 15300
    },
    {
      "epoch": 2.8000729660707773,
      "grad_norm": 195596.28125,
      "learning_rate": 6.667342285279959e-06,
      "loss": 0.1553,
      "step": 15350
    },
    {
      "epoch": 2.8091937249179133,
      "grad_norm": 22802.654296875,
      "learning_rate": 6.616670889283e-06,
      "loss": 0.0658,
      "step": 15400
    },
    {
      "epoch": 2.8183144837650493,
      "grad_norm": 4139.34912109375,
      "learning_rate": 6.5659994932860406e-06,
      "loss": 0.0782,
      "step": 15450
    },
    {
      "epoch": 2.8274352426121854,
      "grad_norm": 45684.21875,
      "learning_rate": 6.515328097289081e-06,
      "loss": 0.1125,
      "step": 15500
    },
    {
      "epoch": 2.8365560014593214,
      "grad_norm": 1223668.75,
      "learning_rate": 6.464656701292121e-06,
      "loss": 0.0861,
      "step": 15550
    },
    {
      "epoch": 2.8456767603064574,
      "grad_norm": 383293.34375,
      "learning_rate": 6.413985305295162e-06,
      "loss": 0.1208,
      "step": 15600
    },
    {
      "epoch": 2.8547975191535935,
      "grad_norm": 567774.3125,
      "learning_rate": 6.363313909298202e-06,
      "loss": 0.112,
      "step": 15650
    },
    {
      "epoch": 2.8639182780007295,
      "grad_norm": 1285287.25,
      "learning_rate": 6.312642513301241e-06,
      "loss": 0.09,
      "step": 15700
    },
    {
      "epoch": 2.8730390368478655,
      "grad_norm": 6639.6904296875,
      "learning_rate": 6.261971117304282e-06,
      "loss": 0.13,
      "step": 15750
    },
    {
      "epoch": 2.882159795695002,
      "grad_norm": 215088.921875,
      "learning_rate": 6.2112997213073225e-06,
      "loss": 0.1047,
      "step": 15800
    },
    {
      "epoch": 2.891280554542138,
      "grad_norm": 428886.59375,
      "learning_rate": 6.160628325310363e-06,
      "loss": 0.1207,
      "step": 15850
    },
    {
      "epoch": 2.900401313389274,
      "grad_norm": 13772.1787109375,
      "learning_rate": 6.109956929313403e-06,
      "loss": 0.1194,
      "step": 15900
    },
    {
      "epoch": 2.90952207223641,
      "grad_norm": 61497.54296875,
      "learning_rate": 6.0592855333164436e-06,
      "loss": 0.1387,
      "step": 15950
    },
    {
      "epoch": 2.918642831083546,
      "grad_norm": 939115.8125,
      "learning_rate": 6.008614137319484e-06,
      "loss": 0.0962,
      "step": 16000
    },
    {
      "epoch": 2.927763589930682,
      "grad_norm": 299580.875,
      "learning_rate": 5.957942741322524e-06,
      "loss": 0.1015,
      "step": 16050
    },
    {
      "epoch": 2.9368843487778182,
      "grad_norm": 4868.5166015625,
      "learning_rate": 5.907271345325564e-06,
      "loss": 0.0702,
      "step": 16100
    },
    {
      "epoch": 2.9460051076249543,
      "grad_norm": 6620.400390625,
      "learning_rate": 5.856599949328604e-06,
      "loss": 0.0972,
      "step": 16150
    },
    {
      "epoch": 2.9551258664720903,
      "grad_norm": 29583.2890625,
      "learning_rate": 5.805928553331644e-06,
      "loss": 0.1455,
      "step": 16200
    },
    {
      "epoch": 2.964246625319227,
      "grad_norm": 137434.390625,
      "learning_rate": 5.755257157334685e-06,
      "loss": 0.1209,
      "step": 16250
    },
    {
      "epoch": 2.9733673841663624,
      "grad_norm": 413871.0625,
      "learning_rate": 5.7045857613377255e-06,
      "loss": 0.1305,
      "step": 16300
    },
    {
      "epoch": 2.982488143013499,
      "grad_norm": 355659.0,
      "learning_rate": 5.653914365340766e-06,
      "loss": 0.1505,
      "step": 16350
    },
    {
      "epoch": 2.991608901860635,
      "grad_norm": 763238.1875,
      "learning_rate": 5.603242969343806e-06,
      "loss": 0.1158,
      "step": 16400
    },
    {
      "epoch": 3.000729660707771,
      "grad_norm": 1432607.25,
      "learning_rate": 5.5525715733468465e-06,
      "loss": 0.0921,
      "step": 16450
    },
    {
      "epoch": 3.009850419554907,
      "grad_norm": 6963.62353515625,
      "learning_rate": 5.5019001773498855e-06,
      "loss": 0.1059,
      "step": 16500
    },
    {
      "epoch": 3.018971178402043,
      "grad_norm": 9982.2763671875,
      "learning_rate": 5.451228781352926e-06,
      "loss": 0.0871,
      "step": 16550
    },
    {
      "epoch": 3.028091937249179,
      "grad_norm": 74583.9765625,
      "learning_rate": 5.400557385355967e-06,
      "loss": 0.0575,
      "step": 16600
    },
    {
      "epoch": 3.037212696096315,
      "grad_norm": 8724.47265625,
      "learning_rate": 5.349885989359007e-06,
      "loss": 0.0464,
      "step": 16650
    },
    {
      "epoch": 3.046333454943451,
      "grad_norm": 213774.203125,
      "learning_rate": 5.299214593362047e-06,
      "loss": 0.0771,
      "step": 16700
    },
    {
      "epoch": 3.0554542137905876,
      "grad_norm": 11507.814453125,
      "learning_rate": 5.248543197365088e-06,
      "loss": 0.0619,
      "step": 16750
    },
    {
      "epoch": 3.0645749726377236,
      "grad_norm": 22478.154296875,
      "learning_rate": 5.1978718013681285e-06,
      "loss": 0.0659,
      "step": 16800
    },
    {
      "epoch": 3.0736957314848596,
      "grad_norm": 10481.3984375,
      "learning_rate": 5.147200405371169e-06,
      "loss": 0.0911,
      "step": 16850
    },
    {
      "epoch": 3.0828164903319957,
      "grad_norm": 2965361.0,
      "learning_rate": 5.096529009374208e-06,
      "loss": 0.092,
      "step": 16900
    },
    {
      "epoch": 3.0919372491791317,
      "grad_norm": 20617.66796875,
      "learning_rate": 5.045857613377249e-06,
      "loss": 0.0875,
      "step": 16950
    },
    {
      "epoch": 3.1010580080262677,
      "grad_norm": 31578.705078125,
      "learning_rate": 4.995186217380289e-06,
      "loss": 0.0624,
      "step": 17000
    },
    {
      "epoch": 3.1101787668734038,
      "grad_norm": 2523296.75,
      "learning_rate": 4.944514821383329e-06,
      "loss": 0.081,
      "step": 17050
    },
    {
      "epoch": 3.11929952572054,
      "grad_norm": 1495018.75,
      "learning_rate": 4.89384342538637e-06,
      "loss": 0.1009,
      "step": 17100
    },
    {
      "epoch": 3.128420284567676,
      "grad_norm": 762487.875,
      "learning_rate": 4.84317202938941e-06,
      "loss": 0.0673,
      "step": 17150
    },
    {
      "epoch": 3.137541043414812,
      "grad_norm": 16695.4921875,
      "learning_rate": 4.79250063339245e-06,
      "loss": 0.0774,
      "step": 17200
    },
    {
      "epoch": 3.1466618022619484,
      "grad_norm": 4820.19189453125,
      "learning_rate": 4.741829237395491e-06,
      "loss": 0.0754,
      "step": 17250
    },
    {
      "epoch": 3.1557825611090844,
      "grad_norm": 20747.03515625,
      "learning_rate": 4.691157841398531e-06,
      "loss": 0.0835,
      "step": 17300
    },
    {
      "epoch": 3.1649033199562204,
      "grad_norm": 31002.921875,
      "learning_rate": 4.640486445401571e-06,
      "loss": 0.0782,
      "step": 17350
    },
    {
      "epoch": 3.1740240788033565,
      "grad_norm": 10415.705078125,
      "learning_rate": 4.589815049404611e-06,
      "loss": 0.0804,
      "step": 17400
    },
    {
      "epoch": 3.1831448376504925,
      "grad_norm": 842599.875,
      "learning_rate": 4.539143653407652e-06,
      "loss": 0.131,
      "step": 17450
    },
    {
      "epoch": 3.1922655964976285,
      "grad_norm": 6829.28271484375,
      "learning_rate": 4.488472257410692e-06,
      "loss": 0.0809,
      "step": 17500
    },
    {
      "epoch": 3.2013863553447646,
      "grad_norm": 86739.6015625,
      "learning_rate": 4.437800861413732e-06,
      "loss": 0.1059,
      "step": 17550
    },
    {
      "epoch": 3.2105071141919006,
      "grad_norm": 11209.5244140625,
      "learning_rate": 4.387129465416772e-06,
      "loss": 0.1009,
      "step": 17600
    },
    {
      "epoch": 3.2196278730390366,
      "grad_norm": 1168781.125,
      "learning_rate": 4.3364580694198126e-06,
      "loss": 0.063,
      "step": 17650
    },
    {
      "epoch": 3.228748631886173,
      "grad_norm": 11359.98828125,
      "learning_rate": 4.285786673422853e-06,
      "loss": 0.0655,
      "step": 17700
    },
    {
      "epoch": 3.237869390733309,
      "grad_norm": 338243.8125,
      "learning_rate": 4.235115277425894e-06,
      "loss": 0.1034,
      "step": 17750
    },
    {
      "epoch": 3.246990149580445,
      "grad_norm": 12001.6728515625,
      "learning_rate": 4.184443881428934e-06,
      "loss": 0.0777,
      "step": 17800
    },
    {
      "epoch": 3.256110908427581,
      "grad_norm": 4443.392578125,
      "learning_rate": 4.133772485431974e-06,
      "loss": 0.0537,
      "step": 17850
    },
    {
      "epoch": 3.2652316672747173,
      "grad_norm": 830020.625,
      "learning_rate": 4.083101089435014e-06,
      "loss": 0.0818,
      "step": 17900
    },
    {
      "epoch": 3.2743524261218533,
      "grad_norm": 1289665.625,
      "learning_rate": 4.032429693438055e-06,
      "loss": 0.0714,
      "step": 17950
    },
    {
      "epoch": 3.2834731849689893,
      "grad_norm": 9587.6884765625,
      "learning_rate": 3.981758297441095e-06,
      "loss": 0.0712,
      "step": 18000
    },
    {
      "epoch": 3.2925939438161254,
      "grad_norm": 688803.8125,
      "learning_rate": 3.931086901444135e-06,
      "loss": 0.0944,
      "step": 18050
    },
    {
      "epoch": 3.3017147026632614,
      "grad_norm": 817776.0625,
      "learning_rate": 3.880415505447176e-06,
      "loss": 0.1196,
      "step": 18100
    },
    {
      "epoch": 3.310835461510398,
      "grad_norm": 11997.1015625,
      "learning_rate": 3.8297441094502156e-06,
      "loss": 0.098,
      "step": 18150
    },
    {
      "epoch": 3.319956220357534,
      "grad_norm": 25180.052734375,
      "learning_rate": 3.779072713453256e-06,
      "loss": 0.0509,
      "step": 18200
    },
    {
      "epoch": 3.32907697920467,
      "grad_norm": 13934.978515625,
      "learning_rate": 3.728401317456296e-06,
      "loss": 0.1297,
      "step": 18250
    },
    {
      "epoch": 3.338197738051806,
      "grad_norm": 3704.89306640625,
      "learning_rate": 3.6777299214593366e-06,
      "loss": 0.0779,
      "step": 18300
    },
    {
      "epoch": 3.347318496898942,
      "grad_norm": 8704.6640625,
      "learning_rate": 3.627058525462377e-06,
      "loss": 0.0605,
      "step": 18350
    },
    {
      "epoch": 3.356439255746078,
      "grad_norm": 9870.6572265625,
      "learning_rate": 3.5763871294654175e-06,
      "loss": 0.11,
      "step": 18400
    },
    {
      "epoch": 3.365560014593214,
      "grad_norm": 26413.69140625,
      "learning_rate": 3.5257157334684573e-06,
      "loss": 0.0997,
      "step": 18450
    },
    {
      "epoch": 3.37468077344035,
      "grad_norm": 4674.2578125,
      "learning_rate": 3.4750443374714975e-06,
      "loss": 0.0685,
      "step": 18500
    },
    {
      "epoch": 3.383801532287486,
      "grad_norm": 15074.7626953125,
      "learning_rate": 3.424372941474538e-06,
      "loss": 0.1012,
      "step": 18550
    },
    {
      "epoch": 3.3929222911346226,
      "grad_norm": 4812.59033203125,
      "learning_rate": 3.3737015454775783e-06,
      "loss": 0.1047,
      "step": 18600
    },
    {
      "epoch": 3.4020430499817587,
      "grad_norm": 6313.27685546875,
      "learning_rate": 3.323030149480618e-06,
      "loss": 0.0639,
      "step": 18650
    },
    {
      "epoch": 3.4111638088288947,
      "grad_norm": 10928.2158203125,
      "learning_rate": 3.2723587534836588e-06,
      "loss": 0.069,
      "step": 18700
    },
    {
      "epoch": 3.4202845676760307,
      "grad_norm": 10354.396484375,
      "learning_rate": 3.221687357486699e-06,
      "loss": 0.0893,
      "step": 18750
    },
    {
      "epoch": 3.4294053265231668,
      "grad_norm": 1006292.375,
      "learning_rate": 3.1710159614897396e-06,
      "loss": 0.0627,
      "step": 18800
    },
    {
      "epoch": 3.438526085370303,
      "grad_norm": 469498.0,
      "learning_rate": 3.1203445654927794e-06,
      "loss": 0.0852,
      "step": 18850
    },
    {
      "epoch": 3.447646844217439,
      "grad_norm": 229193.234375,
      "learning_rate": 3.06967316949582e-06,
      "loss": 0.0756,
      "step": 18900
    },
    {
      "epoch": 3.456767603064575,
      "grad_norm": 6372.13818359375,
      "learning_rate": 3.0190017734988603e-06,
      "loss": 0.047,
      "step": 18950
    },
    {
      "epoch": 3.465888361911711,
      "grad_norm": 119368.328125,
      "learning_rate": 2.9683303775019005e-06,
      "loss": 0.0919,
      "step": 19000
    },
    {
      "epoch": 3.4750091207588474,
      "grad_norm": 41952.55859375,
      "learning_rate": 2.9176589815049407e-06,
      "loss": 0.089,
      "step": 19050
    },
    {
      "epoch": 3.484129879605983,
      "grad_norm": 233723.734375,
      "learning_rate": 2.866987585507981e-06,
      "loss": 0.0746,
      "step": 19100
    },
    {
      "epoch": 3.4932506384531194,
      "grad_norm": 690105.0,
      "learning_rate": 2.8163161895110215e-06,
      "loss": 0.081,
      "step": 19150
    },
    {
      "epoch": 3.5023713973002555,
      "grad_norm": 223874.109375,
      "learning_rate": 2.7656447935140618e-06,
      "loss": 0.0652,
      "step": 19200
    },
    {
      "epoch": 3.5114921561473915,
      "grad_norm": 5498.5478515625,
      "learning_rate": 2.7149733975171015e-06,
      "loss": 0.0901,
      "step": 19250
    },
    {
      "epoch": 3.5206129149945276,
      "grad_norm": 11898.388671875,
      "learning_rate": 2.664302001520142e-06,
      "loss": 0.0955,
      "step": 19300
    },
    {
      "epoch": 3.5297336738416636,
      "grad_norm": 233968.875,
      "learning_rate": 2.6136306055231824e-06,
      "loss": 0.0682,
      "step": 19350
    },
    {
      "epoch": 3.5388544326887996,
      "grad_norm": 32915.3828125,
      "learning_rate": 2.562959209526223e-06,
      "loss": 0.0754,
      "step": 19400
    },
    {
      "epoch": 3.5479751915359357,
      "grad_norm": 6850.73681640625,
      "learning_rate": 2.512287813529263e-06,
      "loss": 0.1165,
      "step": 19450
    },
    {
      "epoch": 3.557095950383072,
      "grad_norm": 872040.5625,
      "learning_rate": 2.461616417532303e-06,
      "loss": 0.0825,
      "step": 19500
    },
    {
      "epoch": 3.5662167092302077,
      "grad_norm": 32425.607421875,
      "learning_rate": 2.4109450215353437e-06,
      "loss": 0.1121,
      "step": 19550
    },
    {
      "epoch": 3.575337468077344,
      "grad_norm": 242646.734375,
      "learning_rate": 2.360273625538384e-06,
      "loss": 0.0636,
      "step": 19600
    },
    {
      "epoch": 3.5844582269244802,
      "grad_norm": 13040.8388671875,
      "learning_rate": 2.309602229541424e-06,
      "loss": 0.084,
      "step": 19650
    },
    {
      "epoch": 3.5935789857716163,
      "grad_norm": 674810.875,
      "learning_rate": 2.2589308335444643e-06,
      "loss": 0.0885,
      "step": 19700
    },
    {
      "epoch": 3.6026997446187523,
      "grad_norm": 5984.54296875,
      "learning_rate": 2.2082594375475045e-06,
      "loss": 0.0687,
      "step": 19750
    },
    {
      "epoch": 3.6118205034658883,
      "grad_norm": 14208.8974609375,
      "learning_rate": 2.1575880415505448e-06,
      "loss": 0.0954,
      "step": 19800
    },
    {
      "epoch": 3.6209412623130244,
      "grad_norm": 5175.86474609375,
      "learning_rate": 2.1069166455535854e-06,
      "loss": 0.0837,
      "step": 19850
    },
    {
      "epoch": 3.6300620211601604,
      "grad_norm": 19154.587890625,
      "learning_rate": 2.0562452495566256e-06,
      "loss": 0.0435,
      "step": 19900
    },
    {
      "epoch": 3.6391827800072964,
      "grad_norm": 2882.356201171875,
      "learning_rate": 2.005573853559666e-06,
      "loss": 0.0635,
      "step": 19950
    },
    {
      "epoch": 3.6483035388544325,
      "grad_norm": 12597.7646484375,
      "learning_rate": 1.954902457562706e-06,
      "loss": 0.0647,
      "step": 20000
    },
    {
      "epoch": 3.657424297701569,
      "grad_norm": 14101.2080078125,
      "learning_rate": 1.9042310615657465e-06,
      "loss": 0.0797,
      "step": 20050
    },
    {
      "epoch": 3.666545056548705,
      "grad_norm": 14294.091796875,
      "learning_rate": 1.8535596655687865e-06,
      "loss": 0.0659,
      "step": 20100
    },
    {
      "epoch": 3.675665815395841,
      "grad_norm": 4369.06787109375,
      "learning_rate": 1.802888269571827e-06,
      "loss": 0.0807,
      "step": 20150
    },
    {
      "epoch": 3.684786574242977,
      "grad_norm": 878622.1875,
      "learning_rate": 1.7522168735748671e-06,
      "loss": 0.0662,
      "step": 20200
    },
    {
      "epoch": 3.693907333090113,
      "grad_norm": 896575.4375,
      "learning_rate": 1.7015454775779075e-06,
      "loss": 0.0982,
      "step": 20250
    },
    {
      "epoch": 3.703028091937249,
      "grad_norm": 1492679.375,
      "learning_rate": 1.6508740815809475e-06,
      "loss": 0.0979,
      "step": 20300
    },
    {
      "epoch": 3.712148850784385,
      "grad_norm": 229225.3125,
      "learning_rate": 1.600202685583988e-06,
      "loss": 0.1088,
      "step": 20350
    },
    {
      "epoch": 3.721269609631521,
      "grad_norm": 9857.41015625,
      "learning_rate": 1.5495312895870282e-06,
      "loss": 0.0656,
      "step": 20400
    },
    {
      "epoch": 3.7303903684786572,
      "grad_norm": 2284149.0,
      "learning_rate": 1.4988598935900686e-06,
      "loss": 0.046,
      "step": 20450
    },
    {
      "epoch": 3.7395111273257937,
      "grad_norm": 13494.51171875,
      "learning_rate": 1.4481884975931088e-06,
      "loss": 0.0898,
      "step": 20500
    },
    {
      "epoch": 3.7486318861729298,
      "grad_norm": 5728.21142578125,
      "learning_rate": 1.3975171015961493e-06,
      "loss": 0.0648,
      "step": 20550
    },
    {
      "epoch": 3.757752645020066,
      "grad_norm": 44093.12890625,
      "learning_rate": 1.3468457055991893e-06,
      "loss": 0.0619,
      "step": 20600
    },
    {
      "epoch": 3.766873403867202,
      "grad_norm": 5438.75244140625,
      "learning_rate": 1.2961743096022297e-06,
      "loss": 0.102,
      "step": 20650
    },
    {
      "epoch": 3.775994162714338,
      "grad_norm": 1593590.5,
      "learning_rate": 1.24550291360527e-06,
      "loss": 0.0836,
      "step": 20700
    },
    {
      "epoch": 3.785114921561474,
      "grad_norm": 10673.7509765625,
      "learning_rate": 1.1948315176083101e-06,
      "loss": 0.0916,
      "step": 20750
    },
    {
      "epoch": 3.79423568040861,
      "grad_norm": 8615.5625,
      "learning_rate": 1.1441601216113503e-06,
      "loss": 0.0662,
      "step": 20800
    },
    {
      "epoch": 3.803356439255746,
      "grad_norm": 350823.71875,
      "learning_rate": 1.0934887256143908e-06,
      "loss": 0.0808,
      "step": 20850
    },
    {
      "epoch": 3.812477198102882,
      "grad_norm": 29787.181640625,
      "learning_rate": 1.042817329617431e-06,
      "loss": 0.0506,
      "step": 20900
    },
    {
      "epoch": 3.8215979569500185,
      "grad_norm": 14586.029296875,
      "learning_rate": 9.921459336204712e-07,
      "loss": 0.0679,
      "step": 20950
    },
    {
      "epoch": 3.830718715797154,
      "grad_norm": 258944.625,
      "learning_rate": 9.414745376235115e-07,
      "loss": 0.1024,
      "step": 21000
    },
    {
      "epoch": 3.8398394746442905,
      "grad_norm": 8170.18603515625,
      "learning_rate": 8.908031416265518e-07,
      "loss": 0.0594,
      "step": 21050
    },
    {
      "epoch": 3.8489602334914266,
      "grad_norm": 10326.6640625,
      "learning_rate": 8.401317456295921e-07,
      "loss": 0.0846,
      "step": 21100
    },
    {
      "epoch": 3.8580809923385626,
      "grad_norm": 9379.43359375,
      "learning_rate": 7.894603496326324e-07,
      "loss": 0.0719,
      "step": 21150
    },
    {
      "epoch": 3.8672017511856986,
      "grad_norm": 1031415.25,
      "learning_rate": 7.387889536356727e-07,
      "loss": 0.0961,
      "step": 21200
    },
    {
      "epoch": 3.8763225100328347,
      "grad_norm": 6597.55029296875,
      "learning_rate": 6.881175576387129e-07,
      "loss": 0.0405,
      "step": 21250
    },
    {
      "epoch": 3.8854432688799707,
      "grad_norm": 2434.20068359375,
      "learning_rate": 6.374461616417532e-07,
      "loss": 0.1077,
      "step": 21300
    },
    {
      "epoch": 3.8945640277271067,
      "grad_norm": 14727.1591796875,
      "learning_rate": 5.867747656447935e-07,
      "loss": 0.0668,
      "step": 21350
    },
    {
      "epoch": 3.9036847865742432,
      "grad_norm": 8782.0625,
      "learning_rate": 5.361033696478339e-07,
      "loss": 0.07,
      "step": 21400
    },
    {
      "epoch": 3.912805545421379,
      "grad_norm": 213089.078125,
      "learning_rate": 4.854319736508741e-07,
      "loss": 0.0984,
      "step": 21450
    },
    {
      "epoch": 3.9219263042685153,
      "grad_norm": 238215.078125,
      "learning_rate": 4.347605776539144e-07,
      "loss": 0.087,
      "step": 21500
    },
    {
      "epoch": 3.9310470631156513,
      "grad_norm": 4902.83056640625,
      "learning_rate": 3.8408918165695466e-07,
      "loss": 0.0653,
      "step": 21550
    },
    {
      "epoch": 3.9401678219627874,
      "grad_norm": 28395.7578125,
      "learning_rate": 3.3341778565999493e-07,
      "loss": 0.0776,
      "step": 21600
    },
    {
      "epoch": 3.9492885808099234,
      "grad_norm": 10078.9765625,
      "learning_rate": 2.8274638966303525e-07,
      "loss": 0.0859,
      "step": 21650
    },
    {
      "epoch": 3.9584093396570594,
      "grad_norm": 10546.3544921875,
      "learning_rate": 2.3207499366607552e-07,
      "loss": 0.075,
      "step": 21700
    },
    {
      "epoch": 3.9675300985041955,
      "grad_norm": 12140.904296875,
      "learning_rate": 1.814035976691158e-07,
      "loss": 0.0845,
      "step": 21750
    },
    {
      "epoch": 3.9766508573513315,
      "grad_norm": 49683.48828125,
      "learning_rate": 1.3073220167215606e-07,
      "loss": 0.0758,
      "step": 21800
    },
    {
      "epoch": 3.985771616198468,
      "grad_norm": 2126122.75,
      "learning_rate": 8.006080567519636e-08,
      "loss": 0.061,
      "step": 21850
    },
    {
      "epoch": 3.9948923750456036,
      "grad_norm": 5106.09765625,
      "learning_rate": 2.9389409678236636e-08,
      "loss": 0.0827,
      "step": 21900
    }
  ],
  "logging_steps": 50,
  "max_steps": 21928,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0696803860214374e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
