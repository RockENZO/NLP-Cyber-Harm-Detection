{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 10964,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009120758847136081,
      "grad_norm": 5936224.0,
      "learning_rate": 4.4687642498860014e-07,
      "loss": 37.961,
      "step": 50
    },
    {
      "epoch": 0.018241517694272163,
      "grad_norm": 3609146.0,
      "learning_rate": 9.028727770177839e-07,
      "loss": 34.6913,
      "step": 100
    },
    {
      "epoch": 0.027362276541408246,
      "grad_norm": 3565987.75,
      "learning_rate": 1.3588691290469677e-06,
      "loss": 31.0193,
      "step": 150
    },
    {
      "epoch": 0.036483035388544326,
      "grad_norm": 6580893.0,
      "learning_rate": 1.8148654810761515e-06,
      "loss": 27.2476,
      "step": 200
    },
    {
      "epoch": 0.04560379423568041,
      "grad_norm": 7123704.5,
      "learning_rate": 2.2708618331053354e-06,
      "loss": 20.2083,
      "step": 250
    },
    {
      "epoch": 0.05472455308281649,
      "grad_norm": 7122810.0,
      "learning_rate": 2.7268581851345193e-06,
      "loss": 16.0538,
      "step": 300
    },
    {
      "epoch": 0.06384531192995258,
      "grad_norm": 7168265.0,
      "learning_rate": 3.1828545371637033e-06,
      "loss": 13.6815,
      "step": 350
    },
    {
      "epoch": 0.07296607077708865,
      "grad_norm": 7192185.5,
      "learning_rate": 3.638850889192887e-06,
      "loss": 11.8997,
      "step": 400
    },
    {
      "epoch": 0.08208682962422474,
      "grad_norm": 7115527.0,
      "learning_rate": 4.094847241222071e-06,
      "loss": 10.2057,
      "step": 450
    },
    {
      "epoch": 0.09120758847136082,
      "grad_norm": 6981652.0,
      "learning_rate": 4.550843593251254e-06,
      "loss": 8.6633,
      "step": 500
    },
    {
      "epoch": 0.1003283473184969,
      "grad_norm": 6811356.5,
      "learning_rate": 5.006839945280439e-06,
      "loss": 7.278,
      "step": 550
    },
    {
      "epoch": 0.10944910616563298,
      "grad_norm": 5803515.5,
      "learning_rate": 5.462836297309621e-06,
      "loss": 5.8033,
      "step": 600
    },
    {
      "epoch": 0.11856986501276906,
      "grad_norm": 4586944.0,
      "learning_rate": 5.918832649338806e-06,
      "loss": 4.4876,
      "step": 650
    },
    {
      "epoch": 0.12769062385990515,
      "grad_norm": 2988665.0,
      "learning_rate": 6.374829001367989e-06,
      "loss": 3.4683,
      "step": 700
    },
    {
      "epoch": 0.1368113827070412,
      "grad_norm": 1596873.125,
      "learning_rate": 6.830825353397174e-06,
      "loss": 2.6278,
      "step": 750
    },
    {
      "epoch": 0.1459321415541773,
      "grad_norm": 823695.4375,
      "learning_rate": 7.286821705426357e-06,
      "loss": 2.1076,
      "step": 800
    },
    {
      "epoch": 0.1550529004013134,
      "grad_norm": 687583.375,
      "learning_rate": 7.742818057455541e-06,
      "loss": 1.3217,
      "step": 850
    },
    {
      "epoch": 0.16417365924844948,
      "grad_norm": 387442.53125,
      "learning_rate": 8.198814409484724e-06,
      "loss": 0.8887,
      "step": 900
    },
    {
      "epoch": 0.17329441809558555,
      "grad_norm": 807059.5,
      "learning_rate": 8.654810761513909e-06,
      "loss": 0.7782,
      "step": 950
    },
    {
      "epoch": 0.18241517694272164,
      "grad_norm": 841452.3125,
      "learning_rate": 9.110807113543091e-06,
      "loss": 0.5813,
      "step": 1000
    },
    {
      "epoch": 0.19153593578985773,
      "grad_norm": 442154.78125,
      "learning_rate": 9.566803465572276e-06,
      "loss": 0.5491,
      "step": 1050
    },
    {
      "epoch": 0.2006566946369938,
      "grad_norm": 597384.6875,
      "learning_rate": 1.002279981760146e-05,
      "loss": 0.5095,
      "step": 1100
    },
    {
      "epoch": 0.20977745348412988,
      "grad_norm": 700358.75,
      "learning_rate": 1.0478796169630643e-05,
      "loss": 0.5135,
      "step": 1150
    },
    {
      "epoch": 0.21889821233126597,
      "grad_norm": 762592.875,
      "learning_rate": 1.0934792521659827e-05,
      "loss": 0.4396,
      "step": 1200
    },
    {
      "epoch": 0.22801897117840203,
      "grad_norm": 711147.125,
      "learning_rate": 1.1390788873689011e-05,
      "loss": 0.4072,
      "step": 1250
    },
    {
      "epoch": 0.23713973002553812,
      "grad_norm": 471668.34375,
      "learning_rate": 1.1846785225718196e-05,
      "loss": 0.4217,
      "step": 1300
    },
    {
      "epoch": 0.2462604888726742,
      "grad_norm": 353904.5625,
      "learning_rate": 1.2302781577747378e-05,
      "loss": 0.4368,
      "step": 1350
    },
    {
      "epoch": 0.2553812477198103,
      "grad_norm": 612999.625,
      "learning_rate": 1.2758777929776563e-05,
      "loss": 0.3692,
      "step": 1400
    },
    {
      "epoch": 0.2645020065669464,
      "grad_norm": 400021.6875,
      "learning_rate": 1.3214774281805747e-05,
      "loss": 0.381,
      "step": 1450
    },
    {
      "epoch": 0.2736227654140824,
      "grad_norm": 655072.375,
      "learning_rate": 1.3670770633834932e-05,
      "loss": 0.3679,
      "step": 1500
    },
    {
      "epoch": 0.2827435242612185,
      "grad_norm": 198947.984375,
      "learning_rate": 1.4126766985864113e-05,
      "loss": 0.4183,
      "step": 1550
    },
    {
      "epoch": 0.2918642831083546,
      "grad_norm": 545327.5,
      "learning_rate": 1.4582763337893297e-05,
      "loss": 0.3756,
      "step": 1600
    },
    {
      "epoch": 0.3009850419554907,
      "grad_norm": 476617.8125,
      "learning_rate": 1.5038759689922481e-05,
      "loss": 0.3379,
      "step": 1650
    },
    {
      "epoch": 0.3101058008026268,
      "grad_norm": 638888.375,
      "learning_rate": 1.5494756041951667e-05,
      "loss": 0.3547,
      "step": 1700
    },
    {
      "epoch": 0.3192265596497629,
      "grad_norm": 437538.875,
      "learning_rate": 1.595075239398085e-05,
      "loss": 0.3613,
      "step": 1750
    },
    {
      "epoch": 0.32834731849689897,
      "grad_norm": 565889.4375,
      "learning_rate": 1.6406748746010033e-05,
      "loss": 0.3084,
      "step": 1800
    },
    {
      "epoch": 0.337468077344035,
      "grad_norm": 689763.625,
      "learning_rate": 1.686274509803922e-05,
      "loss": 0.3319,
      "step": 1850
    },
    {
      "epoch": 0.3465888361911711,
      "grad_norm": 659720.75,
      "learning_rate": 1.73187414500684e-05,
      "loss": 0.2794,
      "step": 1900
    },
    {
      "epoch": 0.3557095950383072,
      "grad_norm": 410950.34375,
      "learning_rate": 1.7774737802097584e-05,
      "loss": 0.31,
      "step": 1950
    },
    {
      "epoch": 0.36483035388544327,
      "grad_norm": 377901.875,
      "learning_rate": 1.8230734154126767e-05,
      "loss": 0.2641,
      "step": 2000
    },
    {
      "epoch": 0.37395111273257936,
      "grad_norm": 69726.6484375,
      "learning_rate": 1.8686730506155953e-05,
      "loss": 0.3077,
      "step": 2050
    },
    {
      "epoch": 0.38307187157971545,
      "grad_norm": 512561.8125,
      "learning_rate": 1.9142726858185135e-05,
      "loss": 0.2944,
      "step": 2100
    },
    {
      "epoch": 0.39219263042685154,
      "grad_norm": 537200.125,
      "learning_rate": 1.9598723210214318e-05,
      "loss": 0.2898,
      "step": 2150
    },
    {
      "epoch": 0.4013133892739876,
      "grad_norm": 501728.40625,
      "learning_rate": 1.9993919432480366e-05,
      "loss": 0.2421,
      "step": 2200
    },
    {
      "epoch": 0.41043414812112367,
      "grad_norm": 499459.9375,
      "learning_rate": 1.9943248036483407e-05,
      "loss": 0.2521,
      "step": 2250
    },
    {
      "epoch": 0.41955490696825976,
      "grad_norm": 439026.53125,
      "learning_rate": 1.9892576640486448e-05,
      "loss": 0.2482,
      "step": 2300
    },
    {
      "epoch": 0.42867566581539585,
      "grad_norm": 508896.6875,
      "learning_rate": 1.9841905244489488e-05,
      "loss": 0.2579,
      "step": 2350
    },
    {
      "epoch": 0.43779642466253194,
      "grad_norm": 1120810.125,
      "learning_rate": 1.979123384849253e-05,
      "loss": 0.2382,
      "step": 2400
    },
    {
      "epoch": 0.446917183509668,
      "grad_norm": 468299.5,
      "learning_rate": 1.974056245249557e-05,
      "loss": 0.2531,
      "step": 2450
    },
    {
      "epoch": 0.45603794235680406,
      "grad_norm": 878376.875,
      "learning_rate": 1.968989105649861e-05,
      "loss": 0.2357,
      "step": 2500
    },
    {
      "epoch": 0.46515870120394015,
      "grad_norm": 135160.609375,
      "learning_rate": 1.963921966050165e-05,
      "loss": 0.2343,
      "step": 2550
    },
    {
      "epoch": 0.47427946005107624,
      "grad_norm": 270697.0,
      "learning_rate": 1.9588548264504688e-05,
      "loss": 0.2641,
      "step": 2600
    },
    {
      "epoch": 0.48340021889821233,
      "grad_norm": 697387.4375,
      "learning_rate": 1.953787686850773e-05,
      "loss": 0.2206,
      "step": 2650
    },
    {
      "epoch": 0.4925209777453484,
      "grad_norm": 374661.125,
      "learning_rate": 1.948720547251077e-05,
      "loss": 0.2651,
      "step": 2700
    },
    {
      "epoch": 0.5016417365924845,
      "grad_norm": 370686.71875,
      "learning_rate": 1.943653407651381e-05,
      "loss": 0.2287,
      "step": 2750
    },
    {
      "epoch": 0.5107624954396206,
      "grad_norm": 906469.8125,
      "learning_rate": 1.938586268051685e-05,
      "loss": 0.2224,
      "step": 2800
    },
    {
      "epoch": 0.5198832542867566,
      "grad_norm": 351443.28125,
      "learning_rate": 1.933519128451989e-05,
      "loss": 0.2629,
      "step": 2850
    },
    {
      "epoch": 0.5290040131338928,
      "grad_norm": 925489.625,
      "learning_rate": 1.9284519888522932e-05,
      "loss": 0.2387,
      "step": 2900
    },
    {
      "epoch": 0.5381247719810288,
      "grad_norm": 662768.375,
      "learning_rate": 1.923384849252597e-05,
      "loss": 0.2902,
      "step": 2950
    },
    {
      "epoch": 0.5472455308281649,
      "grad_norm": 27612.37109375,
      "learning_rate": 1.918317709652901e-05,
      "loss": 0.2399,
      "step": 3000
    },
    {
      "epoch": 0.556366289675301,
      "grad_norm": 465642.96875,
      "learning_rate": 1.913250570053205e-05,
      "loss": 0.2917,
      "step": 3050
    },
    {
      "epoch": 0.565487048522437,
      "grad_norm": 110167.90625,
      "learning_rate": 1.908183430453509e-05,
      "loss": 0.2652,
      "step": 3100
    },
    {
      "epoch": 0.5746078073695732,
      "grad_norm": 926643.0625,
      "learning_rate": 1.903116290853813e-05,
      "loss": 0.2123,
      "step": 3150
    },
    {
      "epoch": 0.5837285662167092,
      "grad_norm": 237950.4375,
      "learning_rate": 1.8980491512541172e-05,
      "loss": 0.2494,
      "step": 3200
    },
    {
      "epoch": 0.5928493250638454,
      "grad_norm": 573800.4375,
      "learning_rate": 1.8929820116544213e-05,
      "loss": 0.1816,
      "step": 3250
    },
    {
      "epoch": 0.6019700839109814,
      "grad_norm": 499455.40625,
      "learning_rate": 1.8879148720547254e-05,
      "loss": 0.2127,
      "step": 3300
    },
    {
      "epoch": 0.6110908427581174,
      "grad_norm": 645332.0625,
      "learning_rate": 1.8828477324550294e-05,
      "loss": 0.2139,
      "step": 3350
    },
    {
      "epoch": 0.6202116016052536,
      "grad_norm": 570036.8125,
      "learning_rate": 1.8777805928553335e-05,
      "loss": 0.2495,
      "step": 3400
    },
    {
      "epoch": 0.6293323604523896,
      "grad_norm": 571552.0625,
      "learning_rate": 1.8727134532556372e-05,
      "loss": 0.2361,
      "step": 3450
    },
    {
      "epoch": 0.6384531192995258,
      "grad_norm": 685822.875,
      "learning_rate": 1.8676463136559413e-05,
      "loss": 0.2217,
      "step": 3500
    },
    {
      "epoch": 0.6475738781466618,
      "grad_norm": 450229.59375,
      "learning_rate": 1.8625791740562453e-05,
      "loss": 0.2005,
      "step": 3550
    },
    {
      "epoch": 0.6566946369937979,
      "grad_norm": 291205.15625,
      "learning_rate": 1.8575120344565494e-05,
      "loss": 0.1855,
      "step": 3600
    },
    {
      "epoch": 0.665815395840934,
      "grad_norm": 568229.8125,
      "learning_rate": 1.8524448948568535e-05,
      "loss": 0.1757,
      "step": 3650
    },
    {
      "epoch": 0.67493615468807,
      "grad_norm": 418715.6875,
      "learning_rate": 1.8473777552571575e-05,
      "loss": 0.2017,
      "step": 3700
    },
    {
      "epoch": 0.6840569135352061,
      "grad_norm": 127682.7265625,
      "learning_rate": 1.8423106156574616e-05,
      "loss": 0.186,
      "step": 3750
    },
    {
      "epoch": 0.6931776723823422,
      "grad_norm": 795673.5,
      "learning_rate": 1.8372434760577657e-05,
      "loss": 0.2412,
      "step": 3800
    },
    {
      "epoch": 0.7022984312294783,
      "grad_norm": 65337.98828125,
      "learning_rate": 1.8321763364580697e-05,
      "loss": 0.2104,
      "step": 3850
    },
    {
      "epoch": 0.7114191900766144,
      "grad_norm": 618753.0625,
      "learning_rate": 1.8271091968583738e-05,
      "loss": 0.2262,
      "step": 3900
    },
    {
      "epoch": 0.7205399489237505,
      "grad_norm": 297139.34375,
      "learning_rate": 1.8220420572586775e-05,
      "loss": 0.2023,
      "step": 3950
    },
    {
      "epoch": 0.7296607077708865,
      "grad_norm": 860036.0625,
      "learning_rate": 1.8169749176589816e-05,
      "loss": 0.2153,
      "step": 4000
    },
    {
      "epoch": 0.7387814666180226,
      "grad_norm": 385338.96875,
      "learning_rate": 1.8119077780592856e-05,
      "loss": 0.202,
      "step": 4050
    },
    {
      "epoch": 0.7479022254651587,
      "grad_norm": 175294.28125,
      "learning_rate": 1.8068406384595897e-05,
      "loss": 0.1806,
      "step": 4100
    },
    {
      "epoch": 0.7570229843122948,
      "grad_norm": 92467.28125,
      "learning_rate": 1.8017734988598938e-05,
      "loss": 0.1844,
      "step": 4150
    },
    {
      "epoch": 0.7661437431594309,
      "grad_norm": 462566.28125,
      "learning_rate": 1.7967063592601978e-05,
      "loss": 0.1634,
      "step": 4200
    },
    {
      "epoch": 0.7752645020065669,
      "grad_norm": 131794.96875,
      "learning_rate": 1.7916392196605015e-05,
      "loss": 0.2244,
      "step": 4250
    },
    {
      "epoch": 0.7843852608537031,
      "grad_norm": 96948.9453125,
      "learning_rate": 1.7865720800608056e-05,
      "loss": 0.2306,
      "step": 4300
    },
    {
      "epoch": 0.7935060197008391,
      "grad_norm": 149633.703125,
      "learning_rate": 1.7815049404611097e-05,
      "loss": 0.1741,
      "step": 4350
    },
    {
      "epoch": 0.8026267785479752,
      "grad_norm": 111322.9765625,
      "learning_rate": 1.7764378008614137e-05,
      "loss": 0.222,
      "step": 4400
    },
    {
      "epoch": 0.8117475373951113,
      "grad_norm": 729147.625,
      "learning_rate": 1.7713706612617178e-05,
      "loss": 0.2101,
      "step": 4450
    },
    {
      "epoch": 0.8208682962422473,
      "grad_norm": 703945.0,
      "learning_rate": 1.766303521662022e-05,
      "loss": 0.1854,
      "step": 4500
    },
    {
      "epoch": 0.8299890550893835,
      "grad_norm": 168290.296875,
      "learning_rate": 1.761236382062326e-05,
      "loss": 0.1955,
      "step": 4550
    },
    {
      "epoch": 0.8391098139365195,
      "grad_norm": 283606.25,
      "learning_rate": 1.75616924246263e-05,
      "loss": 0.2009,
      "step": 4600
    },
    {
      "epoch": 0.8482305727836555,
      "grad_norm": 268640.03125,
      "learning_rate": 1.751102102862934e-05,
      "loss": 0.1936,
      "step": 4650
    },
    {
      "epoch": 0.8573513316307917,
      "grad_norm": 524306.125,
      "learning_rate": 1.746034963263238e-05,
      "loss": 0.1827,
      "step": 4700
    },
    {
      "epoch": 0.8664720904779277,
      "grad_norm": 254491.09375,
      "learning_rate": 1.7409678236635422e-05,
      "loss": 0.2057,
      "step": 4750
    },
    {
      "epoch": 0.8755928493250639,
      "grad_norm": 924322.5625,
      "learning_rate": 1.7359006840638462e-05,
      "loss": 0.1701,
      "step": 4800
    },
    {
      "epoch": 0.8847136081721999,
      "grad_norm": 167627.0625,
      "learning_rate": 1.7308335444641503e-05,
      "loss": 0.2277,
      "step": 4850
    },
    {
      "epoch": 0.893834367019336,
      "grad_norm": 96051.21875,
      "learning_rate": 1.7257664048644544e-05,
      "loss": 0.1808,
      "step": 4900
    },
    {
      "epoch": 0.9029551258664721,
      "grad_norm": 991511.0,
      "learning_rate": 1.7206992652647584e-05,
      "loss": 0.2206,
      "step": 4950
    },
    {
      "epoch": 0.9120758847136081,
      "grad_norm": 285384.96875,
      "learning_rate": 1.715632125665062e-05,
      "loss": 0.2018,
      "step": 5000
    },
    {
      "epoch": 0.9211966435607443,
      "grad_norm": 22526.392578125,
      "learning_rate": 1.7105649860653662e-05,
      "loss": 0.1588,
      "step": 5050
    },
    {
      "epoch": 0.9303174024078803,
      "grad_norm": 1007282.75,
      "learning_rate": 1.7054978464656703e-05,
      "loss": 0.2094,
      "step": 5100
    },
    {
      "epoch": 0.9394381612550164,
      "grad_norm": 586393.3125,
      "learning_rate": 1.7004307068659744e-05,
      "loss": 0.2068,
      "step": 5150
    },
    {
      "epoch": 0.9485589201021525,
      "grad_norm": 523450.8125,
      "learning_rate": 1.6953635672662784e-05,
      "loss": 0.2073,
      "step": 5200
    },
    {
      "epoch": 0.9576796789492886,
      "grad_norm": 929890.1875,
      "learning_rate": 1.690296427666582e-05,
      "loss": 0.1751,
      "step": 5250
    },
    {
      "epoch": 0.9668004377964247,
      "grad_norm": 1103105.25,
      "learning_rate": 1.6852292880668862e-05,
      "loss": 0.1675,
      "step": 5300
    },
    {
      "epoch": 0.9759211966435607,
      "grad_norm": 198764.5625,
      "learning_rate": 1.6801621484671903e-05,
      "loss": 0.1951,
      "step": 5350
    },
    {
      "epoch": 0.9850419554906968,
      "grad_norm": 31200.3125,
      "learning_rate": 1.6750950088674943e-05,
      "loss": 0.216,
      "step": 5400
    },
    {
      "epoch": 0.9941627143378329,
      "grad_norm": 561323.5625,
      "learning_rate": 1.6700278692677984e-05,
      "loss": 0.1999,
      "step": 5450
    },
    {
      "epoch": 1.003283473184969,
      "grad_norm": 757494.1875,
      "learning_rate": 1.6649607296681025e-05,
      "loss": 0.159,
      "step": 5500
    },
    {
      "epoch": 1.0124042320321052,
      "grad_norm": 590224.625,
      "learning_rate": 1.6598935900684065e-05,
      "loss": 0.1656,
      "step": 5550
    },
    {
      "epoch": 1.0215249908792412,
      "grad_norm": 119205.25,
      "learning_rate": 1.6548264504687106e-05,
      "loss": 0.1813,
      "step": 5600
    },
    {
      "epoch": 1.0306457497263772,
      "grad_norm": 98343.6953125,
      "learning_rate": 1.6497593108690147e-05,
      "loss": 0.1775,
      "step": 5650
    },
    {
      "epoch": 1.0397665085735133,
      "grad_norm": 437897.34375,
      "learning_rate": 1.6446921712693187e-05,
      "loss": 0.1385,
      "step": 5700
    },
    {
      "epoch": 1.0488872674206493,
      "grad_norm": 41403.91015625,
      "learning_rate": 1.6396250316696228e-05,
      "loss": 0.1422,
      "step": 5750
    },
    {
      "epoch": 1.0580080262677856,
      "grad_norm": 416996.53125,
      "learning_rate": 1.634557892069927e-05,
      "loss": 0.2186,
      "step": 5800
    },
    {
      "epoch": 1.0671287851149216,
      "grad_norm": 524399.5625,
      "learning_rate": 1.6294907524702306e-05,
      "loss": 0.1505,
      "step": 5850
    },
    {
      "epoch": 1.0762495439620576,
      "grad_norm": 465325.0625,
      "learning_rate": 1.6244236128705346e-05,
      "loss": 0.1445,
      "step": 5900
    },
    {
      "epoch": 1.0853703028091937,
      "grad_norm": 75256.359375,
      "learning_rate": 1.6193564732708387e-05,
      "loss": 0.1292,
      "step": 5950
    },
    {
      "epoch": 1.09449106165633,
      "grad_norm": 337780.5,
      "learning_rate": 1.6142893336711428e-05,
      "loss": 0.1295,
      "step": 6000
    },
    {
      "epoch": 1.103611820503466,
      "grad_norm": 947174.5,
      "learning_rate": 1.6092221940714468e-05,
      "loss": 0.1642,
      "step": 6050
    },
    {
      "epoch": 1.112732579350602,
      "grad_norm": 800064.6875,
      "learning_rate": 1.604155054471751e-05,
      "loss": 0.1734,
      "step": 6100
    },
    {
      "epoch": 1.121853338197738,
      "grad_norm": 478807.5,
      "learning_rate": 1.599087914872055e-05,
      "loss": 0.1573,
      "step": 6150
    },
    {
      "epoch": 1.130974097044874,
      "grad_norm": 288940.875,
      "learning_rate": 1.594020775272359e-05,
      "loss": 0.1948,
      "step": 6200
    },
    {
      "epoch": 1.14009485589201,
      "grad_norm": 28141.294921875,
      "learning_rate": 1.588953635672663e-05,
      "loss": 0.1509,
      "step": 6250
    },
    {
      "epoch": 1.1492156147391464,
      "grad_norm": 1682346.125,
      "learning_rate": 1.5838864960729668e-05,
      "loss": 0.1471,
      "step": 6300
    },
    {
      "epoch": 1.1583363735862824,
      "grad_norm": 1039749.25,
      "learning_rate": 1.578819356473271e-05,
      "loss": 0.2062,
      "step": 6350
    },
    {
      "epoch": 1.1674571324334184,
      "grad_norm": 260467.796875,
      "learning_rate": 1.573752216873575e-05,
      "loss": 0.1461,
      "step": 6400
    },
    {
      "epoch": 1.1765778912805545,
      "grad_norm": 1509705.25,
      "learning_rate": 1.568685077273879e-05,
      "loss": 0.167,
      "step": 6450
    },
    {
      "epoch": 1.1856986501276907,
      "grad_norm": 82328.40625,
      "learning_rate": 1.563617937674183e-05,
      "loss": 0.1225,
      "step": 6500
    },
    {
      "epoch": 1.1948194089748267,
      "grad_norm": 19242.591796875,
      "learning_rate": 1.558550798074487e-05,
      "loss": 0.1785,
      "step": 6550
    },
    {
      "epoch": 1.2039401678219628,
      "grad_norm": 78780.625,
      "learning_rate": 1.5534836584747912e-05,
      "loss": 0.1669,
      "step": 6600
    },
    {
      "epoch": 1.2130609266690988,
      "grad_norm": 5044.451171875,
      "learning_rate": 1.548416518875095e-05,
      "loss": 0.166,
      "step": 6650
    },
    {
      "epoch": 1.2221816855162349,
      "grad_norm": 22234.224609375,
      "learning_rate": 1.543349379275399e-05,
      "loss": 0.1588,
      "step": 6700
    },
    {
      "epoch": 1.231302444363371,
      "grad_norm": 280961.125,
      "learning_rate": 1.538282239675703e-05,
      "loss": 0.1424,
      "step": 6750
    },
    {
      "epoch": 1.2404232032105071,
      "grad_norm": 549833.8125,
      "learning_rate": 1.533215100076007e-05,
      "loss": 0.167,
      "step": 6800
    },
    {
      "epoch": 1.2495439620576432,
      "grad_norm": 23013.283203125,
      "learning_rate": 1.5281479604763112e-05,
      "loss": 0.151,
      "step": 6850
    },
    {
      "epoch": 1.2586647209047792,
      "grad_norm": 573590.625,
      "learning_rate": 1.5230808208766152e-05,
      "loss": 0.159,
      "step": 6900
    },
    {
      "epoch": 1.2677854797519155,
      "grad_norm": 526045.125,
      "learning_rate": 1.5180136812769193e-05,
      "loss": 0.1913,
      "step": 6950
    },
    {
      "epoch": 1.2769062385990515,
      "grad_norm": 945480.25,
      "learning_rate": 1.5129465416772234e-05,
      "loss": 0.1628,
      "step": 7000
    },
    {
      "epoch": 1.2860269974461875,
      "grad_norm": 671979.6875,
      "learning_rate": 1.5078794020775274e-05,
      "loss": 0.2049,
      "step": 7050
    },
    {
      "epoch": 1.2951477562933236,
      "grad_norm": 473662.40625,
      "learning_rate": 1.5028122624778313e-05,
      "loss": 0.1791,
      "step": 7100
    },
    {
      "epoch": 1.3042685151404596,
      "grad_norm": 824720.25,
      "learning_rate": 1.4977451228781354e-05,
      "loss": 0.128,
      "step": 7150
    },
    {
      "epoch": 1.3133892739875956,
      "grad_norm": 133404.390625,
      "learning_rate": 1.4926779832784395e-05,
      "loss": 0.1659,
      "step": 7200
    },
    {
      "epoch": 1.322510032834732,
      "grad_norm": 439313.0,
      "learning_rate": 1.4876108436787435e-05,
      "loss": 0.1822,
      "step": 7250
    },
    {
      "epoch": 1.331630791681868,
      "grad_norm": 13203.1181640625,
      "learning_rate": 1.4825437040790476e-05,
      "loss": 0.1593,
      "step": 7300
    },
    {
      "epoch": 1.340751550529004,
      "grad_norm": 280689.34375,
      "learning_rate": 1.4774765644793516e-05,
      "loss": 0.1432,
      "step": 7350
    },
    {
      "epoch": 1.3498723093761402,
      "grad_norm": 1033610.0625,
      "learning_rate": 1.4724094248796557e-05,
      "loss": 0.1918,
      "step": 7400
    },
    {
      "epoch": 1.3589930682232763,
      "grad_norm": 495282.96875,
      "learning_rate": 1.4673422852799594e-05,
      "loss": 0.1778,
      "step": 7450
    },
    {
      "epoch": 1.3681138270704123,
      "grad_norm": 353377.9375,
      "learning_rate": 1.4622751456802635e-05,
      "loss": 0.1821,
      "step": 7500
    },
    {
      "epoch": 1.3772345859175483,
      "grad_norm": 32913.53125,
      "learning_rate": 1.4572080060805676e-05,
      "loss": 0.1832,
      "step": 7550
    },
    {
      "epoch": 1.3863553447646844,
      "grad_norm": 132365.265625,
      "learning_rate": 1.4521408664808716e-05,
      "loss": 0.1665,
      "step": 7600
    },
    {
      "epoch": 1.3954761036118204,
      "grad_norm": 82107.2421875,
      "learning_rate": 1.4470737268811757e-05,
      "loss": 0.167,
      "step": 7650
    },
    {
      "epoch": 1.4045968624589567,
      "grad_norm": 15999.71875,
      "learning_rate": 1.4420065872814798e-05,
      "loss": 0.1441,
      "step": 7700
    },
    {
      "epoch": 1.4137176213060927,
      "grad_norm": 901016.9375,
      "learning_rate": 1.4369394476817836e-05,
      "loss": 0.1701,
      "step": 7750
    },
    {
      "epoch": 1.4228383801532287,
      "grad_norm": 138710.6875,
      "learning_rate": 1.4318723080820877e-05,
      "loss": 0.1634,
      "step": 7800
    },
    {
      "epoch": 1.4319591390003648,
      "grad_norm": 520854.4375,
      "learning_rate": 1.4268051684823918e-05,
      "loss": 0.1893,
      "step": 7850
    },
    {
      "epoch": 1.441079897847501,
      "grad_norm": 173379.4375,
      "learning_rate": 1.4217380288826958e-05,
      "loss": 0.1396,
      "step": 7900
    },
    {
      "epoch": 1.450200656694637,
      "grad_norm": 62904.48046875,
      "learning_rate": 1.4166708892829999e-05,
      "loss": 0.1375,
      "step": 7950
    },
    {
      "epoch": 1.459321415541773,
      "grad_norm": 1227663.25,
      "learning_rate": 1.411603749683304e-05,
      "loss": 0.2117,
      "step": 8000
    },
    {
      "epoch": 1.4684421743889091,
      "grad_norm": 1724358.375,
      "learning_rate": 1.406536610083608e-05,
      "loss": 0.1412,
      "step": 8050
    },
    {
      "epoch": 1.4775629332360452,
      "grad_norm": 545782.875,
      "learning_rate": 1.4014694704839121e-05,
      "loss": 0.1394,
      "step": 8100
    },
    {
      "epoch": 1.4866836920831812,
      "grad_norm": 20171.197265625,
      "learning_rate": 1.396402330884216e-05,
      "loss": 0.1936,
      "step": 8150
    },
    {
      "epoch": 1.4958044509303174,
      "grad_norm": 364950.53125,
      "learning_rate": 1.39133519128452e-05,
      "loss": 0.1783,
      "step": 8200
    },
    {
      "epoch": 1.5049252097774535,
      "grad_norm": 7368.8564453125,
      "learning_rate": 1.386268051684824e-05,
      "loss": 0.1576,
      "step": 8250
    },
    {
      "epoch": 1.5140459686245895,
      "grad_norm": 55676.6640625,
      "learning_rate": 1.381200912085128e-05,
      "loss": 0.162,
      "step": 8300
    },
    {
      "epoch": 1.5231667274717258,
      "grad_norm": 9863.220703125,
      "learning_rate": 1.376133772485432e-05,
      "loss": 0.1343,
      "step": 8350
    },
    {
      "epoch": 1.5322874863188618,
      "grad_norm": 15558.63671875,
      "learning_rate": 1.371066632885736e-05,
      "loss": 0.1452,
      "step": 8400
    },
    {
      "epoch": 1.5414082451659978,
      "grad_norm": 216820.484375,
      "learning_rate": 1.36599949328604e-05,
      "loss": 0.1496,
      "step": 8450
    },
    {
      "epoch": 1.5505290040131339,
      "grad_norm": 865911.5625,
      "learning_rate": 1.3609323536863441e-05,
      "loss": 0.158,
      "step": 8500
    },
    {
      "epoch": 1.55964976286027,
      "grad_norm": 949935.875,
      "learning_rate": 1.3558652140866482e-05,
      "loss": 0.124,
      "step": 8550
    },
    {
      "epoch": 1.568770521707406,
      "grad_norm": 17069.2265625,
      "learning_rate": 1.3507980744869522e-05,
      "loss": 0.1756,
      "step": 8600
    },
    {
      "epoch": 1.577891280554542,
      "grad_norm": 8642.16796875,
      "learning_rate": 1.3457309348872563e-05,
      "loss": 0.1726,
      "step": 8650
    },
    {
      "epoch": 1.5870120394016782,
      "grad_norm": 17426.0703125,
      "learning_rate": 1.3406637952875603e-05,
      "loss": 0.1555,
      "step": 8700
    },
    {
      "epoch": 1.5961327982488143,
      "grad_norm": 1078521.75,
      "learning_rate": 1.3355966556878644e-05,
      "loss": 0.1452,
      "step": 8750
    },
    {
      "epoch": 1.6052535570959505,
      "grad_norm": 280474.6875,
      "learning_rate": 1.3305295160881683e-05,
      "loss": 0.1912,
      "step": 8800
    },
    {
      "epoch": 1.6143743159430866,
      "grad_norm": 837893.5625,
      "learning_rate": 1.3254623764884724e-05,
      "loss": 0.1308,
      "step": 8850
    },
    {
      "epoch": 1.6234950747902226,
      "grad_norm": 288069.34375,
      "learning_rate": 1.3203952368887764e-05,
      "loss": 0.1662,
      "step": 8900
    },
    {
      "epoch": 1.6326158336373586,
      "grad_norm": 235165.078125,
      "learning_rate": 1.3153280972890805e-05,
      "loss": 0.1406,
      "step": 8950
    },
    {
      "epoch": 1.6417365924844947,
      "grad_norm": 9408.2802734375,
      "learning_rate": 1.3102609576893846e-05,
      "loss": 0.1805,
      "step": 9000
    },
    {
      "epoch": 1.6508573513316307,
      "grad_norm": 814492.625,
      "learning_rate": 1.3051938180896883e-05,
      "loss": 0.134,
      "step": 9050
    },
    {
      "epoch": 1.6599781101787667,
      "grad_norm": 843453.4375,
      "learning_rate": 1.3001266784899924e-05,
      "loss": 0.1474,
      "step": 9100
    },
    {
      "epoch": 1.669098869025903,
      "grad_norm": 8126.31005859375,
      "learning_rate": 1.2950595388902964e-05,
      "loss": 0.1493,
      "step": 9150
    },
    {
      "epoch": 1.678219627873039,
      "grad_norm": 720370.3125,
      "learning_rate": 1.2899923992906005e-05,
      "loss": 0.1354,
      "step": 9200
    },
    {
      "epoch": 1.6873403867201753,
      "grad_norm": 412899.96875,
      "learning_rate": 1.2849252596909045e-05,
      "loss": 0.1669,
      "step": 9250
    },
    {
      "epoch": 1.6964611455673113,
      "grad_norm": 530867.625,
      "learning_rate": 1.2798581200912086e-05,
      "loss": 0.1028,
      "step": 9300
    },
    {
      "epoch": 1.7055819044144473,
      "grad_norm": 9509.8017578125,
      "learning_rate": 1.2747909804915127e-05,
      "loss": 0.1213,
      "step": 9350
    },
    {
      "epoch": 1.7147026632615834,
      "grad_norm": 117835.7265625,
      "learning_rate": 1.2697238408918167e-05,
      "loss": 0.157,
      "step": 9400
    },
    {
      "epoch": 1.7238234221087194,
      "grad_norm": 9566.6044921875,
      "learning_rate": 1.2646567012921206e-05,
      "loss": 0.1434,
      "step": 9450
    },
    {
      "epoch": 1.7329441809558555,
      "grad_norm": 122376.7578125,
      "learning_rate": 1.2595895616924247e-05,
      "loss": 0.1489,
      "step": 9500
    },
    {
      "epoch": 1.7420649398029915,
      "grad_norm": 12444.4609375,
      "learning_rate": 1.2545224220927288e-05,
      "loss": 0.1827,
      "step": 9550
    },
    {
      "epoch": 1.7511856986501277,
      "grad_norm": 209636.859375,
      "learning_rate": 1.2494552824930328e-05,
      "loss": 0.1334,
      "step": 9600
    },
    {
      "epoch": 1.7603064574972638,
      "grad_norm": 138764.125,
      "learning_rate": 1.2443881428933369e-05,
      "loss": 0.176,
      "step": 9650
    },
    {
      "epoch": 1.7694272163443998,
      "grad_norm": 1398038.125,
      "learning_rate": 1.239321003293641e-05,
      "loss": 0.1199,
      "step": 9700
    },
    {
      "epoch": 1.778547975191536,
      "grad_norm": 4111.6298828125,
      "learning_rate": 1.234253863693945e-05,
      "loss": 0.1465,
      "step": 9750
    },
    {
      "epoch": 1.787668734038672,
      "grad_norm": 672973.0625,
      "learning_rate": 1.229186724094249e-05,
      "loss": 0.1564,
      "step": 9800
    },
    {
      "epoch": 1.7967894928858081,
      "grad_norm": 875977.3125,
      "learning_rate": 1.2241195844945528e-05,
      "loss": 0.1393,
      "step": 9850
    },
    {
      "epoch": 1.8059102517329442,
      "grad_norm": 508638.21875,
      "learning_rate": 1.2190524448948569e-05,
      "loss": 0.1443,
      "step": 9900
    },
    {
      "epoch": 1.8150310105800802,
      "grad_norm": 311908.78125,
      "learning_rate": 1.213985305295161e-05,
      "loss": 0.1519,
      "step": 9950
    },
    {
      "epoch": 1.8241517694272162,
      "grad_norm": 31275.94921875,
      "learning_rate": 1.208918165695465e-05,
      "loss": 0.1562,
      "step": 10000
    },
    {
      "epoch": 1.8332725282743523,
      "grad_norm": 576478.3125,
      "learning_rate": 1.203851026095769e-05,
      "loss": 0.0994,
      "step": 10050
    },
    {
      "epoch": 1.8423932871214885,
      "grad_norm": 72607.15625,
      "learning_rate": 1.198783886496073e-05,
      "loss": 0.1282,
      "step": 10100
    },
    {
      "epoch": 1.8515140459686246,
      "grad_norm": 9223.2470703125,
      "learning_rate": 1.193716746896377e-05,
      "loss": 0.1114,
      "step": 10150
    },
    {
      "epoch": 1.8606348048157608,
      "grad_norm": 942542.6875,
      "learning_rate": 1.188649607296681e-05,
      "loss": 0.1546,
      "step": 10200
    },
    {
      "epoch": 1.8697555636628969,
      "grad_norm": 509781.625,
      "learning_rate": 1.1835824676969851e-05,
      "loss": 0.1712,
      "step": 10250
    },
    {
      "epoch": 1.878876322510033,
      "grad_norm": 24648.212890625,
      "learning_rate": 1.1785153280972892e-05,
      "loss": 0.1773,
      "step": 10300
    },
    {
      "epoch": 1.887997081357169,
      "grad_norm": 345658.4375,
      "learning_rate": 1.1734481884975933e-05,
      "loss": 0.1519,
      "step": 10350
    },
    {
      "epoch": 1.897117840204305,
      "grad_norm": 389879.65625,
      "learning_rate": 1.1683810488978973e-05,
      "loss": 0.1342,
      "step": 10400
    },
    {
      "epoch": 1.906238599051441,
      "grad_norm": 26194.060546875,
      "learning_rate": 1.1633139092982014e-05,
      "loss": 0.1263,
      "step": 10450
    },
    {
      "epoch": 1.915359357898577,
      "grad_norm": 475314.625,
      "learning_rate": 1.1582467696985053e-05,
      "loss": 0.152,
      "step": 10500
    },
    {
      "epoch": 1.9244801167457133,
      "grad_norm": 377996.40625,
      "learning_rate": 1.1531796300988094e-05,
      "loss": 0.1471,
      "step": 10550
    },
    {
      "epoch": 1.9336008755928493,
      "grad_norm": 193409.953125,
      "learning_rate": 1.1481124904991134e-05,
      "loss": 0.1287,
      "step": 10600
    },
    {
      "epoch": 1.9427216344399854,
      "grad_norm": 151753.75,
      "learning_rate": 1.1430453508994175e-05,
      "loss": 0.1657,
      "step": 10650
    },
    {
      "epoch": 1.9518423932871216,
      "grad_norm": 88547.9296875,
      "learning_rate": 1.1379782112997214e-05,
      "loss": 0.1398,
      "step": 10700
    },
    {
      "epoch": 1.9609631521342576,
      "grad_norm": 533953.75,
      "learning_rate": 1.1329110717000253e-05,
      "loss": 0.1369,
      "step": 10750
    },
    {
      "epoch": 1.9700839109813937,
      "grad_norm": 280480.03125,
      "learning_rate": 1.1278439321003293e-05,
      "loss": 0.1371,
      "step": 10800
    },
    {
      "epoch": 1.9792046698285297,
      "grad_norm": 1142595.625,
      "learning_rate": 1.1227767925006334e-05,
      "loss": 0.156,
      "step": 10850
    },
    {
      "epoch": 1.9883254286756658,
      "grad_norm": 63976.15234375,
      "learning_rate": 1.1177096529009375e-05,
      "loss": 0.1144,
      "step": 10900
    },
    {
      "epoch": 1.9974461875228018,
      "grad_norm": 315432.125,
      "learning_rate": 1.1126425133012415e-05,
      "loss": 0.177,
      "step": 10950
    }
  ],
  "logging_steps": 50,
  "max_steps": 21928,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.348401930107187e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
