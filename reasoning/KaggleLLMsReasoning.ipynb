{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Enhanced LLM Reasoning Comparison with Advanced Models & Visualizations\n",
    "\n",
    "## üéØ What's New in This Enhanced Version:\n",
    "\n",
    "### ü§ñ **Expanded Model Coverage**\n",
    "- **12+ Advanced Models** including DeepSeek, Qwen, GPT-Neo, TinyLlama\n",
    "- **Multiple Categories**: Small/Fast, Medium, Large, Specialized, Open Source\n",
    "- **Performance-Optimized**: Models prioritized by expected performance\n",
    "- **Resource-Aware**: Clear indicators for GPU/memory requirements\n",
    "\n",
    "### üìä **Comprehensive Visualizations**\n",
    "- **Performance Bar Charts**: Quality and speed comparison\n",
    "- **Radar Charts**: Multi-dimensional quality analysis\n",
    "- **Speed vs Quality Scatter**: Find optimal balance points\n",
    "- **Size vs Performance**: Parameter efficiency analysis\n",
    "- **Category Heatmaps**: Detailed breakdown by fraud indicators\n",
    "- **Interactive Dashboard**: Plotly-powered exploration\n",
    "\n",
    "### üî¨ **Advanced Quality Assessment**\n",
    "- **8 Quality Categories** with 97+ individual indicators\n",
    "- **Real Dataset Integration** using your CSV fraud data\n",
    "- **Fraud-Type Specific Scoring** for relevance assessment\n",
    "- **Coherence & Relevance Metrics** for explanation quality\n",
    "\n",
    "### üí° **Smart Recommendations**\n",
    "- **Use-Case Specific**: Production, Research, Speed-Critical, Quality-First\n",
    "- **Resource Considerations**: GPU memory and parameter requirements\n",
    "- **Open Source Options**: For budget-conscious or research use\n",
    "- **Deployment Guidance**: Integration recommendations\n",
    "\n",
    "---\n",
    "\n",
    "## üé™ Why This Matters for GPT-2 vs Advanced Models:\n",
    "\n",
    "Since you found GPT-2 reasoning \"sounds like gibberish\", this enhanced framework will help you discover:\n",
    "- **Better Open Source Alternatives** like DeepSeek Coder (advanced reasoning)\n",
    "- **Specialized Models** optimized for text generation and explanation\n",
    "- **Balanced Options** that provide better quality without sacrificing speed\n",
    "- **Cutting-Edge Models** that might significantly outperform GPT-2\n",
    "\n",
    "Let's find the **perfect model** for your fraud detection reasoning needs! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages for local reasoning\n",
    "!pip install transformers torch accelerate --quiet\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üöÄ Fraud Detection Reasoning Environment Setup\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚úÖ GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Using CPU - consider enabling GPU accelerator\")\n",
    "\n",
    "print(\"‚úÖ Environment ready for local reasoning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# üß† Local Fraud Detection Reasoning on Kaggle\n",
    "\n",
    "This notebook provides **local AI-powered reasoning** for fraud detection using Kaggle's GPU resources instead of paid APIs.\n",
    "\n",
    "## üöÄ SETUP INSTRUCTIONS FOR KAGGLE\n",
    "\n",
    "### Step 1: Upload Your Trained Model\n",
    "1. **Create a Kaggle Dataset** with your trained model files:\n",
    "   - Upload your `distilbert_model/` folder (contains config.json, model.safetensors)\n",
    "   - Upload your `distilbert_tokenizer/` folder (contains tokenizer files)\n",
    "   - Name your dataset (e.g., \"fraud-detection-models\")\n",
    "\n",
    "### Step 2: Add Dataset to This Notebook\n",
    "1. **Add your dataset as input** to this notebook:\n",
    "   - Click \"Add data\" ‚Üí \"Your datasets\" ‚Üí Select your model dataset\n",
    "   - This makes your model files available at `/kaggle/input/your-dataset-name/`\n",
    "\n",
    "### Step 3: Update Model Paths\n",
    "1. **Update the paths in Cell 4** to match your dataset name:\n",
    "   ```python\n",
    "   MODEL_PATH = '/kaggle/input/YOUR-DATASET-NAME/distilbert_model'\n",
    "   TOKENIZER_PATH = '/kaggle/input/YOUR-DATASET-NAME/distilbert_tokenizer'\n",
    "   ```\n",
    "\n",
    "### Step 4: Run the Notebook\n",
    "1. **Enable GPU accelerator** for faster inference\n",
    "2. **Run all cells** to load your model and start analyzing texts\n",
    "\n",
    "## üìä Pipeline Flow\n",
    "1. **Load** your trained DistilBERT model from Kaggle dataset\n",
    "2. **Classify** texts into fraud categories using your actual model  \n",
    "3. **Generate reasoning** using local LM for non-legitimate classifications\n",
    "4. **Download** results with explanations\n",
    "\n",
    "## üîß Requirements\n",
    "- Your trained `distilbert_model/` and `distilbert_tokenizer/` uploaded as a Kaggle dataset\n",
    "- GPU accelerator enabled for faster inference\n",
    "- No API keys needed - everything runs locally!\n",
    "\n",
    "## ‚ö†Ô∏è Important Notes\n",
    "- **Without proper model upload**: Notebook will run in demo mode with simulated results\n",
    "- **With proper model upload**: You get real AI-powered fraud detection and reasoning\n",
    "- The reasoning engine works with ANY classification result (real or demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load Local Language Model for Reasoning (Free Alternative to APIs)\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"üß† Loading Local Language Model for Reasoning...\")\n",
    "\n",
    "# Use a smaller, efficient model that works well on Kaggle's free tier\n",
    "# Options: 'microsoft/DialoGPT-medium', 'gpt2', 'distilgpt2'\n",
    "reasoning_model_name = \"microsoft/DialoGPT-medium\"  # Good balance of quality and speed\n",
    "\n",
    "try:\n",
    "    # Initialize reasoning pipeline\n",
    "    reasoning_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=reasoning_model_name,\n",
    "        device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        max_length=512,\n",
    "        pad_token_id=50256  # Set pad token to avoid warnings\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Local reasoning model loaded: {reasoning_model_name}\")\n",
    "    print(\"üí° This model will generate explanations locally (no API costs!)\")\n",
    "    \n",
    "    # Test the reasoning model\n",
    "    test_prompt = \"This text appears to be a scam because\"\n",
    "    test_response = reasoning_pipe(test_prompt, max_length=50, num_return_sequences=1)\n",
    "    print(\"üß™ Model test successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error loading model: {e}\")\n",
    "    print(\"Falling back to simpler model...\")\n",
    "    # Fallback to smaller model\n",
    "    reasoning_pipe = pipeline(\"text-generation\", model=\"distilgpt2\", device=0 if torch.cuda.is_available() else -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Advanced LLM Testing Framework - Compare Multiple Models + Visualizations\n",
    "\n",
    "This section tests different language models to find the best one for fraud detection reasoning with **comprehensive visualizations**. We'll evaluate:\n",
    "- **Quality**: How well does the model explain fraud patterns? (8 categories, 97+ indicators)\n",
    "- **Speed**: How fast is inference?\n",
    "- **Memory**: How much GPU/CPU memory does it use?\n",
    "- **Coherence**: How readable and logical are the explanations?\n",
    "- **Relevance**: How relevant are explanations to specific fraud types?\n",
    "\n",
    "## üöÄ Advanced Models to Test:\n",
    "\n",
    "### Small/Fast Models (High Priority)\n",
    "- **distilgpt2** - 82M params (fastest, smallest)\n",
    "- **microsoft/DialoGPT-small** - 117M params (conversational)\n",
    "\n",
    "### Medium Models (Balanced Performance)\n",
    "- **gpt2** - 124M params (standard, often best balance)\n",
    "- **microsoft/DialoGPT-medium** - 345M params (good explanations)\n",
    "- **gpt2-medium** - 355M params (detailed reasoning)\n",
    "\n",
    "### Advanced Open Source Models\n",
    "- **EleutherAI/gpt-neo-125m** - 125M params (EleutherAI OSS)\n",
    "- **microsoft/DialoGPT-large** - 762M params (high quality dialogue)\n",
    "\n",
    "### Cutting-edge Specialized Models\n",
    "- **deepseek-ai/deepseek-coder-1.3b-base** - 1.3B params (advanced reasoning)\n",
    "- **Qwen/Qwen1.5-0.5B** - 0.5B params (Alibaba Cloud OSS)\n",
    "- **TinyLlama/TinyLlama-1.1B-Chat-v1.0** - 1.1B params (optimized chat)\n",
    "\n",
    "### Production-Ready MoE Models\n",
    "- **microsoft/gpt-oss-20b** - 21B total params with 3.6B active (optimal latency)\n",
    "\n",
    "### Large Models (Resource Intensive)\n",
    "- **EleutherAI/gpt-j-6b** - 6B params (requires 16GB+ GPU)\n",
    "\n",
    "## üìä Comprehensive Visualizations Included:\n",
    "1. **Performance Bar Charts** - Overall quality and speed comparison\n",
    "2. **Radar Charts** - Quality categories breakdown for top models\n",
    "3. **Speed vs Quality Scatter Plot** - Find optimal balance models\n",
    "4. **Model Size vs Performance** - Understand parameter efficiency\n",
    "5. **Category Performance Heatmap** - Detailed quality analysis\n",
    "6. **Interactive Dashboard** - Plotly-powered exploration\n",
    "\n",
    "Each model will be tested on **real fraud examples** from your CSV dataset with detailed quality analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced Multi-Model Testing Framework for Fraud Reasoning\n",
    "import time\n",
    "import gc\n",
    "import traceback\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "# Install additional visualization packages\n",
    "!pip install plotly matplotlib seaborn --quiet\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class EnhancedLLMTester:\n",
    "    \"\"\"Test multiple language models for fraud detection reasoning with comprehensive quality metrics and advanced visualizations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models_to_test = [\n",
    "            # Small/Fast Models (High Priority)\n",
    "            {\n",
    "                'name': 'distilgpt2',\n",
    "                'type': 'causal',\n",
    "                'size': 'small',\n",
    "                'category': 'small',\n",
    "                'description': 'Distilled GPT-2 (fastest, smallest)',\n",
    "                'params': '82M',\n",
    "                'priority': 'high',\n",
    "                'expected_performance': 'fast_inference'\n",
    "            },\n",
    "            {\n",
    "                'name': 'microsoft/DialoGPT-small',\n",
    "                'type': 'conversational',\n",
    "                'size': 'small',\n",
    "                'category': 'small',\n",
    "                'description': 'Small conversational model',\n",
    "                'params': '117M',\n",
    "                'priority': 'medium',\n",
    "                'expected_performance': 'good_dialogue'\n",
    "            },\n",
    "            # Medium Models (Balanced)\n",
    "            {\n",
    "                'name': 'gpt2',\n",
    "                'type': 'causal',\n",
    "                'size': 'medium',\n",
    "                'category': 'medium',\n",
    "                'description': 'Standard GPT-2 model',\n",
    "                'params': '124M',\n",
    "                'priority': 'high',\n",
    "                'expected_performance': 'best_balance'\n",
    "            },\n",
    "            {\n",
    "                'name': 'microsoft/DialoGPT-medium',\n",
    "                'type': 'conversational',\n",
    "                'size': 'medium',\n",
    "                'category': 'medium',\n",
    "                'description': 'Medium conversational AI',\n",
    "                'params': '345M',\n",
    "                'priority': 'high',\n",
    "                'expected_performance': 'good_explanations'\n",
    "            },\n",
    "            {\n",
    "                'name': 'gpt2-medium',\n",
    "                'type': 'causal',\n",
    "                'size': 'medium',\n",
    "                'category': 'medium',\n",
    "                'description': 'GPT-2 Medium (355M)',\n",
    "                'params': '355M',\n",
    "                'priority': 'medium',\n",
    "                'expected_performance': 'detailed_reasoning'\n",
    "            },\n",
    "            # Advanced Open Source Models\n",
    "            {\n",
    "                'name': 'EleutherAI/gpt-neo-125m',\n",
    "                'type': 'causal',\n",
    "                'size': 'small-medium',\n",
    "                'category': 'neo',\n",
    "                'description': 'GPT-Neo 125M (EleutherAI OSS)',\n",
    "                'params': '125M',\n",
    "                'priority': 'high',\n",
    "                'expected_performance': 'open_source_quality'\n",
    "            },\n",
    "            {\n",
    "                'name': 'microsoft/DialoGPT-large',\n",
    "                'type': 'conversational',\n",
    "                'size': 'large',\n",
    "                'category': 'large',\n",
    "                'description': 'Large conversational model',\n",
    "                'params': '762M',\n",
    "                'priority': 'medium',\n",
    "                'expected_performance': 'high_quality_dialogue'\n",
    "            },\n",
    "            # Cutting-edge Specialized Models\n",
    "            {\n",
    "                'name': 'deepseek-ai/deepseek-coder-1.3b-base',\n",
    "                'type': 'coding',\n",
    "                'size': 'medium-large',\n",
    "                'category': 'specialized',\n",
    "                'description': 'DeepSeek Coder 1.3B (advanced reasoning)',\n",
    "                'params': '1.3B',\n",
    "                'priority': 'high',\n",
    "                'expected_performance': 'superior_reasoning'\n",
    "            },\n",
    "            {\n",
    "                'name': 'Qwen/Qwen1.5-0.5B',\n",
    "                'type': 'causal',\n",
    "                'size': 'small-medium',\n",
    "                'category': 'qwen',\n",
    "                'description': 'Qwen 0.5B (Alibaba Cloud OSS)',\n",
    "                'params': '0.5B',\n",
    "                'priority': 'medium',\n",
    "                'expected_performance': 'efficient_multilingual'\n",
    "            },\n",
    "            # Alternative Open Source Models\n",
    "            {\n",
    "                'name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    "                'type': 'chat',\n",
    "                'size': 'medium',\n",
    "                'category': 'tiny',\n",
    "                'description': 'TinyLlama 1.1B Chat (Optimized OSS)',\n",
    "                'params': '1.1B',\n",
    "                'priority': 'medium',\n",
    "                'expected_performance': 'chat_optimized'\n",
    "            }\n",
    "            # # Large Models (Resource Intensive)\n",
    "            # {\n",
    "            #     'name': 'EleutherAI/gpt-j-6b',\n",
    "            #     'type': 'causal',\n",
    "            #     'size': 'very-large',\n",
    "            #     'category': 'large',\n",
    "            #     'description': 'GPT-J 6B (requires 16GB+ GPU)',\n",
    "            #     'params': '6B',\n",
    "            #     'priority': 'low',\n",
    "            #     'expected_performance': 'highest_quality'\n",
    "            # },\n",
    "            # {\n",
    "            #     'name': 'microsoft/DialoGPT-large',\n",
    "            #     'type': 'conversational',\n",
    "            #     'size': 'large',\n",
    "            #     'category': 'large',\n",
    "            #     'description': 'Large conversational model',\n",
    "            #     'params': '762M',\n",
    "            #     'priority': 'low',\n",
    "            #     'expected_performance': 'detailed_conversations'\n",
    "            # },\n",
    "            # # Production-Ready MoE Models\n",
    "            # {\n",
    "            #     'name': 'microsoft/gpt-oss-20b',\n",
    "            #     'type': 'mixture-of-experts',\n",
    "            #     'size': 'extra-large',\n",
    "            #     'category': 'production',\n",
    "            #     'description': 'GPT-OSS 20B MoE - 21B total params with 3.6B active for lower latency',\n",
    "            #     'params': '21B (3.6B active)',\n",
    "            #     'priority': 'high',\n",
    "            #     'expected_performance': 'production_ready_excellence',\n",
    "            #     'use_case': 'Ideal for production deployment - excellent quality with optimized latency'\n",
    "            # }\n",
    "        ]\n",
    "        \n",
    "        # Load real test cases from CSV dataset\n",
    "        self.test_cases = []\n",
    "        self.load_real_test_cases()\n",
    "        \n",
    "        # Comprehensive quality indicators for fraud reasoning\n",
    "        self.quality_indicators = {\n",
    "            'fraud_keywords': [\n",
    "                'scam', 'fraud', 'fraudulent', 'suspicious', 'fake', 'phishing', 'deceptive', \n",
    "                'malicious', 'illegitimate', 'unauthorized', 'impersonation', 'steal', 'theft',\n",
    "                'criminal', 'illegal', 'dangerous', 'harmful', 'misleading', 'dishonest'\n",
    "            ],\n",
    "            'urgency_indicators': [\n",
    "                'urgent', 'immediate', 'emergency', 'critical', 'expires', 'deadline',\n",
    "                'limited time', 'act now', 'hurry', 'quickly', 'asap', 'time sensitive'\n",
    "            ],\n",
    "            'financial_indicators': [\n",
    "                'money', 'payment', 'prize', 'reward', 'fee', 'cost', 'charge', 'refund',\n",
    "                'billing', 'account', 'bank', 'credit', 'deposit', 'transfer', 'claim'\n",
    "            ],\n",
    "            'action_indicators': [\n",
    "                'click', 'call', 'contact', 'verify', 'confirm', 'update', 'download',\n",
    "                'install', 'visit', 'respond', 'reply', 'submit', 'provide', 'send'\n",
    "            ],\n",
    "            'technical_indicators': [\n",
    "                'virus', 'malware', 'infected', 'security', 'breach', 'hacked', 'compromised',\n",
    "                'system', 'computer', 'device', 'software', 'update', 'patch', 'support'\n",
    "            ],\n",
    "            'social_engineering': [\n",
    "                'trust', 'authority', 'official', 'government', 'legitimate', 'verify',\n",
    "                'confirm', 'identity', 'personal', 'confidential', 'private', 'secure'\n",
    "            ],\n",
    "            'emotional_manipulation': [\n",
    "                'fear', 'panic', 'worry', 'concern', 'anxiety', 'stress', 'relief',\n",
    "                'excitement', 'congratulations', 'winner', 'lucky', 'selected', 'special'\n",
    "            ],\n",
    "            'explanation_quality': [\n",
    "                'because', 'since', 'due to', 'reason', 'explains', 'indicates', 'suggests',\n",
    "                'shows', 'demonstrates', 'reveals', 'exposes', 'highlights', 'pattern'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        self.results = {}\n",
    "    \n",
    "    def load_real_test_cases(self):\n",
    "        \"\"\"Load test cases from the actual CSV dataset\"\"\"\n",
    "        print(\"üìÑ Loading real test cases from CSV dataset...\")\n",
    "        \n",
    "        try:\n",
    "            # Try to load a sample of the dataset\n",
    "            import pandas as pd\n",
    "            \n",
    "            # Load dataset (adjust path as needed)\n",
    "            csv_path = '/kaggle/input/fraud-dataset/final_fraud_detection_dataset.csv'\n",
    "            if not os.path.exists(csv_path):\n",
    "                csv_path = '/kaggle/input/fraud-detection-dataset/final_fraud_detection_dataset.csv'\n",
    "            \n",
    "            if os.path.exists(csv_path):\n",
    "                # Load a sample of each fraud type for testing\n",
    "                print(f\"‚úÖ Found dataset at: {csv_path}\")\n",
    "                \n",
    "                # Read dataset in chunks to handle large files\n",
    "                chunk_size = 1000\n",
    "                sample_cases = []\n",
    "                \n",
    "                for chunk in pd.read_csv(csv_path, chunksize=chunk_size):\n",
    "                    # Get samples from each category\n",
    "                    for category in chunk['detailed_category'].unique():\n",
    "                        category_samples = chunk[chunk['detailed_category'] == category].head(2)\n",
    "                        for _, row in category_samples.iterrows():\n",
    "                            if len(sample_cases) < 20:  # Limit to 20 test cases\n",
    "                                sample_cases.append({\n",
    "                                    'type': row['detailed_category'],\n",
    "                                    'text': row['text'][:500],  # Limit text length\n",
    "                                    'true_label': row['detailed_category']\n",
    "                                })\n",
    "                    \n",
    "                    if len(sample_cases) >= 20:\n",
    "                        break\n",
    "                \n",
    "                self.test_cases = sample_cases\n",
    "                print(f\"‚úÖ Loaded {len(self.test_cases)} real test cases from dataset\")\n",
    "                \n",
    "                # Show categories loaded\n",
    "                categories = Counter([case['type'] for case in self.test_cases])\n",
    "                print(\"üìä Test cases by category:\")\n",
    "                for category, count in categories.items():\n",
    "                    print(f\"   {category}: {count} cases\")\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  CSV dataset not found, using fallback test cases\")\n",
    "                self.load_fallback_test_cases()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading CSV dataset: {e}\")\n",
    "            print(\"üîÑ Using fallback test cases instead\")\n",
    "            self.load_fallback_test_cases()\n",
    "    \n",
    "    def load_fallback_test_cases(self):\n",
    "        \"\"\"Load fallback test cases if CSV is not available\"\"\"\n",
    "        self.test_cases = [\n",
    "            {\n",
    "                'type': 'phishing',\n",
    "                'text': \"URGENT: Your PayPal account suspended. Click here to verify: http://paypal-fake.com\",\n",
    "                'true_label': 'phishing'\n",
    "            },\n",
    "            {\n",
    "                'type': 'tech_support_scam',\n",
    "                'text': \"WARNING: Virus detected! Call Microsoft Support: 1-800-FAKE\",\n",
    "                'true_label': 'tech_support_scam'\n",
    "            },\n",
    "            {\n",
    "                'type': 'reward_scam', \n",
    "                'text': \"Congratulations! You won $1000! Send $50 fee to claim prize!\",\n",
    "                'true_label': 'reward_scam'\n",
    "            },\n",
    "            {\n",
    "                'type': 'legitimate',\n",
    "                'text': \"Your package has been delivered and is waiting at your front door.\",\n",
    "                'true_label': 'legitimate'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    def load_model(self, model_info: Dict) -> Tuple[bool, object, str]:\n",
    "        \"\"\"Load a specific model and return success status, pipeline, and error message\"\"\"\n",
    "        try:\n",
    "            print(f\"üîÑ Loading {model_info['name']}...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Clear GPU memory first\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            pipeline_obj = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model_info['name'],\n",
    "                device=0 if torch.cuda.is_available() else -1,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                max_length=256,\n",
    "                pad_token_id=50256,\n",
    "                return_full_text=False  # Only return generated text\n",
    "            )\n",
    "            \n",
    "            load_time = time.time() - start_time\n",
    "            print(f\"‚úÖ Loaded {model_info['name']} in {load_time:.2f}s\")\n",
    "            \n",
    "            return True, pipeline_obj, f\"Loaded successfully in {load_time:.2f}s\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Failed to load: {str(e)}\"\n",
    "            print(f\"‚ùå {model_info['name']}: {error_msg}\")\n",
    "            return False, None, error_msg\n",
    "    \n",
    "    def calculate_comprehensive_quality_score(self, generated_text: str, test_case: Dict) -> Dict:\n",
    "        \"\"\"Calculate comprehensive quality score using multiple indicators\"\"\"\n",
    "        text_lower = generated_text.lower()\n",
    "        scores = {}\n",
    "        total_indicators = 0\n",
    "        found_indicators = 0\n",
    "        \n",
    "        # Calculate score for each indicator category\n",
    "        for category, indicators in self.quality_indicators.items():\n",
    "            category_found = sum(1 for indicator in indicators if indicator in text_lower)\n",
    "            category_total = len(indicators)\n",
    "            category_score = (category_found / category_total) * 100 if category_total > 0 else 0\n",
    "            \n",
    "            scores[f'{category}_score'] = category_score\n",
    "            scores[f'{category}_found'] = category_found\n",
    "            scores[f'{category}_total'] = category_total\n",
    "            \n",
    "            total_indicators += category_total\n",
    "            found_indicators += category_found\n",
    "        \n",
    "        # Overall quality metrics\n",
    "        scores['overall_quality'] = (found_indicators / total_indicators) * 100 if total_indicators > 0 else 0\n",
    "        scores['total_indicators_found'] = found_indicators\n",
    "        scores['total_indicators_available'] = total_indicators\n",
    "        \n",
    "        # Additional quality metrics\n",
    "        scores['text_length'] = len(generated_text)\n",
    "        scores['word_count'] = len(generated_text.split())\n",
    "        scores['coherence_score'] = self.calculate_coherence_score(generated_text)\n",
    "        scores['relevance_score'] = self.calculate_relevance_score(generated_text, test_case)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def calculate_coherence_score(self, text: str) -> float:\n",
    "        \"\"\"Calculate text coherence based on sentence structure and flow\"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        # Simple coherence metrics\n",
    "        sentences = text.split('.')\n",
    "        coherence_indicators = [\n",
    "            'because', 'since', 'due to', 'therefore', 'thus', 'consequently',\n",
    "            'however', 'moreover', 'furthermore', 'additionally', 'also'\n",
    "        ]\n",
    "        \n",
    "        coherence_count = sum(1 for indicator in coherence_indicators if indicator in text.lower())\n",
    "        sentence_count = len([s for s in sentences if len(s.strip()) > 3])\n",
    "        \n",
    "        # Score based on logical connectors and sentence structure\n",
    "        if sentence_count == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        coherence_score = min(100, (coherence_count / sentence_count) * 100 + 20)  # Base score of 20\n",
    "        return coherence_score\n",
    "    \n",
    "    def calculate_relevance_score(self, generated_text: str, test_case: Dict) -> float:\n",
    "        \"\"\"Calculate how relevant the reasoning is to the specific fraud type\"\"\"\n",
    "        fraud_type = test_case['type']\n",
    "        text_lower = generated_text.lower()\n",
    "        \n",
    "        # Type-specific keywords\n",
    "        type_keywords = {\n",
    "            'phishing': ['phishing', 'credential', 'login', 'password', 'account', 'verify', 'click'],\n",
    "            'tech_support_scam': ['tech support', 'virus', 'computer', 'microsoft', 'infected', 'call'],\n",
    "            'reward_scam': ['prize', 'winner', 'reward', 'lottery', 'claim', 'congratulations'],\n",
    "            'job_scam': ['job', 'work', 'employment', 'income', 'opportunity', 'hiring'],\n",
    "            'sms_spam': ['text', 'sms', 'message', 'reply', 'stop', 'unsubscribe'],\n",
    "            'popup_scam': ['popup', 'alert', 'warning', 'browser', 'virus detected'],\n",
    "            'refund_scam': ['refund', 'billing', 'payment', 'charge', 'bank', 'transaction'],\n",
    "            'ssn_scam': ['social security', 'ssn', 'government', 'identity', 'verify'],\n",
    "            'legitimate': ['normal', 'safe', 'legitimate', 'genuine', 'real']\n",
    "        }\n",
    "        \n",
    "        relevant_keywords = type_keywords.get(fraud_type, [])\n",
    "        if not relevant_keywords:\n",
    "            return 50.0  # Default score for unknown types\n",
    "            \n",
    "        found_keywords = sum(1 for keyword in relevant_keywords if keyword in text_lower)\n",
    "        relevance_score = (found_keywords / len(relevant_keywords)) * 100\n",
    "        \n",
    "        return relevance_score\n",
    "    \n",
    "    def test_model_reasoning(self, pipeline_obj, model_name: str, test_case: Dict) -> Dict:\n",
    "        \"\"\"Test a model's reasoning capability with comprehensive quality assessment\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Skip reasoning for legitimate cases (focus on fraud detection)\n",
    "            if test_case['type'] == 'legitimate':\n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'generated_text': \"This text appears to be legitimate communication.\",\n",
    "                    'inference_time': 0.01,\n",
    "                    'quality_scores': {'overall_quality': 100, 'relevance_score': 100},\n",
    "                    'prompt_used': 'N/A (legitimate text)',\n",
    "                    'error': None,\n",
    "                    'skipped_legitimate': True\n",
    "                }\n",
    "            \n",
    "            # Create reasoning prompt for fraud cases\n",
    "            prompt = f\"This text appears to be a {test_case['type']} scam because\"\n",
    "            \n",
    "            # Generate reasoning\n",
    "            response = pipeline_obj(\n",
    "                prompt,\n",
    "                max_length=150,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                do_sample=True\n",
    "            )\n",
    "            \n",
    "            inference_time = time.time() - start_time\n",
    "            generated_text = response[0]['generated_text'] if response else \"No response generated\"\n",
    "            \n",
    "            # Calculate comprehensive quality scores\n",
    "            quality_scores = self.calculate_comprehensive_quality_score(generated_text, test_case)\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'generated_text': generated_text,\n",
    "                'inference_time': inference_time,\n",
    "                'quality_scores': quality_scores,\n",
    "                'prompt_used': prompt,\n",
    "                'error': None,\n",
    "                'skipped_legitimate': False\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'generated_text': None,\n",
    "                'inference_time': 0,\n",
    "                'quality_scores': {'overall_quality': 0},\n",
    "                'prompt_used': prompt if 'prompt' in locals() else 'Error before prompt creation',\n",
    "                'error': str(e),\n",
    "                'skipped_legitimate': False\n",
    "            }\n",
    "    \n",
    "    def run_comprehensive_test(self):\n",
    "        \"\"\"Run all models on all test cases with enhanced metrics\"\"\"\n",
    "        print(\"üöÄ Starting Enhanced LLM Testing for Fraud Reasoning\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"üìä Testing {len(self.models_to_test)} models on {len(self.test_cases)} real cases\")\n",
    "        print(\"üéØ Enhanced quality metrics include:\")\n",
    "        print(\"   ‚Ä¢ Fraud detection keywords (19 indicators)\")\n",
    "        print(\"   ‚Ä¢ Urgency manipulation tactics (12 indicators)\")  \n",
    "        print(\"   ‚Ä¢ Financial exploitation patterns (15 indicators)\")\n",
    "        print(\"   ‚Ä¢ Action-oriented language (13 indicators)\")\n",
    "        print(\"   ‚Ä¢ Technical deception methods (13 indicators)\")\n",
    "        print(\"   ‚Ä¢ Social engineering techniques (12 indicators)\")\n",
    "        print(\"   ‚Ä¢ Emotional manipulation patterns (13 indicators)\")\n",
    "        print(\"   ‚Ä¢ Explanation coherence and relevance\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for model_info in self.models_to_test:\n",
    "            print(f\"\\nüîç Testing: {model_info['name']}\")\n",
    "            print(f\"üìã Type: {model_info['type']} | Size: {model_info['size']}\")\n",
    "            print(f\"üí° Description: {model_info['description']}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            # Load model\n",
    "            success, pipeline_obj, load_msg = self.load_model(model_info)\n",
    "            \n",
    "            if not success:\n",
    "                self.results[model_info['name']] = {\n",
    "                    'model_info': model_info,\n",
    "                    'load_success': False,\n",
    "                    'load_message': load_msg,\n",
    "                    'test_results': {}\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            # Test on all cases\n",
    "            model_results = {\n",
    "                'model_info': model_info,\n",
    "                'load_success': True,\n",
    "                'load_message': load_msg,\n",
    "                'test_results': {}\n",
    "            }\n",
    "            \n",
    "            total_inference_time = 0\n",
    "            total_quality_scores = {}\n",
    "            successful_tests = 0\n",
    "            fraud_tests = 0\n",
    "            \n",
    "            # Initialize quality score accumulators\n",
    "            for category in self.quality_indicators.keys():\n",
    "                total_quality_scores[f'{category}_score'] = 0\n",
    "            total_quality_scores['overall_quality'] = 0\n",
    "            total_quality_scores['coherence_score'] = 0\n",
    "            total_quality_scores['relevance_score'] = 0\n",
    "            \n",
    "            for test_case in self.test_cases:\n",
    "                fraud_type = test_case['type']\n",
    "                print(f\"  üß™ Testing {fraud_type}...\")\n",
    "                \n",
    "                result = self.test_model_reasoning(pipeline_obj, model_info['name'], test_case)\n",
    "                model_results['test_results'][f\"{fraud_type}_{successful_tests}\"] = result\n",
    "                \n",
    "                if result['success']:\n",
    "                    total_inference_time += result['inference_time']\n",
    "                    successful_tests += 1\n",
    "                    \n",
    "                    if not result.get('skipped_legitimate', False):\n",
    "                        fraud_tests += 1\n",
    "                        # Accumulate quality scores\n",
    "                        for score_name, score_value in result['quality_scores'].items():\n",
    "                            if score_name in total_quality_scores:\n",
    "                                total_quality_scores[score_name] += score_value\n",
    "                    \n",
    "                    # Show detailed results\n",
    "                    if result.get('skipped_legitimate', False):\n",
    "                        print(f\"     ‚è≠Ô∏è  Skipped (legitimate) | Time: {result['inference_time']:.2f}s\")\n",
    "                    else:\n",
    "                        overall_q = result['quality_scores']['overall_quality']\n",
    "                        relevance_q = result['quality_scores']['relevance_score']\n",
    "                        coherence_q = result['quality_scores']['coherence_score']\n",
    "                        print(f\"     ‚úÖ Overall: {overall_q:.1f}% | Relevance: {relevance_q:.1f}% | Coherence: {coherence_q:.1f}% | Time: {result['inference_time']:.2f}s\")\n",
    "                else:\n",
    "                    print(f\"     ‚ùå Failed: {result['error']}\")\n",
    "            \n",
    "            # Calculate averages\n",
    "            if successful_tests > 0:\n",
    "                model_results['avg_inference_time'] = total_inference_time / successful_tests\n",
    "                model_results['success_rate'] = (successful_tests / len(self.test_cases)) * 100\n",
    "                \n",
    "                # Calculate average quality scores (only for fraud cases)\n",
    "                if fraud_tests > 0:\n",
    "                    model_results['avg_quality_scores'] = {\n",
    "                        score_name: score_value / fraud_tests \n",
    "                        for score_name, score_value in total_quality_scores.items()\n",
    "                    }\n",
    "                    model_results['overall_quality'] = model_results['avg_quality_scores']['overall_quality']\n",
    "                else:\n",
    "                    model_results['avg_quality_scores'] = {score: 0 for score in total_quality_scores.keys()}\n",
    "                    model_results['overall_quality'] = 0\n",
    "                    \n",
    "                model_results['fraud_tests_completed'] = fraud_tests\n",
    "            else:\n",
    "                model_results['avg_inference_time'] = 0\n",
    "                model_results['overall_quality'] = 0\n",
    "                model_results['success_rate'] = 0\n",
    "                model_results['avg_quality_scores'] = {score: 0 for score in total_quality_scores.keys()}\n",
    "                model_results['fraud_tests_completed'] = 0\n",
    "            \n",
    "            self.results[model_info['name']] = model_results\n",
    "            \n",
    "            # Clean up\n",
    "            del pipeline_obj\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            print(f\"  üìä Overall Quality: {model_results['overall_quality']:.1f}%\")\n",
    "            print(f\"  ‚ö° Average Speed: {model_results['avg_inference_time']:.2f}s\")\n",
    "            print(f\"  ‚úÖ Success Rate: {model_results['success_rate']:.1f}%\")\n",
    "            print(f\"  üéØ Fraud Cases: {model_results['fraud_tests_completed']}/{len(self.test_cases)}\")\n",
    "    \n",
    "    def generate_comparison_report(self):\n",
    "        \"\"\"Generate a detailed comparison report with enhanced metrics\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìä ENHANCED LLM COMPARISON REPORT\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Sort models by performance\n",
    "        successful_models = {\n",
    "            name: results for name, results in self.results.items() \n",
    "            if results['load_success'] and results['success_rate'] > 0\n",
    "        }\n",
    "        \n",
    "        if not successful_models:\n",
    "            print(\"‚ùå No models successfully completed testing!\")\n",
    "            return\n",
    "        \n",
    "        # Performance ranking by overall quality\n",
    "        ranked_models = sorted(\n",
    "            successful_models.items(),\n",
    "            key=lambda x: (x[1]['overall_quality'], -x[1]['avg_inference_time']),\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüèÜ PERFORMANCE RANKING (by Overall Quality + Speed)\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for rank, (name, results) in enumerate(ranked_models, 1):\n",
    "            print(f\"{rank}. {name}\")\n",
    "            print(f\"   Overall Quality: {results['overall_quality']:.1f}% | Speed: {results['avg_inference_time']:.2f}s\")\n",
    "            print(f\"   Success Rate: {results['success_rate']:.1f}% | Fraud Cases: {results['fraud_tests_completed']}\")\n",
    "            print(f\"   Size: {results['model_info']['size']} | Type: {results['model_info']['type']}\")\n",
    "        \n",
    "        # Detailed quality breakdown\n",
    "        print(f\"\\nüîç DETAILED QUALITY BREAKDOWN\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        quality_categories = [\n",
    "            ('fraud_keywords', 'Fraud Detection'),\n",
    "            ('urgency_indicators', 'Urgency Recognition'), \n",
    "            ('financial_indicators', 'Financial Patterns'),\n",
    "            ('action_indicators', 'Action-Oriented Language'),\n",
    "            ('technical_indicators', 'Technical Deception'),\n",
    "            ('social_engineering', 'Social Engineering'),\n",
    "            ('emotional_manipulation', 'Emotional Manipulation'),\n",
    "            ('coherence_score', 'Text Coherence'),\n",
    "            ('relevance_score', 'Type Relevance')\n",
    "        ]\n",
    "        \n",
    "        for name, results in ranked_models:\n",
    "            print(f\"\\nü§ñ {name}:\")\n",
    "            if 'avg_quality_scores' in results:\n",
    "                for category_key, category_name in quality_categories:\n",
    "                    score_key = f'{category_key}_score' if category_key in ['fraud_keywords', 'urgency_indicators', 'financial_indicators', 'action_indicators', 'technical_indicators', 'social_engineering', 'emotional_manipulation'] else category_key\n",
    "                    score = results['avg_quality_scores'].get(score_key, 0)\n",
    "                    print(f\"   {category_name:20s}: {score:6.1f}%\")\n",
    "        \n",
    "        # Best in category analysis\n",
    "        print(f\"\\nüéñÔ∏è  CATEGORY LEADERS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for category_key, category_name in quality_categories:\n",
    "            score_key = f'{category_key}_score' if category_key in ['fraud_keywords', 'urgency_indicators', 'financial_indicators', 'action_indicators', 'technical_indicators', 'social_engineering', 'emotional_manipulation'] else category_key\n",
    "            \n",
    "            best_model = max(\n",
    "                ranked_models, \n",
    "                key=lambda x: x[1]['avg_quality_scores'].get(score_key, 0)\n",
    "            )\n",
    "            best_score = best_model[1]['avg_quality_scores'].get(score_key, 0)\n",
    "            print(f\"üèÖ {category_name:20s}: {best_model[0]} ({best_score:.1f}%)\")\n",
    "        \n",
    "        # Speed analysis\n",
    "        print(f\"\\n‚ö° SPEED ANALYSIS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        fastest_model = min(ranked_models, key=lambda x: x[1]['avg_inference_time'])\n",
    "        slowest_model = max(ranked_models, key=lambda x: x[1]['avg_inference_time'])\n",
    "        \n",
    "        print(f\"ü•á Fastest: {fastest_model[0]} ({fastest_model[1]['avg_inference_time']:.2f}s)\")\n",
    "        print(f\"üêå Slowest: {slowest_model[0]} ({slowest_model[1]['avg_inference_time']:.2f}s)\")\n",
    "        \n",
    "        # Sample reasoning outputs\n",
    "        print(f\"\\nüìù SAMPLE REASONING OUTPUTS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for name, results in ranked_models[:2]:  # Show top 2 models\n",
    "            print(f\"\\nü§ñ {name}:\")\n",
    "            # Find a fraud test case to show\n",
    "            for test_name, test_result in results['test_results'].items():\n",
    "                if (test_result['success'] and \n",
    "                    not test_result.get('skipped_legitimate', False) and \n",
    "                    test_result['generated_text']):\n",
    "                    print(f\"   Sample: {test_result['generated_text'][:120]}...\")\n",
    "                    break\n",
    "        \n",
    "        # Comprehensive recommendations\n",
    "        print(f\"\\nüí° COMPREHENSIVE RECOMMENDATIONS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        if ranked_models:\n",
    "            top_model = ranked_models[0]\n",
    "            print(f\"ü•á Overall Best: {top_model[0]}\")\n",
    "            print(f\"   Best balance: {top_model[1]['overall_quality']:.1f}% quality, {top_model[1]['avg_inference_time']:.2f}s speed\")\n",
    "            \n",
    "            # Specific recommendations\n",
    "            print(f\"\\nüéØ Use Cases:\")\n",
    "            \n",
    "            # Best for accuracy\n",
    "            best_quality = max(ranked_models, key=lambda x: x[1]['overall_quality'])\n",
    "            if best_quality[1]['overall_quality'] > 50:\n",
    "                print(f\"   üìä Highest Quality: {best_quality[0]} ({best_quality[1]['overall_quality']:.1f}%)\")\n",
    "            \n",
    "            # Best for speed\n",
    "            if fastest_model[1]['avg_inference_time'] < 2.0:\n",
    "                print(f\"   ‚ö° Speed Critical: {fastest_model[0]} ({fastest_model[1]['avg_inference_time']:.2f}s)\")\n",
    "            \n",
    "            # Best for specific fraud types\n",
    "            best_relevance = max(ranked_models, key=lambda x: x[1]['avg_quality_scores'].get('relevance_score', 0))\n",
    "            print(f\"   üéØ Type-Specific Detection: {best_relevance[0]} ({best_relevance[1]['avg_quality_scores'].get('relevance_score', 0):.1f}% relevance)\")\n",
    "        \n",
    "        print(f\"\\nüìà DATASET INSIGHTS\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Test Cases Used: {len(self.test_cases)} (from real CSV dataset)\")\n",
    "        \n",
    "        # Show category distribution\n",
    "        category_counts = Counter([case['type'] for case in self.test_cases])\n",
    "        print(\"Category Distribution:\")\n",
    "        for category, count in category_counts.most_common():\n",
    "            print(f\"   {category}: {count} cases\")\n",
    "        \n",
    "        return ranked_models\n",
    "    \n",
    "    def create_performance_visualizations(self, save_figures=True):\n",
    "        \"\"\"Create comprehensive visualizations of model performance\"\"\"\n",
    "        \n",
    "        successful_models = {\n",
    "            name: results for name, results in self.results.items() \n",
    "            if results['load_success'] and results.get('success_rate', 0) > 0\n",
    "        }\n",
    "        \n",
    "        if not successful_models:\n",
    "            print(\"‚ùå No successful models to visualize!\")\n",
    "            return\n",
    "        \n",
    "        print(\"üìä Creating Advanced Performance Visualizations...\")\n",
    "        \n",
    "        # 1. Overall Performance Comparison (Bar Chart)\n",
    "        self._create_overall_performance_chart(successful_models, save_figures)\n",
    "        \n",
    "        # 2. Quality Categories Radar Chart\n",
    "        self._create_quality_radar_chart(successful_models, save_figures)\n",
    "        \n",
    "        # 3. Speed vs Quality Scatter Plot\n",
    "        self._create_speed_quality_scatter(successful_models, save_figures)\n",
    "        \n",
    "        # 4. Model Size vs Performance\n",
    "        self._create_size_performance_chart(successful_models, save_figures)\n",
    "        \n",
    "        # 5. Category Performance Heatmap\n",
    "        self._create_category_heatmap(successful_models, save_figures)\n",
    "        \n",
    "        # 6. Interactive Dashboard\n",
    "        self._create_interactive_dashboard(successful_models)\n",
    "        \n",
    "        print(\"‚úÖ All visualizations created successfully!\")\n",
    "    \n",
    "    def _create_overall_performance_chart(self, models, save_figures):\n",
    "        \"\"\"Create overall performance comparison bar chart\"\"\"\n",
    "        \n",
    "        model_names = []\n",
    "        overall_scores = []\n",
    "        speed_scores = []\n",
    "        colors = []\n",
    "        \n",
    "        # Color mapping by category\n",
    "        color_map = {\n",
    "            'small': '#FF6B6B',     # Red\n",
    "            'medium': '#4ECDC4',    # Teal  \n",
    "            'large': '#45B7D1',     # Blue\n",
    "            'neo': '#96CEB4',       # Green\n",
    "            'specialized': '#FFEAA7', # Yellow\n",
    "            'qwen': '#DDA0DD',      # Plum\n",
    "            'tiny': '#FFB347'       # Orange\n",
    "        }\n",
    "        \n",
    "        for name, results in models.items():\n",
    "            model_names.append(name.split('/')[-1])  # Clean name\n",
    "            overall_scores.append(results.get('overall_quality', 0))\n",
    "            # Convert speed to inverse for \"performance\" (higher = better)\n",
    "            speed_scores.append(100 / (results.get('avg_inference_time', 1) + 0.1))\n",
    "            \n",
    "            # Get color based on model category\n",
    "            model_info = next((m for m in self.models_to_test if m['name'] == name), {})\n",
    "            category = model_info.get('category', 'medium')\n",
    "            colors.append(color_map.get(category, '#95A5A6'))\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        \n",
    "        # Quality scores\n",
    "        bars1 = ax1.bar(model_names, overall_scores, color=colors, alpha=0.8)\n",
    "        ax1.set_title('üéØ Overall Quality Scores by Model', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylabel('Quality Score (%)', fontsize=12)\n",
    "        ax1.set_ylim(0, 100)\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, score in zip(bars1, overall_scores):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{score:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Speed performance  \n",
    "        bars2 = ax2.bar(model_names, speed_scores, color=colors, alpha=0.8)\n",
    "        ax2.set_title('‚ö° Speed Performance by Model', fontsize=14, fontweight='bold')\n",
    "        ax2.set_ylabel('Speed Performance (Higher = Faster)', fontsize=12)\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, name in zip(bars2, model_names):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                    name, ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_figures:\n",
    "            plt.savefig('llm_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def _create_quality_radar_chart(self, models, save_figures):\n",
    "        \"\"\"Create radar chart for quality categories\"\"\"\n",
    "        \n",
    "        quality_categories = [\n",
    "            'fraud_keywords_score', 'urgency_indicators_score', 'financial_indicators_score',\n",
    "            'action_indicators_score', 'technical_indicators_score', 'social_engineering_score',\n",
    "            'emotional_manipulation_score', 'coherence_score', 'relevance_score'\n",
    "        ]\n",
    "        \n",
    "        category_labels = [\n",
    "            'Fraud Keywords', 'Urgency Detection', 'Financial Patterns',\n",
    "            'Action Indicators', 'Technical Detection', 'Social Engineering',\n",
    "            'Emotional Manipulation', 'Coherence', 'Relevance'\n",
    "        ]\n",
    "        \n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Take top 4 models for clarity\n",
    "        top_models = sorted(models.items(), \n",
    "                           key=lambda x: x[1].get('overall_quality', 0), \n",
    "                           reverse=True)[:4]\n",
    "        \n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "        \n",
    "        for i, (model_name, model_data) in enumerate(top_models):\n",
    "            scores = []\n",
    "            for category in quality_categories:\n",
    "                score = model_data.get('avg_quality_scores', {}).get(category, 0)\n",
    "                scores.append(score)\n",
    "            \n",
    "            # Create subplot for each model\n",
    "            ax = fig.add_subplot(2, 2, i+1, projection='polar')\n",
    "            \n",
    "            # Number of variables\n",
    "            N = len(category_labels)\n",
    "            \n",
    "            # Compute angle for each axis\n",
    "            angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "            angles += angles[:1]  # Complete the circle\n",
    "            \n",
    "            # Add scores\n",
    "            scores += scores[:1]  # Complete the circle\n",
    "            \n",
    "            # Plot\n",
    "            ax.plot(angles, scores, 'o-', linewidth=2, label=model_name.split('/')[-1], color=colors[i])\n",
    "            ax.fill(angles, scores, alpha=0.25, color=colors[i])\n",
    "            ax.set_xticks(angles[:-1])\n",
    "            ax.set_xticklabels(category_labels, fontsize=8)\n",
    "            ax.set_ylim(0, 100)\n",
    "            ax.set_title(f'{model_name.split(\"/\")[-1]}', fontsize=12, fontweight='bold', pad=20)\n",
    "            ax.grid(True)\n",
    "        \n",
    "        plt.suptitle('üéØ Quality Categories Comparison (Radar Charts)', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        if save_figures:\n",
    "            plt.savefig('llm_quality_radar.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def _create_speed_quality_scatter(self, models, save_figures):\n",
    "        \"\"\"Create speed vs quality scatter plot\"\"\"\n",
    "        \n",
    "        model_names = []\n",
    "        quality_scores = []\n",
    "        inference_times = []\n",
    "        sizes = []\n",
    "        categories = []\n",
    "        \n",
    "        for name, results in models.items():\n",
    "            model_names.append(name.split('/')[-1])\n",
    "            quality_scores.append(results.get('overall_quality', 0))\n",
    "            inference_times.append(results.get('avg_inference_time', 1))\n",
    "            \n",
    "            # Get model info for size and category\n",
    "            model_info = next((m for m in self.models_to_test if m['name'] == name), {})\n",
    "            param_str = model_info.get('params', '100M')\n",
    "            # Extract numeric value from params (e.g., '124M' -> 124)\n",
    "            param_num = float(re.findall(r'[\\d.]+', param_str)[0]) if re.findall(r'[\\d.]+', param_str) else 100\n",
    "            if 'B' in param_str:\n",
    "                param_num *= 1000  # Convert billions to millions for scale\n",
    "            sizes.append(param_num)\n",
    "            categories.append(model_info.get('category', 'medium'))\n",
    "        \n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # Create scatter plot\n",
    "        scatter = plt.scatter(inference_times, quality_scores, s=[s*2 for s in sizes], \n",
    "                            c=range(len(model_names)), cmap='viridis', alpha=0.7, edgecolors='black')\n",
    "        \n",
    "        # Add model name labels\n",
    "        for i, name in enumerate(model_names):\n",
    "            plt.annotate(name, (inference_times[i], quality_scores[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points', \n",
    "                        fontsize=9, fontweight='bold',\n",
    "                        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
    "        \n",
    "        plt.xlabel('Average Inference Time (seconds)', fontsize=12, fontweight='bold')\n",
    "        plt.ylabel('Overall Quality Score (%)', fontsize=12, fontweight='bold')\n",
    "        plt.title('‚ö° Speed vs Quality Performance\\n(Bubble size = Model parameters)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Add ideal quadrant annotations\n",
    "        plt.axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "        plt.axvline(x=2, color='gray', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        plt.text(0.1, 80, 'IDEAL\\n(Fast + High Quality)', \n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),\n",
    "                fontsize=10, fontweight='bold')\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        if save_figures:\n",
    "            plt.savefig('llm_speed_quality_scatter.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def _create_size_performance_chart(self, models, save_figures):\n",
    "        \"\"\"Create model size vs performance comparison\"\"\"\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12))\n",
    "        \n",
    "        model_data = []\n",
    "        for name, results in models.items():\n",
    "            model_info = next((m for m in self.models_to_test if m['name'] == name), {})\n",
    "            param_str = model_info.get('params', '100M')\n",
    "            param_num = float(re.findall(r'[\\d.]+', param_str)[0]) if re.findall(r'[\\d.]+', param_str) else 100\n",
    "            if 'B' in param_str:\n",
    "                param_num *= 1000\n",
    "                \n",
    "            model_data.append({\n",
    "                'name': name.split('/')[-1],\n",
    "                'params': param_num,\n",
    "                'quality': results.get('overall_quality', 0),\n",
    "                'speed': results.get('avg_inference_time', 1),\n",
    "                'category': model_info.get('category', 'medium')\n",
    "            })\n",
    "        \n",
    "        # Sort by parameter count\n",
    "        model_data.sort(key=lambda x: x['params'])\n",
    "        \n",
    "        names = [m['name'] for m in model_data]\n",
    "        params = [m['params'] for m in model_data]\n",
    "        qualities = [m['quality'] for m in model_data]\n",
    "        speeds = [m['speed'] for m in model_data]\n",
    "        \n",
    "        # Model size vs quality\n",
    "        bars1 = ax1.bar(names, qualities, color='skyblue', alpha=0.8)\n",
    "        ax1_twin = ax1.twinx()\n",
    "        line1 = ax1_twin.plot(names, params, 'ro-', linewidth=2, markersize=8, color='red')\n",
    "        \n",
    "        ax1.set_ylabel('Quality Score (%)', fontsize=12, fontweight='bold', color='blue')\n",
    "        ax1_twin.set_ylabel('Model Parameters (Millions)', fontsize=12, fontweight='bold', color='red')\n",
    "        ax1.set_title('üìä Model Size vs Quality Performance', fontsize=14, fontweight='bold')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Model size vs speed\n",
    "        bars2 = ax2.bar(names, speeds, color='lightcoral', alpha=0.8)\n",
    "        ax2_twin = ax2.twinx()\n",
    "        line2 = ax2_twin.plot(names, params, 'go-', linewidth=2, markersize=8, color='green')\n",
    "        \n",
    "        ax2.set_ylabel('Inference Time (seconds)', fontsize=12, fontweight='bold', color='red')\n",
    "        ax2_twin.set_ylabel('Model Parameters (Millions)', fontsize=12, fontweight='bold', color='green')\n",
    "        ax2.set_title('‚ö° Model Size vs Speed Performance', fontsize=14, fontweight='bold')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_figures:\n",
    "            plt.savefig('llm_size_performance.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def _create_category_heatmap(self, models, save_figures):\n",
    "        \"\"\"Create heatmap of category performance across models\"\"\"\n",
    "        \n",
    "        quality_categories = [\n",
    "            ('fraud_keywords_score', 'Fraud Keywords'),\n",
    "            ('urgency_indicators_score', 'Urgency Detection'),\n",
    "            ('financial_indicators_score', 'Financial Patterns'),\n",
    "            ('action_indicators_score', 'Action Indicators'),\n",
    "            ('technical_indicators_score', 'Technical Detection'),\n",
    "            ('social_engineering_score', 'Social Engineering'),\n",
    "            ('emotional_manipulation_score', 'Emotional Manipulation'),\n",
    "            ('coherence_score', 'Coherence'),\n",
    "            ('relevance_score', 'Relevance')\n",
    "        ]\n",
    "        \n",
    "        # Prepare data matrix\n",
    "        model_names = [name.split('/')[-1] for name in models.keys()]\n",
    "        category_names = [cat[1] for cat in quality_categories]\n",
    "        \n",
    "        data_matrix = []\n",
    "        for name, results in models.items():\n",
    "            row = []\n",
    "            for cat_key, cat_name in quality_categories:\n",
    "                score = results.get('avg_quality_scores', {}).get(cat_key, 0)\n",
    "                row.append(score)\n",
    "            data_matrix.append(row)\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        sns.heatmap(data_matrix, \n",
    "                   xticklabels=category_names,\n",
    "                   yticklabels=model_names,\n",
    "                   annot=True, \n",
    "                   fmt='.1f',\n",
    "                   cmap='RdYlGn',\n",
    "                   center=50,\n",
    "                   square=True,\n",
    "                   linewidths=0.5,\n",
    "                   cbar_kws={'label': 'Performance Score (%)'})\n",
    "        \n",
    "        plt.title('üî• Model Performance Heatmap Across Quality Categories', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.xlabel('Quality Categories', fontsize=12, fontweight='bold')\n",
    "        plt.ylabel('Models', fontsize=12, fontweight='bold')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_figures:\n",
    "            plt.savefig('llm_category_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def _create_interactive_dashboard(self, models):\n",
    "        \"\"\"Create interactive dashboard using Plotly\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Prepare data for interactive plots\n",
    "            model_names = []\n",
    "            quality_scores = []\n",
    "            speed_scores = []\n",
    "            categories = []\n",
    "            params = []\n",
    "            \n",
    "            for name, results in models.items():\n",
    "                model_names.append(name.split('/')[-1])\n",
    "                quality_scores.append(results.get('overall_quality', 0))\n",
    "                speed_scores.append(results.get('avg_inference_time', 1))\n",
    "                \n",
    "                model_info = next((m for m in self.models_to_test if m['name'] == name), {})\n",
    "                categories.append(model_info.get('category', 'medium'))\n",
    "                params.append(model_info.get('params', '100M'))\n",
    "            \n",
    "            # Create interactive subplot dashboard\n",
    "            fig = make_subplots(\n",
    "                rows=2, cols=2,\n",
    "                subplot_titles=('Quality vs Speed', 'Model Categories', 'Quality Distribution', 'Speed Distribution'),\n",
    "                specs=[[{\"secondary_y\": False}, {\"type\": \"pie\"}],\n",
    "                       [{\"type\": \"histogram\"}, {\"type\": \"histogram\"}]]\n",
    "            )\n",
    "            \n",
    "            # 1. Quality vs Speed scatter\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=speed_scores,\n",
    "                    y=quality_scores,\n",
    "                    mode='markers+text',\n",
    "                    text=model_names,\n",
    "                    textposition=\"top center\",\n",
    "                    marker=dict(size=12, color=quality_scores, colorscale='Viridis', showscale=True),\n",
    "                    name='Models',\n",
    "                    hovertemplate='<b>%{text}</b><br>Speed: %{x:.2f}s<br>Quality: %{y:.1f}%<extra></extra>'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # 2. Category distribution pie chart\n",
    "            category_counts = Counter(categories)\n",
    "            fig.add_trace(\n",
    "                go.Pie(\n",
    "                    labels=list(category_counts.keys()),\n",
    "                    values=list(category_counts.values()),\n",
    "                    name=\"Categories\"\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # 3. Quality distribution histogram\n",
    "            fig.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=quality_scores,\n",
    "                    nbinsx=10,\n",
    "                    name=\"Quality Distribution\",\n",
    "                    marker_color='lightblue'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            # 4. Speed distribution histogram\n",
    "            fig.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=speed_scores,\n",
    "                    nbinsx=10,\n",
    "                    name=\"Speed Distribution\",\n",
    "                    marker_color='lightcoral'\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "            \n",
    "            # Update layout\n",
    "            fig.update_layout(\n",
    "                title_text=\"üöÄ Interactive LLM Performance Dashboard\",\n",
    "                title_font_size=20,\n",
    "                showlegend=False,\n",
    "                height=800,\n",
    "                width=1200\n",
    "            )\n",
    "            \n",
    "            # Update axes labels\n",
    "            fig.update_xaxes(title_text=\"Inference Time (s)\", row=1, col=1)\n",
    "            fig.update_yaxes(title_text=\"Quality Score (%)\", row=1, col=1)\n",
    "            fig.update_xaxes(title_text=\"Quality Score (%)\", row=2, col=1)\n",
    "            fig.update_xaxes(title_text=\"Inference Time (s)\", row=2, col=2)\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Interactive dashboard creation failed: {e}\")\n",
    "            print(\"üí° Install plotly with: !pip install plotly\")\n",
    "\n",
    "# Initialize the enhanced tester\n",
    "llm_tester = EnhancedLLMTester()\n",
    "print(\"‚úÖ Enhanced LLM Testing Framework Ready!\")\n",
    "print(\"üéØ Features:\")\n",
    "print(\"   ‚Ä¢ Uses real CSV dataset for testing\")\n",
    "print(\"   ‚Ä¢ 8 comprehensive quality indicator categories\")\n",
    "print(\"   ‚Ä¢ 97+ individual quality indicators\")\n",
    "print(\"   ‚Ä¢ Coherence and relevance scoring\")\n",
    "print(\"   ‚Ä¢ Detailed performance breakdown by fraud type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# üöÄ RUN THE ENHANCED LLM TESTING\n",
    "# This will test all models using REAL CSV DATA and comprehensive quality metrics\n",
    "\n",
    "print(\"üß™ Starting Enhanced LLM Testing with Real Dataset...\")\n",
    "print(\"‚ö†Ô∏è  This may take several minutes as we test multiple models\")\n",
    "print(\"üìä Enhanced Features:\")\n",
    "print(\"   ‚Ä¢ Uses real fraud detection dataset (CSV)\")\n",
    "print(\"   ‚Ä¢ 8 quality indicator categories with 97+ individual metrics\")\n",
    "print(\"   ‚Ä¢ Fraud keyword detection (19 indicators)\")\n",
    "print(\"   ‚Ä¢ Urgency manipulation recognition (12 indicators)\")\n",
    "print(\"   ‚Ä¢ Financial exploitation patterns (15 indicators)\")\n",
    "print(\"   ‚Ä¢ Action-oriented language analysis (13 indicators)\")\n",
    "print(\"   ‚Ä¢ Technical deception detection (13 indicators)\")\n",
    "print(\"   ‚Ä¢ Social engineering identification (12 indicators)\")\n",
    "print(\"   ‚Ä¢ Emotional manipulation patterns (13 indicators)\")\n",
    "print(\"   ‚Ä¢ Text coherence and relevance scoring\")\n",
    "\n",
    "# Run the enhanced comprehensive test\n",
    "llm_tester.run_comprehensive_test()\n",
    "\n",
    "# Generate the enhanced comparison report\n",
    "print(\"\\n\" + \"üéØ\" * 40)\n",
    "ranked_results = llm_tester.generate_comparison_report()\n",
    "\n",
    "# Save enhanced results for later reference\n",
    "results_summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'framework_version': 'enhanced_v2',\n",
    "    'dataset_source': 'real_csv_data',\n",
    "    'models_tested': len(llm_tester.results),\n",
    "    'successful_models': len([r for r in llm_tester.results.values() if r['load_success']]),\n",
    "    'test_cases': len(llm_tester.test_cases),\n",
    "    'quality_indicators_total': sum(len(indicators) for indicators in llm_tester.quality_indicators.values()),\n",
    "    'quality_categories': list(llm_tester.quality_indicators.keys()),\n",
    "    'detailed_results': llm_tester.results\n",
    "}\n",
    "\n",
    "print(f\"\\nüìà TESTING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Framework: Enhanced v2.0 with Real Data\")\n",
    "print(f\"Dataset: CSV with {len(llm_tester.test_cases)} real fraud cases\")\n",
    "print(f\"Quality Indicators: {results_summary['quality_indicators_total']} total across 8 categories\")\n",
    "print(f\"Models Tested: {results_summary['models_tested']}\")\n",
    "print(f\"Successful Tests: {results_summary['successful_models']}\")\n",
    "\n",
    "# Display final recommendation with enhanced insights\n",
    "if ranked_results:\n",
    "    best_model = ranked_results[0]\n",
    "    print(f\"\\nüéñÔ∏è  FINAL RECOMMENDATION (Based on Real Data)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ü•á Best Model: {best_model[0]}\")\n",
    "    print(f\"üìä Overall Quality: {best_model[1]['overall_quality']:.1f}%\")\n",
    "    print(f\"‚ö° Speed: {best_model[1]['avg_inference_time']:.2f}s average\")\n",
    "    print(f\"‚úÖ Reliability: {best_model[1]['success_rate']:.1f}% success rate\")\n",
    "    print(f\"üéØ Fraud Cases: {best_model[1]['fraud_tests_completed']} completed\")\n",
    "    \n",
    "    # Show top quality categories\n",
    "    if 'avg_quality_scores' in best_model[1]:\n",
    "        print(f\"\\nüèÜ Top Quality Scores:\")\n",
    "        quality_scores = best_model[1]['avg_quality_scores']\n",
    "        top_scores = sorted(\n",
    "            [(k, v) for k, v in quality_scores.items() if '_score' in k], \n",
    "            key=lambda x: x[1], reverse=True\n",
    "        )[:5]\n",
    "        \n",
    "        for score_name, score_value in top_scores:\n",
    "            clean_name = score_name.replace('_score', '').replace('_', ' ').title()\n",
    "            print(f\"   {clean_name:20s}: {score_value:.1f}%\")\n",
    "    \n",
    "    # Update the original reasoning model variable\n",
    "    recommended_model = best_model[0]\n",
    "    print(f\"\\nüí° To use this model, update your reasoning_model_name to: '{recommended_model}'\")\n",
    "    print(f\"üîÑ This recommendation is based on {len(llm_tester.test_cases)} real fraud examples\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No models completed testing successfully!\")\n",
    "\n",
    "print(f\"\\n‚úÖ Enhanced testing complete! Results include real dataset analysis.\")\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "print(f\"\\nüé® Creating Advanced Performance Visualizations...\")\n",
    "try:\n",
    "    llm_tester.create_performance_visualizations(save_figures=True)\n",
    "    print(\"‚úÖ All visualizations created successfully!\")\n",
    "    print(\"üìÅ Charts saved as PNG files for download\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Visualization creation failed: {e}\")\n",
    "    print(\"üí° Some models may not have completed testing successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# üéØ Advanced Model Analysis & Recommendations\n",
    "\n",
    "# Create a comprehensive comparison table\n",
    "if ranked_results:\n",
    "    print(\"üìä DETAILED MODEL COMPARISON TABLE\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    # Prepare data for comparison table\n",
    "    comparison_data = []\n",
    "    for name, results in llm_tester.results.items():\n",
    "        if results['load_success'] and results.get('success_rate', 0) > 0:\n",
    "            model_info = next((m for m in llm_tester.models_to_test if m['name'] == name), {})\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Model': name.split('/')[-1],\n",
    "                'Category': model_info.get('category', 'unknown'),\n",
    "                'Size': model_info.get('params', 'unknown'),\n",
    "                'Type': model_info.get('type', 'unknown'),\n",
    "                'Quality (%)': f\"{results.get('overall_quality', 0):.1f}\",\n",
    "                'Speed (s)': f\"{results.get('avg_inference_time', 0):.2f}\",\n",
    "                'Success Rate (%)': f\"{results.get('success_rate', 0):.1f}\",\n",
    "                'Fraud Cases': results.get('fraud_tests_completed', 0),\n",
    "                'Priority': model_info.get('priority', 'medium'),\n",
    "                'Expected Performance': model_info.get('expected_performance', 'unknown')\n",
    "            })\n",
    "    \n",
    "    # Create and display comparison DataFrame\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values('Quality (%)', ascending=False)\n",
    "    \n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Save comparison table\n",
    "    comparison_df.to_csv('llm_comparison_table.csv', index=False)\n",
    "    print(f\"\\nüíæ Comparison table saved as 'llm_comparison_table.csv'\")\n",
    "    \n",
    "    # Advanced recommendations by use case\n",
    "    print(f\"\\nüéØ ADVANCED RECOMMENDATIONS BY USE CASE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Best overall (quality + speed balance)\n",
    "    best_overall = max(llm_tester.results.items(), \n",
    "                      key=lambda x: (x[1].get('overall_quality', 0) * 0.7 + \n",
    "                                   (100 / (x[1].get('avg_inference_time', 1) + 0.1)) * 0.3)\n",
    "                      if x[1]['load_success'] else -1)\n",
    "    \n",
    "    # Fastest model\n",
    "    fastest = min([r for r in llm_tester.results.items() if r[1]['load_success']], \n",
    "                 key=lambda x: x[1].get('avg_inference_time', float('inf')))\n",
    "    \n",
    "    # Highest quality\n",
    "    highest_quality = max([r for r in llm_tester.results.items() if r[1]['load_success']], \n",
    "                         key=lambda x: x[1].get('overall_quality', 0))\n",
    "    \n",
    "    # Best open source\n",
    "    open_source_models = [r for r in llm_tester.results.items() \n",
    "                         if r[1]['load_success'] and any(keyword in r[0].lower() \n",
    "                         for keyword in ['eleutherai', 'deepseek', 'qwen', 'tinyllama'])]\n",
    "    if open_source_models:\n",
    "        best_oss = max(open_source_models, key=lambda x: x[1].get('overall_quality', 0))\n",
    "    else:\n",
    "        best_oss = None\n",
    "    \n",
    "    print(f\"ü•á Best Overall Balance: {best_overall[0]}\")\n",
    "    print(f\"   Quality: {best_overall[1].get('overall_quality', 0):.1f}% | Speed: {best_overall[1].get('avg_inference_time', 0):.2f}s\")\n",
    "    print(f\"   üí° Use for: Production environments requiring good quality and speed\")\n",
    "    \n",
    "    print(f\"\\n‚ö° Fastest Model: {fastest[0]}\")\n",
    "    print(f\"   Speed: {fastest[1].get('avg_inference_time', 0):.2f}s | Quality: {fastest[1].get('overall_quality', 0):.1f}%\")\n",
    "    print(f\"   üí° Use for: Real-time applications where speed is critical\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Highest Quality: {highest_quality[0]}\")\n",
    "    print(f\"   Quality: {highest_quality[1].get('overall_quality', 0):.1f}% | Speed: {highest_quality[1].get('avg_inference_time', 0):.2f}s\")\n",
    "    print(f\"   üí° Use for: Applications where explanation quality is most important\")\n",
    "    \n",
    "    if best_oss:\n",
    "        print(f\"\\nüåü Best Open Source: {best_oss[0]}\")\n",
    "        print(f\"   Quality: {best_oss[1].get('overall_quality', 0):.1f}% | Speed: {best_oss[1].get('avg_inference_time', 0):.2f}s\")\n",
    "        print(f\"   üí° Use for: Open source projects, research, or budget-conscious deployments\")\n",
    "    \n",
    "    # Model selection guidance\n",
    "    print(f\"\\nüìã MODEL SELECTION GUIDANCE\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"üéØ For Production Fraud Detection:\")\n",
    "    print(f\"   Primary: {best_overall[0]} (best balance)\")\n",
    "    print(f\"   Backup: {fastest[0]} (for high-traffic scenarios)\")\n",
    "    \n",
    "    print(\"\\nüî¨ For Research & Development:\")\n",
    "    if best_oss:\n",
    "        print(f\"   Recommended: {best_oss[0]} (open source)\")\n",
    "    print(f\"   Alternative: {highest_quality[0]} (maximum quality)\")\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è  Resource Considerations:\")\n",
    "    large_models = [r for r in llm_tester.results.items() \n",
    "                   if r[1]['load_success'] and any(m['name'] == r[0] and \n",
    "                   ('large' in m.get('size', '') or 'B' in m.get('params', '')) \n",
    "                   for m in llm_tester.models_to_test)]\n",
    "    \n",
    "    if large_models:\n",
    "        print(\"   Large models (>1B params) require significant GPU memory:\")\n",
    "        for name, results in large_models:\n",
    "            model_info = next((m for m in llm_tester.models_to_test if m['name'] == name), {})\n",
    "            print(f\"     ‚Ä¢ {name}: {model_info.get('params', 'unknown')} parameters\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No successful test results to analyze!\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(\"1. Review visualizations and comparison table\")\n",
    "print(\"2. Test selected model(s) with your specific fraud data\")\n",
    "print(\"3. Integrate chosen model into your fraud detection pipeline\")\n",
    "print(\"4. Monitor performance and adjust based on real-world usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# üîç Individual Model Deep Dive Testing\n",
    "# Test a specific model in detail to see actual reasoning outputs\n",
    "\n",
    "def test_single_model_detailed(model_name, test_cases=None):\n",
    "    \"\"\"Test a single model with detailed output analysis\"\"\"\n",
    "    \n",
    "    if test_cases is None:\n",
    "        test_cases = [\n",
    "            \"URGENT: Your PayPal account has been suspended. Click here to verify: http://fake-paypal.com\",\n",
    "            \"WARNING: Your computer is infected with 5 viruses! Call Microsoft: 1-800-FAKE-TECH\",\n",
    "            \"Congratulations! You've won $5000! Send $100 processing fee to claim your prize!\",\n",
    "            \"Your package from Amazon has been delivered and is waiting at your front door.\",\n",
    "            \"NOTICE: Your Social Security Number has been suspended. Call SSA: 1-800-FAKE-SSA\"\n",
    "        ]\n",
    "    \n",
    "    print(f\"üîç Deep Testing: {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Load the specific model\n",
    "        print(f\"üîÑ Loading {model_name}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        test_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            max_length=300,\n",
    "            pad_token_id=50256\n",
    "        )\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"‚úÖ Loaded in {load_time:.2f}s\")\n",
    "        \n",
    "        print(f\"\\nüìù Testing {len(test_cases)} fraud examples:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for i, test_text in enumerate(test_cases, 1):\n",
    "            print(f\"\\nüß™ Test {i}: {test_text[:50]}...\")\n",
    "            \n",
    "            # Create reasoning prompt\n",
    "            prompt = f\"This message appears to be fraudulent because\"\n",
    "            \n",
    "            # Generate reasoning\n",
    "            start_inference = time.time()\n",
    "            try:\n",
    "                response = test_pipeline(\n",
    "                    prompt,\n",
    "                    max_length=200,\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    return_full_text=False\n",
    "                )\n",
    "                \n",
    "                inference_time = time.time() - start_inference\n",
    "                reasoning = response[0]['generated_text'] if response else \"No response\"\n",
    "                \n",
    "                print(f\"   ‚ö° Time: {inference_time:.2f}s\")\n",
    "                print(f\"   üß† Reasoning: {reasoning}\")\n",
    "                \n",
    "                # Simple quality assessment\n",
    "                quality_keywords = ['scam', 'fraud', 'suspicious', 'fake', 'phishing', 'deceptive', 'malicious']\n",
    "                quality_score = sum(1 for word in quality_keywords if word in reasoning.lower())\n",
    "                print(f\"   üìä Quality indicators found: {quality_score}/{len(quality_keywords)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error: {str(e)}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del test_pipeline\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Deep testing of {model_name} complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to test {model_name}: {str(e)}\")\n",
    "\n",
    "# Example: Test a specific model in detail\n",
    "print(\"üéØ Choose a model to test in detail:\")\n",
    "print(\"1. microsoft/DialoGPT-medium (current default)\")\n",
    "print(\"2. gpt2 (standard GPT-2)\")  \n",
    "print(\"3. distilgpt2 (faster, smaller)\")\n",
    "print(\"4. microsoft/DialoGPT-small (smaller conversational)\")\n",
    "\n",
    "# Test the current default model\n",
    "test_model = \"microsoft/DialoGPT-medium\"\n",
    "print(f\"\\nüöÄ Testing {test_model} in detail...\")\n",
    "test_single_model_detailed(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# üîÑ Easy Model Switching Based on Test Results\n",
    "# Use this cell to switch to your preferred model after testing\n",
    "\n",
    "def switch_reasoning_model(new_model_name):\n",
    "    \"\"\"Switch to a different reasoning model\"\"\"\n",
    "    global reasoning_pipe, reasoning_model_name\n",
    "    \n",
    "    print(f\"üîÑ Switching from {reasoning_model_name} to {new_model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Clean up current model\n",
    "        if 'reasoning_pipe' in globals():\n",
    "            del reasoning_pipe\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        # Load new model\n",
    "        start_time = time.time()\n",
    "        reasoning_pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=new_model_name,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            max_length=512,\n",
    "            pad_token_id=50256\n",
    "        )\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        reasoning_model_name = new_model_name\n",
    "        \n",
    "        print(f\"‚úÖ Successfully switched to {new_model_name} in {load_time:.2f}s\")\n",
    "        \n",
    "        # Test the new model\n",
    "        test_prompt = \"This text appears to be a scam because\"\n",
    "        test_response = reasoning_pipe(test_prompt, max_length=60, num_return_sequences=1)\n",
    "        print(f\"üß™ Test successful: {test_response[0]['generated_text'][:100]}...\")\n",
    "        \n",
    "        # Update the reasoning engine\n",
    "        global local_reasoning_engine\n",
    "        local_reasoning_engine = LocalFraudReasoningEngine(reasoning_pipe)\n",
    "        print(f\"üîÑ Local reasoning engine updated with new model\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error switching to {new_model_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# üìä Model Performance Summary (update after running tests)\n",
    "model_recommendations = {\n",
    "    \"Best Overall\": \"microsoft/DialoGPT-medium\",  # Update based on test results\n",
    "    \"Fastest\": \"distilgpt2\",\n",
    "    \"Most Conversational\": \"microsoft/DialoGPT-medium\", \n",
    "    \"Smallest\": \"distilgpt2\",\n",
    "    \"Best Quality\": \"gpt2\"  # Update based on test results\n",
    "}\n",
    "\n",
    "print(\"üéØ Model Recommendations (update after testing):\")\n",
    "for category, model in model_recommendations.items():\n",
    "    print(f\"   {category}: {model}\")\n",
    "\n",
    "print(f\"\\nüí° Current model: {reasoning_model_name}\")\n",
    "print(\"üîÑ To switch models, run: switch_reasoning_model('model-name')\")\n",
    "\n",
    "# Example: Uncomment to switch to a different model\n",
    "# switch_reasoning_model(\"distilgpt2\")  # Switch to faster model\n",
    "# switch_reasoning_model(\"gpt2\")        # Switch to standard GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load Your Trained DistilBERT Fraud Detection Model\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "print(\"üì¶ Loading Your Trained DistilBERT Model...\")\n",
    "\n",
    "# KAGGLE PATHS - Update these to match your uploaded dataset name\n",
    "MODEL_PATH = '/kaggle/input/distilbert/transformers/default/1/distilbert_model'  # Update this path to your dataset\n",
    "TOKENIZER_PATH = '/kaggle/input/distilbert/transformers/default/1/distilbert_tokenizer'  # Update this path to your dataset\n",
    "\n",
    "\n",
    "\n",
    "# Class labels (must match your training - alphabetical order)\n",
    "CLASS_LABELS = [\n",
    "    'job_scam',\n",
    "    'legitimate', \n",
    "    'phishing',\n",
    "    'popup_scam',\n",
    "    'refund_scam',\n",
    "    'reward_scam',\n",
    "    'sms_spam',\n",
    "    'ssn_scam',\n",
    "    'tech_support_scam'\n",
    "]\n",
    "\n",
    "print(f\"üîç Checking paths:\")\n",
    "print(f\"   Model: {MODEL_PATH}\")\n",
    "print(f\"   Tokenizer: {TOKENIZER_PATH}\")\n",
    "\n",
    "# Check if paths exist\n",
    "import os\n",
    "model_exists = os.path.exists(MODEL_PATH)\n",
    "tokenizer_exists = os.path.exists(TOKENIZER_PATH)\n",
    "print(f\"   Model exists: {model_exists}\")\n",
    "print(f\"   Tokenizer exists: {tokenizer_exists}\")\n",
    "\n",
    "if not model_exists or not tokenizer_exists:\n",
    "    print(\"\\n‚ùå Model files not found!\")\n",
    "    print(\"üìÅ Make sure you've uploaded your model files to Kaggle:\")\n",
    "    print(\"   1. Go to Kaggle Datasets\")\n",
    "    print(\"   2. Create a new dataset\")\n",
    "    print(\"   3. Upload your 'distilbert_model/' and 'distilbert_tokenizer/' folders\")\n",
    "    print(\"   4. Update the paths above to match your dataset name\")\n",
    "    print(\"   5. Add your dataset as input to this notebook\")\n",
    "    fraud_model = None\n",
    "    fraud_tokenizer = None\n",
    "else:\n",
    "    try:\n",
    "        # Load your trained model and tokenizer\n",
    "        print(\"üîÑ Loading tokenizer...\")\n",
    "        fraud_tokenizer = DistilBertTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "        \n",
    "        print(\"üîÑ Loading model...\")\n",
    "        fraud_model = DistilBertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "        \n",
    "        # Move to GPU for faster inference\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        fraud_model.to(device)\n",
    "        fraud_model.eval()\n",
    "        \n",
    "        print(f\"‚úÖ DistilBERT model loaded successfully!\")\n",
    "        print(f\"üéØ Device: {device}\")\n",
    "        print(f\"üìã Classes: {len(CLASS_LABELS)} fraud types + legitimate\")\n",
    "        print(f\"üè∑Ô∏è  Labels: {CLASS_LABELS}\")\n",
    "        \n",
    "        # Quick test to verify model works\n",
    "        test_text = \"Your package has been successfully delivered and left at your front door. If you do not locate the parcel, please check with members of your household or nearby areas where it may have been placed for security.\"\n",
    "        test_encoding = fraud_tokenizer(\n",
    "            test_text,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        test_input_ids = test_encoding['input_ids'].to(device)\n",
    "        test_attention_mask = test_encoding['attention_mask'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_outputs = fraud_model(input_ids=test_input_ids, attention_mask=test_attention_mask)\n",
    "            test_probabilities = torch.softmax(test_outputs.logits, dim=1).cpu().numpy()[0]\n",
    "            test_predicted_class = CLASS_LABELS[np.argmax(test_probabilities)]\n",
    "        \n",
    "        print(f\"\\nüß™ Model test - '{test_text}':\")\n",
    "        print(f\"   Predicted: {test_predicted_class} ({test_probabilities[np.argmax(test_probabilities)]:.2%})\")\n",
    "        print(\"   ‚úÖ Model is working!\" if test_predicted_class == 'legitimate' else f\"   ‚ö†Ô∏è Unexpected result: {test_predicted_class}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        print(\"üîß This will cause the classification to use demo mode.\")\n",
    "        fraud_model = None\n",
    "        fraud_tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# üîß Local Reasoning Engine Configuration\n",
    "\n",
    "This section sets up the local reasoning engine that generates explanations for fraud classifications using the language model we loaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Local Reasoning Engine Configuration\n",
    "class LocalFraudReasoningEngine:\n",
    "    \"\"\"\n",
    "    Local reasoning engine that generates explanations without API calls\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reasoning_pipeline):\n",
    "        self.reasoning_pipe = reasoning_pipeline\n",
    "        self.min_confidence = 0.5\n",
    "        \n",
    "        # Scam type descriptions for better reasoning\n",
    "        self.scam_descriptions = {\n",
    "            'phishing': {\n",
    "                'description': 'Attempts to steal sensitive information like passwords, credit card numbers, or personal data',\n",
    "                'indicators': ['urgent action required', 'verify account', 'click here', 'suspicious links', 'fake sender']\n",
    "            },\n",
    "            'popup_scam': {\n",
    "                'description': 'Fake popup messages claiming virus infections or system issues',\n",
    "                'indicators': ['virus detected', 'system error', 'immediate action', 'fake technical alerts']\n",
    "            },\n",
    "            'sms_spam': {\n",
    "                'description': 'Unwanted promotional or fraudulent text messages',\n",
    "                'indicators': ['unsolicited offers', 'prize claims', 'urgent responses', 'suspicious phone numbers']\n",
    "            },\n",
    "            'reward_scam': {\n",
    "                'description': 'False promises of rewards, prizes, or free items',\n",
    "                'indicators': ['congratulations', 'you have won', 'free gift', 'claim now', 'limited time']\n",
    "            },\n",
    "            'tech_support_scam': {\n",
    "                'description': 'Fake technical support claiming to fix computer problems',\n",
    "                'indicators': ['computer infected', 'microsoft support', 'remote access', 'technical issues']\n",
    "            },\n",
    "            'refund_scam': {\n",
    "                'description': 'Fake refund notifications or requests for payment information',\n",
    "                'indicators': ['refund available', 'payment failed', 'update payment', 'billing issue']\n",
    "            },\n",
    "            'ssn_scam': {\n",
    "                'description': 'Attempts to steal Social Security Numbers or similar personal identifiers',\n",
    "                'indicators': ['SSN verification', 'social security', 'identity verification', 'government agency']\n",
    "            },\n",
    "            'job_scam': {\n",
    "                'description': 'Fake job offers or employment opportunities',\n",
    "                'indicators': ['work from home', 'easy money', 'no experience required', 'guaranteed income']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.stats = {\n",
    "            'total_processed': 0,\n",
    "            'reasoning_generated': 0,\n",
    "            'skipped_legitimate': 0,\n",
    "            'skipped_low_confidence': 0\n",
    "        }\n",
    "        \n",
    "    def should_generate_reasoning(self, predicted_label, confidence):\n",
    "        \"\"\"Determine if reasoning should be generated\"\"\"\n",
    "        return predicted_label != 'legitimate' and confidence >= self.min_confidence\n",
    "    \n",
    "    def generate_local_reasoning(self, text, predicted_label, confidence, all_predictions):\n",
    "        \"\"\"Generate enhanced reasoning using local language model\"\"\"\n",
    "        scam_info = self.scam_descriptions.get(predicted_label, {\n",
    "            'description': 'Unknown scam type',\n",
    "            'indicators': []\n",
    "        })\n",
    "        \n",
    "        # Enhanced reasoning without relying on language model generation\n",
    "        # Analyze text content directly\n",
    "        text_lower = text.lower()\n",
    "        detected_indicators = []\n",
    "        \n",
    "        # Check for specific indicators in the text\n",
    "        for indicator in scam_info['indicators']:\n",
    "            if any(word in text_lower for word in indicator.split()):\n",
    "                detected_indicators.append(indicator)\n",
    "        \n",
    "        # Add common fraud patterns\n",
    "        urgent_words = ['urgent', 'immediate', 'now', 'quickly', 'hurry', 'expires']\n",
    "        if any(word in text_lower for word in urgent_words):\n",
    "            detected_indicators.append('urgent language to pressure victims')\n",
    "            \n",
    "        money_words = ['$', 'money', 'prize', 'won', 'claim', 'free', 'gift']\n",
    "        if any(word in text_lower for word in money_words):\n",
    "            detected_indicators.append('financial incentives or rewards')\n",
    "            \n",
    "        action_words = ['click', 'call', 'text', 'visit', 'send', 'verify']\n",
    "        if any(word in text_lower for word in action_words):\n",
    "            detected_indicators.append('requests for immediate action')\n",
    "            \n",
    "        suspicious_elements = ['suspicious links', 'phone numbers', 'email addresses']\n",
    "        if 'http' in text_lower or '@' in text_lower or any(char.isdigit() for char in text):\n",
    "            detected_indicators.append('suspicious contact information')\n",
    "        \n",
    "        # Create comprehensive reasoning\n",
    "        reasoning_parts = []\n",
    "        reasoning_parts.append(f\"This text was classified as {predicted_label} with {confidence:.1%} confidence.\")\n",
    "        reasoning_parts.append(f\"\\n{scam_info['description']}\")\n",
    "        \n",
    "        if detected_indicators:\n",
    "            reasoning_parts.append(f\"\\nKey fraud indicators detected:\")\n",
    "            for i, indicator in enumerate(detected_indicators[:4], 1):  # Limit to top 4\n",
    "                reasoning_parts.append(f\"‚Ä¢ {indicator}\")\n",
    "        \n",
    "        # Add context about why this is dangerous\n",
    "        danger_context = {\n",
    "            'phishing': 'This could lead to identity theft and financial loss.',\n",
    "            'sms_spam': 'This could lead to unwanted charges and privacy violations.',\n",
    "            'reward_scam': 'This could lead to financial scams and personal data theft.',\n",
    "            'tech_support_scam': 'This could lead to remote access scams and financial fraud.',\n",
    "            'job_scam': 'This could lead to advance fee fraud and identity theft.',\n",
    "            'popup_scam': 'This could lead to malware installation and system compromise.',\n",
    "            'refund_scam': 'This could lead to payment fraud and account takeover.',\n",
    "            'ssn_scam': 'This could lead to identity theft and government impersonation fraud.'\n",
    "        }\n",
    "        \n",
    "        if predicted_label in danger_context:\n",
    "            reasoning_parts.append(f\"\\n‚ö†Ô∏è Risk: {danger_context[predicted_label]}\")\n",
    "        \n",
    "        # Add confidence context\n",
    "        if confidence > 0.9:\n",
    "            reasoning_parts.append(f\"\\nHigh confidence ({confidence:.1%}) indicates strong fraud patterns.\")\n",
    "        elif confidence > 0.7:\n",
    "            reasoning_parts.append(f\"Moderate confidence ({confidence:.1%}) suggests probable fraud patterns.\")\n",
    "        \n",
    "        return '\\n'.join(reasoning_parts)\n",
    "\n",
    "# Initialize the local reasoning engine\n",
    "local_reasoning_engine = LocalFraudReasoningEngine(reasoning_pipe)\n",
    "print(\"‚úÖ Local reasoning engine initialized with enhanced analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Fraud Classification and Reasoning Functions\n",
    "def classify_text(text, max_length=128):\n",
    "    \"\"\"Classify text using the loaded DistilBERT model\"\"\"\n",
    "    if fraud_model is None or fraud_tokenizer is None:\n",
    "        print(\"üö® WARNING: Model not loaded! Using demo mode.\")\n",
    "        print(\"üí° Please check model paths and ensure your dataset is properly uploaded to Kaggle.\")\n",
    "        print(\"üîÑ Demo mode always returns 'phishing' - this is NOT real classification!\")\n",
    "        \n",
    "        # Return a more realistic demo that varies by text content\n",
    "        text_lower = text.lower()\n",
    "        if any(word in text_lower for word in ['thanks', 'meeting', 'delivered', 'shipped', 'hi ', 'hello']):\n",
    "            demo_label = 'legitimate'\n",
    "            demo_conf = 0.75\n",
    "        elif any(word in text_lower for word in ['urgent', 'click', 'verify', 'suspended']):\n",
    "            demo_label = 'phishing'\n",
    "            demo_conf = 0.85\n",
    "        elif any(word in text_lower for word in ['won', 'prize', 'congratulations']):\n",
    "            demo_label = 'reward_scam'\n",
    "            demo_conf = 0.80\n",
    "        else:\n",
    "            demo_label = 'phishing'  # Default fallback\n",
    "            demo_conf = 0.70\n",
    "            \n",
    "        return {\n",
    "            'text': text,\n",
    "            'predicted_label': demo_label,\n",
    "            'confidence': demo_conf,\n",
    "            'all_predictions': {\n",
    "                demo_label: demo_conf,\n",
    "                'legitimate': 0.15 if demo_label != 'legitimate' else demo_conf,\n",
    "                'phishing': 0.10 if demo_label != 'phishing' else demo_conf,\n",
    "                'reward_scam': 0.05,\n",
    "                'tech_support_scam': 0.05\n",
    "            },\n",
    "            'demo_mode': True\n",
    "        }\n",
    "    \n",
    "    # Real model classification\n",
    "    try:\n",
    "        # Tokenize input\n",
    "        encoding = fraud_tokenizer(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = fraud_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "            predicted_class_id = np.argmax(probabilities)\n",
    "        \n",
    "        # Format results\n",
    "        predicted_label = CLASS_LABELS[predicted_class_id]\n",
    "        confidence = float(probabilities[predicted_class_id])\n",
    "        \n",
    "        all_predictions = {\n",
    "            CLASS_LABELS[i]: float(probabilities[i]) \n",
    "            for i in range(len(CLASS_LABELS))\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'predicted_label': predicted_label,\n",
    "            'confidence': confidence,\n",
    "            'all_predictions': all_predictions,\n",
    "            'demo_mode': False\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during classification: {e}\")\n",
    "        return {\n",
    "            'text': text,\n",
    "            'predicted_label': 'error',\n",
    "            'confidence': 0.0,\n",
    "            'all_predictions': {'error': 1.0},\n",
    "            'demo_mode': True\n",
    "        }\n",
    "\n",
    "def analyze_with_local_reasoning(text):\n",
    "    \"\"\"Complete analysis: classification + local reasoning\"\"\"\n",
    "    # Step 1: Classify the text\n",
    "    classification_result = classify_text(text)\n",
    "    \n",
    "    # Check if we're in demo mode and warn user\n",
    "    if classification_result.get('demo_mode', False):\n",
    "        if classification_result['predicted_label'] == 'error':\n",
    "            print(\"üö® CLASSIFICATION ERROR - Please check your model setup!\")\n",
    "        else:\n",
    "            print(\"üö® DEMO MODE ACTIVE - Results are simulated, not real model predictions!\")\n",
    "    \n",
    "    # Step 2: Generate local reasoning (only for non-legitimate classifications)\n",
    "    if local_reasoning_engine.should_generate_reasoning(\n",
    "        classification_result['predicted_label'], \n",
    "        classification_result['confidence']\n",
    "    ):\n",
    "        reasoning = local_reasoning_engine.generate_local_reasoning(\n",
    "            text=classification_result['text'],\n",
    "            predicted_label=classification_result['predicted_label'],\n",
    "            confidence=classification_result['confidence'],\n",
    "            all_predictions=classification_result['all_predictions']\n",
    "        )\n",
    "        \n",
    "        local_reasoning_engine.stats['reasoning_generated'] += 1\n",
    "        skip_reason = None\n",
    "        reasoning_generated = True\n",
    "    else:\n",
    "        if classification_result['predicted_label'] == 'legitimate':\n",
    "            skip_reason = 'legitimate_classification'\n",
    "            local_reasoning_engine.stats['skipped_legitimate'] += 1\n",
    "        elif classification_result['predicted_label'] == 'error':\n",
    "            skip_reason = 'classification_error'\n",
    "            local_reasoning_engine.stats['skipped_low_confidence'] += 1\n",
    "        else:\n",
    "            skip_reason = f\"low_confidence_{classification_result['confidence']:.2f}\"\n",
    "            local_reasoning_engine.stats['skipped_low_confidence'] += 1\n",
    "        \n",
    "        reasoning = None\n",
    "        reasoning_generated = False\n",
    "    \n",
    "    local_reasoning_engine.stats['total_processed'] += 1\n",
    "    \n",
    "    return {\n",
    "        **classification_result,\n",
    "        'reasoning': reasoning,\n",
    "        'reasoning_generated': reasoning_generated,\n",
    "        'skip_reason': skip_reason,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def print_analysis_result(result):\n",
    "    \"\"\"Pretty print analysis result\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîç LOCAL FRAUD DETECTION + REASONING ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Show demo mode warning prominently\n",
    "    if result.get('demo_mode', False):\n",
    "        print(\"üö® DEMO MODE ACTIVE - NOT REAL MODEL PREDICTIONS!\")\n",
    "        print(\"üìã Upload your trained model to Kaggle and update paths to get real results\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìù Original Text:\")\n",
    "    print(f\"   {result['text']}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Classification:\")\n",
    "    print(f\"   Label: {result['predicted_label']}\")\n",
    "    print(f\"   Confidence: {result['confidence']:.2%}\")\n",
    "    \n",
    "    if not result.get('demo_mode', False):\n",
    "        print(f\"\\nüìä All Predictions:\")\n",
    "        for label, prob in sorted(result['all_predictions'].items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"   {label}: {prob:.2%}\")\n",
    "    \n",
    "    if result['reasoning_generated']:\n",
    "        print(f\"\\nüß† Local AI Reasoning:\")\n",
    "        print(\"   \" + result['reasoning'].replace('\\n', '\\n   '))\n",
    "    else:\n",
    "        print(f\"\\n‚è≠Ô∏è  Reasoning Skipped: {result['skip_reason']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"‚úÖ Classification and reasoning functions ready!\")\n",
    "print(\"üöÄ Ready to analyze texts with local AI reasoning!\")\n",
    "print(\"üí° Note: Make sure to upload your trained model to Kaggle for real predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Sample Tests - Try Different Fraud Types\n",
    "\n",
    "Let's test the local reasoning system with various types of fraudulent and legitimate messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Sample Test Cases for Different Fraud Types\n",
    "sample_texts = [\n",
    "    {\n",
    "        'category': 'Phishing Attack',\n",
    "        'text': \"URGENT: Your PayPal account has been suspended due to suspicious activity. Click here immediately to verify your information and restore access: http://paypal-verification-secure.fraudsite.com\"\n",
    "    },\n",
    "    {\n",
    "        'category': 'Tech Support Scam', \n",
    "        'text': \"WARNING: Your computer is infected with 5 viruses! Your files will be deleted in 24 hours. Call Microsoft Support immediately at 1-800-555-SCAM. Don't restart your computer or you'll lose everything!\"\n",
    "    },\n",
    "    {\n",
    "        'category': 'Reward Scam',\n",
    "        'text': \"üéâ CONGRATULATIONS! üéâ You've been selected as our LUCKY WINNER for a $1000 Amazon gift card! You're one of only 3 winners today! Claim your prize now by clicking this link and entering your credit card info for verification. Hurry, expires in 1 hour!\"\n",
    "    },\n",
    "    {\n",
    "        'category': 'Job Scam',\n",
    "        'text': \"Amazing work from home opportunity! Earn $5000/week working just 2 hours per day! No experience required! Just send $99 registration fee and start earning today! Guaranteed income or money back!\"\n",
    "    },\n",
    "    {\n",
    "        'category': 'SMS Spam',\n",
    "        'text': \"FREE iPhone 15 Pro! You have been randomly selected as a winner. Text CLAIM to 12345 or visit bit.ly/freeiphone15winner to get your prize. Message and data rates may apply. Text STOP to opt out.\"\n",
    "    },\n",
    "    {\n",
    "        'category': 'Legitimate Message',\n",
    "        'text': \"Hi Sarah, thank you for your order #12345. Your package has been shipped and will arrive within 3-5 business days. You can track your shipment using the tracking number provided in your confirmation email. Have a great day!\"\n",
    "    },\n",
    "    {\n",
    "        'category': 'SSN Scam',\n",
    "        'text': \"IMPORTANT NOTICE: Your Social Security Number has been suspended due to suspicious illegal activity. Call the SSA office immediately at 1-800-555-FAKE to verify your identity and reactivate your SSN. Failure to respond will result in arrest.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing Local AI Reasoning on Sample Fraud Types\")\n",
    "\n",
    "for i, sample in enumerate(sample_texts):\n",
    "    print(f\"\\nüéØ Test {i+1}: {sample['category']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result = analyze_with_local_reasoning(sample['text'])\n",
    "    print_analysis_result(result)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"Total Processed: {local_reasoning_engine.stats['total_processed']}\")\n",
    "print(f\"Reasoning Generated: {local_reasoning_engine.stats['reasoning_generated']}\")\n",
    "print(f\"Legitimate (Skipped): {local_reasoning_engine.stats['skipped_legitimate']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù Interactive Text Analysis\n",
    "\n",
    "Enter your own text below to analyze with the local fraud detection + reasoning system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Interactive Text Analysis\n",
    "# Change the text below to analyze your own messages!\n",
    "\n",
    "your_text = \"Congratulations! You've won $1 million! Send your bank details to claim your prize!\"\n",
    "\n",
    "# Analyze your custom text\n",
    "print(\"üîç Analyzing Your Custom Text...\")\n",
    "custom_result = analyze_with_local_reasoning(your_text.strip())\n",
    "print_analysis_result(custom_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Batch Processing - Analyze Multiple Texts\n",
    "\n",
    "Upload a CSV file or analyze multiple texts at once with local reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Batch Processing with Local Reasoning\n",
    "def batch_analyze_texts(texts, save_results=True):\n",
    "    \"\"\"Analyze multiple texts and generate local reasoning\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"üîÑ Processing {len(texts)} texts...\")\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        if i % 5 == 0:  # Only print every 5th item to reduce clutter\n",
    "            print(f\"Progress: {i+1}/{len(texts)}\")\n",
    "        \n",
    "        result = analyze_with_local_reasoning(text)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Small delay to avoid overwhelming the local model\n",
    "        time.sleep(0.2)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    df_results = pd.DataFrame([\n",
    "        {\n",
    "            'text': r['text'][:100] + '...' if len(r['text']) > 100 else r['text'],\n",
    "            'predicted_label': r['predicted_label'],\n",
    "            'confidence': r['confidence'],\n",
    "            'reasoning_generated': r['reasoning_generated'],\n",
    "            'reasoning': r['reasoning'][:200] + '...' if r['reasoning'] and len(r['reasoning']) > 200 else r['reasoning'],\n",
    "            'timestamp': r['timestamp']\n",
    "        }\n",
    "        for r in results\n",
    "    ])\n",
    "    \n",
    "    if save_results:\n",
    "        # Save results to CSV\n",
    "        output_file = f'fraud_analysis_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        df_results.to_csv(output_file, index=False)\n",
    "        print(f\"üíæ Results saved to: {output_file}\")\n",
    "    \n",
    "    return results, df_results\n",
    "\n",
    "# Example batch processing\n",
    "batch_texts = [\n",
    "    \"Your account will be closed unless you verify immediately!\",\n",
    "    \"Hi John, thanks for the great meeting today. Let's follow up next week.\",\n",
    "    \"You've won a free vacation! Call now to claim your prize!\",\n",
    "    \"Your package has been delivered to your front door.\",\n",
    "    \"URGENT: Your social security number has been compromised!\"\n",
    "]\n",
    "\n",
    "print(\"üìä Batch Analysis with Local Reasoning\")\n",
    "batch_results, batch_df = batch_analyze_texts(batch_texts)\n",
    "\n",
    "print(\"\\nüìà Batch Analysis Summary:\")\n",
    "fraud_count = (batch_df['predicted_label'] != 'legitimate').sum()\n",
    "reasoning_count = batch_df['reasoning_generated'].sum()\n",
    "print(f\"Fraud detected: {fraud_count}/{len(batch_df)}\")\n",
    "print(f\"Reasoning generated: {reasoning_count}/{len(batch_df)}\")\n",
    "\n",
    "# Display sample results\n",
    "display(batch_df.head())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
