{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1003e825",
   "metadata": {},
   "source": [
    "# NLP Fraud Detection Baseline Model\n",
    "\n",
    "This notebook implements a comprehensive baseline model for fraud and scam detection using Natural Language Processing techniques. We'll build and compare multiple approaches from traditional machine learning to modern transformer-based models.\n",
    "\n",
    "## üéØ Objectives\n",
    "1. Build traditional ML baselines (TF-IDF + Logistic Regression, SVM)\n",
    "2. Implement BERT-based classification\n",
    "3. Evaluate and compare model performance\n",
    "4. Provide a foundation for more advanced fraud detection systems\n",
    "\n",
    "## üìä Dataset\n",
    "We'll start with synthetic data and show how to adapt to real datasets like SMS Spam Collection, phishing emails, etc.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54843f8",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Let's start by importing all the necessary libraries for our fraud detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687cb6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Text processing and NLP\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")\n",
    "print(f\"ü§ñ Scikit-learn version:\", end=\" \")\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14416a5a",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset\n",
    "\n",
    "We'll create a comprehensive dataset with various types of fraud and legitimate messages. In a real project, you would load your actual dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921a8fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fraud_dataset():\n",
    "    \"\"\"\n",
    "    Load fraud/scam data from CSV dataset or create sample data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv('final_fraud_detection_dataset.csv', nrows=1000)  # Subset for notebook\n",
    "        print(f\"Loaded dataset with {len(df)} samples (subset for notebook)\")\n",
    "        \n",
    "        # Map binary_label to string labels\n",
    "        df['label'] = df['binary_label'].map({1: 'fraud', 0: 'normal'})\n",
    "        \n",
    "        # Select relevant columns and rename\n",
    "        df = df[['text', 'label']].copy()\n",
    "        df.columns = ['message', 'label']\n",
    "        \n",
    "        print(f\"Dataset prepared with {len(df)} samples\")\n",
    "        print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: final_fraud_detection_dataset.csv not found.\")\n",
    "        print(\"Falling back to sample data...\")\n",
    "        return _create_sample_dataset()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        print(\"Falling back to sample data...\")\n",
    "        return _create_sample_dataset()\n",
    "\n",
    "def _create_sample_dataset():\n",
    "    \"\"\"\n",
    "    Create a comprehensive fraud detection dataset with various types of fraud and legitimate messages (fallback)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fraud/Scam messages\n",
    "    fraud_messages = [\n",
    "        # Phishing/Account security scams\n",
    "        \"URGENT! Your account will be suspended. Click here to verify: suspicious-link.com\",\n",
    "        \"ALERT: Suspicious activity detected on your account. Verify your identity now or face suspension\",\n",
    "        \"Your credit card has been charged $500. If this wasn't you, click here immediately\",\n",
    "        \"Bank security alert! Update your information immediately to prevent account closure\",\n",
    "        \"PayPal Security: Your account is limited. Click to restore access now\",\n",
    "        \"Amazon: Your account has been compromised. Verify details to secure your account\",\n",
    "        \n",
    "        # Prize/Lottery scams\n",
    "        \"Congratulations! You've won $10,000 in our daily draw! Send your bank details to claim\",\n",
    "        \"WINNER! You've been selected for a $50,000 cash prize! Click to claim now\",\n",
    "        \"You have inherited $2 million from a distant relative. Send processing fee to claim\",\n",
    "        \"Lottery Commission: You've won ‚Ç¨1,000,000! Pay the processing fee to receive your prize\",\n",
    "        \n",
    "        # Investment/Get-rich-quick schemes\n",
    "        \"Limited time offer! Make $5000 weekly working from home! No experience needed\",\n",
    "        \"Guaranteed 500% returns in 30 days! This investment opportunity won't last\",\n",
    "        \"Earn $1000 daily with our proven system! Join thousands of successful members\",\n",
    "        \"URGENT Investment Alert: Double your money in 24 hours with crypto trading\",\n",
    "        \n",
    "        # Romance/Advance fee scams\n",
    "        \"Hello dear, I'm a soldier in Afghanistan and need help transferring my funds\",\n",
    "        \"My love, I'm stuck in another country and need money for travel expenses\",\n",
    "        \"Beautiful, I have fallen in love with you. I need your help with some funds\",\n",
    "        \n",
    "        # Tech support scams\n",
    "        \"Microsoft Alert: Your computer is infected with 5 viruses. Call now for support\",\n",
    "        \"Apple Security: Your iPhone has been hacked. Download our security software now\",\n",
    "        \"Tech Support: Your computer will be disabled unless you call this number immediately\",\n",
    "        \n",
    "        # Job/Employment scams\n",
    "        \"Congratulations! You've been selected for a high-paying remote job. Send $200 for training materials\",\n",
    "        \"Work from home opportunity! Earn $300/day processing payments for our company\",\n",
    "        \n",
    "        # Online shopping/Payment scams\n",
    "        \"Your package is delayed. Pay additional $50 shipping fees to receive your order\",\n",
    "        \"eBay Alert: Complete your payment verification or your purchase will be cancelled\",\n",
    "        \"Your order cannot be delivered. Pay customs fee of $75 to release your package\",\n",
    "        \n",
    "        # Debt collection scams\n",
    "        \"FINAL NOTICE: Pay your outstanding debt of $1,500 immediately or face legal action\",\n",
    "        \"Legal Department: You owe $2,000 in unpaid taxes. Pay now to avoid arrest\",\n",
    "        \"Collection Agency: Settle your debt of $800 today or we'll seize your assets\"\n",
    "    ]\n",
    "    \n",
    "    # Legitimate messages\n",
    "    normal_messages = [\n",
    "        # Personal communications\n",
    "        \"Hey, are we still meeting for lunch tomorrow at the usual place?\",\n",
    "        \"Thanks for your help with the project presentation yesterday\",\n",
    "        \"Happy birthday! Hope you have a wonderful day with family and friends\",\n",
    "        \"How was your weekend? Did you enjoy the concert you mentioned?\",\n",
    "        \"Let me know if you need any assistance with the quarterly report\",\n",
    "        \"Great job on the presentation! The client seemed very impressed\",\n",
    "        \"Can we reschedule our meeting to next week? Something urgent came up\",\n",
    "        \n",
    "        # Business communications\n",
    "        \"The meeting has been rescheduled to 3 PM in conference room B\",\n",
    "        \"Please review the attached document and provide your feedback by Friday\",\n",
    "        \"Reminder: Your monthly team meeting is scheduled for this Thursday\",\n",
    "        \"Conference call with the client is set for next Tuesday at 2 PM\",\n",
    "        \"The quarterly results look good. Let's discuss them in our next meeting\",\n",
    "        \"Please submit your expense reports by the end of this week\",\n",
    "        \n",
    "        # Service notifications\n",
    "        \"Your appointment with Dr. Smith is confirmed for Friday at 10 AM\",\n",
    "        \"Reminder: Your library books are due next Tuesday\",\n",
    "        \"Your prescription is ready for pickup at the pharmacy\",\n",
    "        \"Thank you for your recent purchase. Your order will arrive in 3-5 business days\",\n",
    "        \"Your flight to New York has been delayed by 30 minutes due to weather\",\n",
    "        \n",
    "        # Social and casual\n",
    "        \"The weather is great today, perfect for a walk in the park\",\n",
    "        \"The new restaurant downtown has excellent reviews. Want to try it?\",\n",
    "        \"Movie night this Friday? I heard the new Marvel film is really good\",\n",
    "        \"Thanks for the book recommendation, I really enjoyed reading it\",\n",
    "        \"The team building event is planned for next month at the beach resort\",\n",
    "        \n",
    "        # Educational/Informational\n",
    "        \"Don't forget to register for the upcoming workshop on data science\",\n",
    "        \"The university library will be closed next Monday for maintenance\",\n",
    "        \"New course registration opens tomorrow. Make sure to enroll early\",\n",
    "        \"The seminar on artificial intelligence was very informative\",\n",
    "        \n",
    "        # News and updates\n",
    "        \"The software update has been successfully installed on all systems\",\n",
    "        \"New safety protocols will be implemented starting next month\",\n",
    "        \"The annual company picnic is scheduled for the last Saturday of June\",\n",
    "        \"Please update your emergency contact information in the HR system\"\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    messages = fraud_messages + normal_messages\n",
    "    labels = ['fraud'] * len(fraud_messages) + ['normal'] * len(normal_messages)\n",
    "    \n",
    "    # Add message IDs for tracking\n",
    "    message_ids = [f\"MSG_{i:03d}\" for i in range(len(messages))]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'message_id': message_ids,\n",
    "        'message': messages,\n",
    "        'label': labels\n",
    "    })\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create the dataset\n",
    "df = create_fraud_dataset()\n",
    "\n",
    "print(\"üìä Dataset created successfully!\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nLabel distribution:\")\n",
    "label_counts = df['label'].value_counts()\n",
    "print(label_counts)\n",
    "print(f\"\\nClass balance: {label_counts['normal']/label_counts['fraud']:.2f}:1 (normal:fraud)\")\n",
    "\n",
    "# Show sample messages\n",
    "print(f\"\\nüìù Sample fraud messages:\")\n",
    "print(\"-\" * 30)\n",
    "fraud_samples = df[df['label'] == 'fraud']['message'].head(3)\n",
    "for i, msg in enumerate(fraud_samples, 1):\n",
    "    print(f\"{i}. {msg[:80]}...\")\n",
    "\n",
    "print(f\"\\nüìù Sample normal messages:\")\n",
    "print(\"-\" * 30)\n",
    "normal_samples = df[df['label'] == 'normal']['message'].head(3)\n",
    "for i, msg in enumerate(normal_samples, 1):\n",
    "    print(f\"{i}. {msg[:80]}...\")\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\nüìã First 5 rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for data exploration\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Label distribution\n",
    "label_counts = df['label'].value_counts()\n",
    "axes[0, 0].pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%', \n",
    "               colors=['#ff7f7f', '#7fbf7f'])\n",
    "axes[0, 0].set_title('Distribution of Labels', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Message length distribution\n",
    "df['message_length'] = df['message'].str.len()\n",
    "fraud_lengths = df[df['label'] == 'fraud']['message_length']\n",
    "normal_lengths = df[df['label'] == 'normal']['message_length']\n",
    "\n",
    "axes[0, 1].hist(fraud_lengths, alpha=0.7, label='Fraud', bins=20, color='red')\n",
    "axes[0, 1].hist(normal_lengths, alpha=0.7, label='Normal', bins=20, color='green')\n",
    "axes[0, 1].set_xlabel('Message Length (characters)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Message Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Word count distribution\n",
    "df['word_count'] = df['message'].str.split().str.len()\n",
    "fraud_words = df[df['label'] == 'fraud']['word_count']\n",
    "normal_words = df[df['label'] == 'normal']['word_count']\n",
    "\n",
    "axes[1, 0].boxplot([fraud_words, normal_words], labels=['Fraud', 'Normal'])\n",
    "axes[1, 0].set_ylabel('Word Count')\n",
    "axes[1, 0].set_title('Word Count Distribution by Label', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Average statistics\n",
    "stats_data = df.groupby('label').agg({\n",
    "    'message_length': 'mean',\n",
    "    'word_count': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "x = range(len(stats_data.columns))\n",
    "width = 0.35\n",
    "fraud_stats = stats_data.loc['fraud'].values\n",
    "normal_stats = stats_data.loc['normal'].values\n",
    "\n",
    "axes[1, 1].bar([i - width/2 for i in x], fraud_stats, width, label='Fraud', color='red', alpha=0.7)\n",
    "axes[1, 1].bar([i + width/2 for i in x], normal_stats, width, label='Normal', color='green', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Metrics')\n",
    "axes[1, 1].set_ylabel('Average Values')\n",
    "axes[1, 1].set_title('Average Statistics by Label', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(['Char Length', 'Word Count'])\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Add values on bars\n",
    "for i, (fraud_val, normal_val) in enumerate(zip(fraud_stats, normal_stats)):\n",
    "    axes[1, 1].text(i - width/2, fraud_val + 1, str(fraud_val), ha='center', va='bottom')\n",
    "    axes[1, 1].text(i + width/2, normal_val + 1, str(normal_val), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nüìä MESSAGE STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Average message length (characters):\")\n",
    "print(f\"  Fraud: {df[df['label'] == 'fraud']['message_length'].mean():.1f}\")\n",
    "print(f\"  Normal: {df[df['label'] == 'normal']['message_length'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\nAverage word count:\")\n",
    "print(f\"  Fraud: {df[df['label'] == 'fraud']['word_count'].mean():.1f}\")\n",
    "print(f\"  Normal: {df[df['label'] == 'normal']['word_count'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\nMessage length range:\")\n",
    "print(f\"  Minimum: {df['message_length'].min()} characters\")\n",
    "print(f\"  Maximum: {df['message_length'].max()} characters\")\n",
    "print(f\"  Median: {df['message_length'].median():.1f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94a4c2",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Text Cleaning\n",
    "\n",
    "Now we'll clean and preprocess the text data to prepare it for machine learning models. This includes removing noise, normalizing text, and creating features that our models can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f458df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing pipeline for fraud detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Add domain-specific words to stop words if needed\n",
    "        # self.stop_words.update(['would', 'could', 'should'])\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean and normalize text\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove phone numbers\n",
    "        text = re.sub(r'[\\+]?[1-9]?[0-9]{7,15}', '', text)\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize_and_lemmatize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize text and apply lemmatization\n",
    "        \"\"\"\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords and short words\n",
    "        tokens = [token for token in tokens if token not in self.stop_words and len(token) > 2]\n",
    "        \n",
    "        # Lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline\n",
    "        \"\"\"\n",
    "        # Clean text\n",
    "        cleaned = self.clean_text(text)\n",
    "        \n",
    "        # Tokenize and lemmatize\n",
    "        processed = self.tokenize_and_lemmatize(cleaned)\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def extract_features(self, text):\n",
    "        \"\"\"\n",
    "        Extract additional features from text\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Basic text features\n",
    "        features['char_count'] = len(text)\n",
    "        features['word_count'] = len(text.split())\n",
    "        features['sentence_count'] = len(re.findall(r'[.!?]+', text))\n",
    "        features['avg_word_length'] = np.mean([len(word) for word in text.split()]) if text.split() else 0\n",
    "        \n",
    "        # Uppercase features\n",
    "        features['upper_case_count'] = sum(1 for c in text if c.isupper())\n",
    "        features['upper_case_ratio'] = features['upper_case_count'] / len(text) if len(text) > 0 else 0\n",
    "        \n",
    "        # Punctuation features\n",
    "        features['exclamation_count'] = text.count('!')\n",
    "        features['question_count'] = text.count('?')\n",
    "        features['dollar_count'] = text.count('$')\n",
    "        \n",
    "        # Fraud-specific features\n",
    "        fraud_indicators = ['urgent', 'click', 'verify', 'winner', 'prize', 'money', 'free', 'offer']\n",
    "        features['fraud_words'] = sum(1 for word in fraud_indicators if word in text.lower())\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "print(\"üîÑ Preprocessing text data...\")\n",
    "df['cleaned_message'] = df['message'].apply(preprocessor.preprocess)\n",
    "\n",
    "# Extract additional features\n",
    "print(\"üîç Extracting additional features...\")\n",
    "feature_data = df['message'].apply(preprocessor.extract_features)\n",
    "feature_df = pd.DataFrame(list(feature_data))\n",
    "\n",
    "# Combine with main dataframe\n",
    "df = pd.concat([df, feature_df], axis=1)\n",
    "\n",
    "print(\"‚úÖ Preprocessing complete!\")\n",
    "\n",
    "# Show preprocessing examples\n",
    "print(\"\\nüìù PREPROCESSING EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_indices = [0, 15, 30]  # Show examples from different categories\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    print(f\"\\nExample {i+1} - Label: {df.iloc[idx]['label'].upper()}\")\n",
    "    print(f\"Original: {df.iloc[idx]['message'][:80]}...\")\n",
    "    print(f\"Cleaned:  {df.iloc[idx]['cleaned_message'][:80]}...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Show feature statistics\n",
    "print(\"\\nüìä EXTRACTED FEATURES STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "feature_cols = ['char_count', 'word_count', 'upper_case_ratio', 'fraud_words']\n",
    "print(df.groupby('label')[feature_cols].mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a92b2",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering with TF-IDF\n",
    "\n",
    "We'll convert the cleaned text into numerical features that machine learning algorithms can understand using TF-IDF (Term Frequency-Inverse Document Frequency) vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257fd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,          # Limit vocabulary to top 5000 words\n",
    "    ngram_range=(1, 2),         # Use unigrams and bigrams\n",
    "    stop_words='english',       # Remove English stop words\n",
    "    min_df=2,                   # Ignore terms that appear in less than 2 documents\n",
    "    max_df=0.95,               # Ignore terms that appear in more than 95% of documents\n",
    "    lowercase=True,             # Convert to lowercase\n",
    "    sublinear_tf=True          # Apply sublinear tf scaling\n",
    ")\n",
    "\n",
    "print(\"üîÑ Creating TF-IDF features...\")\n",
    "\n",
    "# Fit and transform the cleaned text\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_message'])\n",
    "\n",
    "print(f\"‚úÖ TF-IDF vectorization complete!\")\n",
    "print(f\"üìä Feature matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"üìö Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=feature_names)\n",
    "\n",
    "print(f\"\\nüîç Sample TF-IDF features:\")\n",
    "print(f\"First 10 features: {list(feature_names[:10])}\")\n",
    "\n",
    "# Analyze most important features for each class\n",
    "print(\"\\nüìä TOP TF-IDF FEATURES BY CLASS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate mean TF-IDF scores for each class\n",
    "fraud_mask = df['label'] == 'fraud'\n",
    "normal_mask = df['label'] == 'normal'\n",
    "\n",
    "fraud_tfidf_mean = X_tfidf_df[fraud_mask].mean()\n",
    "normal_tfidf_mean = X_tfidf_df[normal_mask].mean()\n",
    "\n",
    "# Get top features for fraud class\n",
    "top_fraud_features = fraud_tfidf_mean.nlargest(10)\n",
    "print(\"üö® Top 10 Fraud Features:\")\n",
    "for feature, score in top_fraud_features.items():\n",
    "    print(f\"  {feature}: {score:.4f}\")\n",
    "\n",
    "# Get top features for normal class\n",
    "top_normal_features = normal_tfidf_mean.nlargest(10)\n",
    "print(\"\\n‚úÖ Top 10 Normal Features:\")\n",
    "for feature, score in top_normal_features.items():\n",
    "    print(f\"  {feature}: {score:.4f}\")\n",
    "\n",
    "# Visualize top features\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Top fraud features\n",
    "top_fraud_features.plot(kind='barh', ax=ax1, color='red', alpha=0.7)\n",
    "ax1.set_title('Top 10 Features in Fraud Messages', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Average TF-IDF Score')\n",
    "\n",
    "# Top normal features\n",
    "top_normal_features.plot(kind='barh', ax=ax2, color='green', alpha=0.7)\n",
    "ax2.set_title('Top 10 Features in Normal Messages', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Average TF-IDF Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create word clouds for visual representation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Fraud word cloud\n",
    "fraud_text = ' '.join(df[df['label'] == 'fraud']['cleaned_message'])\n",
    "fraud_wordcloud = WordCloud(width=400, height=300, background_color='white').generate(fraud_text)\n",
    "ax1.imshow(fraud_wordcloud, interpolation='bilinear')\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Fraud Messages Word Cloud', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Normal word cloud\n",
    "normal_text = ' '.join(df[df['label'] == 'normal']['cleaned_message'])\n",
    "normal_wordcloud = WordCloud(width=400, height=300, background_color='white').generate(normal_text)\n",
    "ax2.imshow(normal_wordcloud, interpolation='bilinear')\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Normal Messages Word Cloud', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Combine TF-IDF features with additional engineered features\n",
    "additional_features = ['char_count', 'word_count', 'upper_case_ratio', 'fraud_words', \n",
    "                      'exclamation_count', 'dollar_count']\n",
    "\n",
    "# Normalize additional features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_additional = scaler.fit_transform(df[additional_features])\n",
    "\n",
    "# Combine features\n",
    "X_combined = np.hstack([X_tfidf.toarray(), X_additional])\n",
    "print(f\"\\nüîó Combined feature matrix shape: {X_combined.shape}\")\n",
    "print(f\"   TF-IDF features: {X_tfidf.shape[1]}\")\n",
    "print(f\"   Additional features: {len(additional_features)}\")\n",
    "print(f\"   Total features: {X_combined.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4318c8af",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split\n",
    "\n",
    "Now we'll split our data into training and testing sets, ensuring proper stratification to maintain class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8435ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare target variable\n",
    "y = df['label'].map({'normal': 0, 'fraud': 1})\n",
    "\n",
    "print(\"üîÑ Splitting dataset...\")\n",
    "\n",
    "# Split the data with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y, \n",
    "    test_size=0.3,          # 70% train, 30% test\n",
    "    random_state=42,        # For reproducibility\n",
    "    stratify=y             # Maintain class balance\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dataset split complete!\")\n",
    "\n",
    "# Print split information\n",
    "print(f\"\\nüìä DATASET SPLIT SUMMARY\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Training samples: {len(X_train)} ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"Testing samples: {len(X_test)} ({len(X_test)/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è LABEL DISTRIBUTION\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Training set distribution\n",
    "train_fraud = sum(y_train)\n",
    "train_normal = len(y_train) - train_fraud\n",
    "print(f\"Training set:\")\n",
    "print(f\"  Normal: {train_normal} ({train_normal/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Fraud:  {train_fraud} ({train_fraud/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "# Testing set distribution\n",
    "test_fraud = sum(y_test)\n",
    "test_normal = len(y_test) - test_fraud\n",
    "print(f\"\\nTesting set:\")\n",
    "print(f\"  Normal: {test_normal} ({test_normal/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Fraud:  {test_fraud} ({test_fraud/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# Visualize the split\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Training set distribution\n",
    "train_labels = ['Normal', 'Fraud']\n",
    "train_sizes = [train_normal, train_fraud]\n",
    "ax1.pie(train_sizes, labels=train_labels, autopct='%1.1f%%', colors=['green', 'red'], alpha=0.7)\n",
    "ax1.set_title('Training Set Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Testing set distribution\n",
    "test_labels = ['Normal', 'Fraud']\n",
    "test_sizes = [test_normal, test_fraud]\n",
    "ax2.pie(test_sizes, labels=test_labels, autopct='%1.1f%%', colors=['green', 'red'], alpha=0.7)\n",
    "ax2.set_title('Testing Set Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüî¢ FEATURE MATRIX SHAPES\")\n",
    "print(\"=\"*30)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape:  {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape:  {y_test.shape}\")\n",
    "\n",
    "# Check for any potential data leakage or issues\n",
    "print(f\"\\n‚úÖ DATA QUALITY CHECKS\")\n",
    "print(\"=\"*25)\n",
    "print(f\"No missing values in X_train: {not np.isnan(X_train).any()}\")\n",
    "print(f\"No missing values in X_test:  {not np.isnan(X_test).any()}\")\n",
    "print(f\"Feature dimensions match: {X_train.shape[1] == X_test.shape[1]}\")\n",
    "print(f\"Class balance maintained: {abs(train_fraud/len(y_train) - test_fraud/len(y_test)) < 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ec839",
   "metadata": {},
   "source": [
    "## 6. Build Baseline Models\n",
    "\n",
    "We'll implement and compare multiple baseline models to establish a strong foundation for fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14e376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize baseline models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "print(\"ü§ñ Training baseline models...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîÑ Training {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc_score': auc\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {name} complete!\")\n",
    "    print(f\"   Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"   F1-Score: {f1:.3f}\")\n",
    "    print(f\"   AUC:      {auc:.3f}\")\n",
    "\n",
    "print(f\"\\nüéØ MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'AUC':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name:<20} {metrics['accuracy']:<10.3f} {metrics['precision']:<10.3f} \"\n",
    "          f\"{metrics['recall']:<10.3f} {metrics['f1_score']:<10.3f} {metrics['auc_score']:<10.3f}\")\n",
    "\n",
    "# Find best model based on F1-score (important for fraud detection)\n",
    "best_model_name = max(results.keys(), key=lambda x: results[x]['f1_score'])\n",
    "best_f1 = results[best_model_name]['f1_score']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} (F1-Score: {best_f1:.3f})\")\n",
    "\n",
    "# Visualize model performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Performance metrics comparison\n",
    "metrics_df = pd.DataFrame({\n",
    "    name: [results[name]['accuracy'], results[name]['precision'], \n",
    "           results[name]['recall'], results[name]['f1_score'], results[name]['auc_score']]\n",
    "    for name in results.keys()\n",
    "}, index=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC'])\n",
    "\n",
    "metrics_df.plot(kind='bar', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. ROC Curves\n",
    "for name, result in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])\n",
    "    axes[0, 1].plot(fpr, tpr, label=f\"{name} (AUC = {result['auc_score']:.3f})\")\n",
    "\n",
    "axes[0, 1].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "axes[0, 1].set_xlabel('False Positive Rate')\n",
    "axes[0, 1].set_ylabel('True Positive Rate')\n",
    "axes[0, 1].set_title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Feature importance for Random Forest\n",
    "if 'Random Forest' in results:\n",
    "    rf_model = results['Random Forest']['model']\n",
    "    feature_importance = rf_model.feature_importances_\n",
    "    \n",
    "    # Get top 15 features\n",
    "    top_indices = np.argsort(feature_importance)[-15:]\n",
    "    top_features = [f\"Feature_{i}\" for i in top_indices]  # Simplified feature names\n",
    "    top_importance = feature_importance[top_indices]\n",
    "    \n",
    "    axes[1, 0].barh(range(len(top_features)), top_importance)\n",
    "    axes[1, 0].set_yticks(range(len(top_features)))\n",
    "    axes[1, 0].set_yticklabels(top_features)\n",
    "    axes[1, 0].set_xlabel('Importance')\n",
    "    axes[1, 0].set_title('Random Forest - Top 15 Features', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Model complexity vs performance\n",
    "model_complexity = {'Logistic Regression': 1, 'Naive Bayes': 1, 'SVM': 3, 'Random Forest': 4}\n",
    "f1_scores = [results[name]['f1_score'] for name in model_complexity.keys()]\n",
    "complexity_scores = list(model_complexity.values())\n",
    "\n",
    "axes[1, 1].scatter(complexity_scores, f1_scores, s=100, alpha=0.7)\n",
    "for i, name in enumerate(model_complexity.keys()):\n",
    "    axes[1, 1].annotate(name, (complexity_scores[i], f1_scores[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "axes[1, 1].set_xlabel('Model Complexity (1=Simple, 4=Complex)')\n",
    "axes[1, 1].set_ylabel('F1-Score')\n",
    "axes[1, 1].set_title('Model Complexity vs Performance', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bf39d6",
   "metadata": {},
   "source": [
    "## 7. Model Training and Evaluation\n",
    "\n",
    "Let's perform cross-validation and detailed analysis of our models to ensure robust performance estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation for more robust evaluation\n",
    "print(\"üîÑ Performing cross-validation...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cv_results = {}\n",
    "cv_folds = 5\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüìä Cross-validating {name}...\")\n",
    "    \n",
    "    # Perform cross-validation on different metrics\n",
    "    cv_accuracy = cross_val_score(model, X_combined, y, cv=cv_folds, scoring='accuracy')\n",
    "    cv_precision = cross_val_score(model, X_combined, y, cv=cv_folds, scoring='precision')\n",
    "    cv_recall = cross_val_score(model, X_combined, y, cv=cv_folds, scoring='recall')\n",
    "    cv_f1 = cross_val_score(model, X_combined, y, cv=cv_folds, scoring='f1')\n",
    "    cv_auc = cross_val_score(model, X_combined, y, cv=cv_folds, scoring='roc_auc')\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'accuracy': cv_accuracy,\n",
    "        'precision': cv_precision,\n",
    "        'recall': cv_recall,\n",
    "        'f1': cv_f1,\n",
    "        'auc': cv_auc\n",
    "    }\n",
    "    \n",
    "    print(f\"   Accuracy:  {cv_accuracy.mean():.3f} (¬±{cv_accuracy.std()*2:.3f})\")\n",
    "    print(f\"   Precision: {cv_precision.mean():.3f} (¬±{cv_precision.std()*2:.3f})\")\n",
    "    print(f\"   Recall:    {cv_recall.mean():.3f} (¬±{cv_recall.std()*2:.3f})\")\n",
    "    print(f\"   F1-Score:  {cv_f1.mean():.3f} (¬±{cv_f1.std()*2:.3f})\")\n",
    "    print(f\"   AUC:       {cv_auc.mean():.3f} (¬±{cv_auc.std()*2:.3f})\")\n",
    "\n",
    "# Visualize cross-validation results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    \n",
    "    # Prepare data for box plot\n",
    "    data_to_plot = [cv_results[name][metric] for name in models.keys()]\n",
    "    model_names = list(models.keys())\n",
    "    \n",
    "    # Create box plot\n",
    "    box_plot = axes[row, col].boxplot(data_to_plot, labels=model_names, patch_artist=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
    "    for patch, color in zip(box_plot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    axes[row, col].set_title(f'{metric.upper()} - Cross Validation', fontsize=12, fontweight='bold')\n",
    "    axes[row, col].set_ylabel(metric.title())\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Remove the empty subplot\n",
    "axes[1, 2].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical significance testing\n",
    "print(f\"\\nüìà CROSS-VALIDATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<20} {'Metric':<12} {'Mean':<8} {'Std':<8} {'Min':<8} {'Max':<8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name in models.keys():\n",
    "    for metric in ['f1', 'precision', 'recall', 'accuracy', 'auc']:\n",
    "        scores = cv_results[name][metric]\n",
    "        print(f\"{name:<20} {metric:<12} {scores.mean():<8.3f} {scores.std():<8.3f} \"\n",
    "              f\"{scores.min():<8.3f} {scores.max():<8.3f}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# Hyperparameter tuning for best models\n",
    "print(f\"\\nüîß HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Tune Logistic Regression\n",
    "print(\"üîÑ Tuning Logistic Regression...\")\n",
    "lr_params = {\n",
    "    'C': [0.1, 1.0, 10.0, 100.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    LogisticRegression(random_state=42, max_iter=1000),\n",
    "    lr_params,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr_grid.fit(X_train, y_train)\n",
    "print(f\"‚úÖ Best LR params: {lr_grid.best_params_}\")\n",
    "print(f\"‚úÖ Best LR score: {lr_grid.best_score_:.3f}\")\n",
    "\n",
    "# Tune SVM\n",
    "print(f\"\\nüîÑ Tuning SVM...\")\n",
    "svm_params = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(\n",
    "    SVC(random_state=42, probability=True),\n",
    "    svm_params,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "svm_grid.fit(X_train, y_train)\n",
    "print(f\"‚úÖ Best SVM params: {svm_grid.best_params_}\")\n",
    "print(f\"‚úÖ Best SVM score: {svm_grid.best_score_:.3f}\")\n",
    "\n",
    "# Store tuned models\n",
    "tuned_models = {\n",
    "    'Tuned Logistic Regression': lr_grid.best_estimator_,\n",
    "    'Tuned SVM': svm_grid.best_estimator_\n",
    "}\n",
    "\n",
    "# Evaluate tuned models\n",
    "print(f\"\\nüéØ TUNED MODEL PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.3f}\")\n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall:    {recall:.3f}\")\n",
    "    print(f\"  F1-Score:  {f1:.3f}\")\n",
    "    print(f\"  AUC:       {auc:.3f}\")\n",
    "    \n",
    "    # Update results with tuned models\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc_score': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1b09f",
   "metadata": {},
   "source": [
    "## 8. Performance Metrics and Confusion Matrix\n",
    "\n",
    "Let's dive deep into the performance analysis with detailed confusion matrices and error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d3d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed confusion matrix analysis\n",
    "def plot_confusion_matrices(results, y_test):\n",
    "    \"\"\"Plot confusion matrices for all models\"\"\"\n",
    "    \n",
    "    n_models = len(results)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (name, result) in enumerate(results.items()):\n",
    "        if idx >= len(axes):\n",
    "            break\n",
    "            \n",
    "        cm = confusion_matrix(y_test, result['y_pred'])\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=['Normal', 'Fraud'], \n",
    "                   yticklabels=['Normal', 'Fraud'],\n",
    "                   ax=axes[idx])\n",
    "        \n",
    "        axes[idx].set_title(f'{name}\\nAccuracy: {result[\"accuracy\"]:.3f}', \n",
    "                           fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Predicted Label')\n",
    "        axes[idx].set_ylabel('True Label')\n",
    "        \n",
    "        # Add percentage annotations\n",
    "        total = cm.sum()\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                percent = cm[i, j] / total * 100\n",
    "                axes[idx].text(j + 0.5, i + 0.7, f'({percent:.1f}%)', \n",
    "                             ha='center', va='center', fontsize=10, color='red')\n",
    "    \n",
    "    # Remove unused subplots\n",
    "    for idx in range(len(results), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_confusion_matrices(results, y_test)\n",
    "\n",
    "# Detailed classification reports\n",
    "print(\"üìä DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"\\nü§ñ {name}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(classification_report(y_test, result['y_pred'], \n",
    "                              target_names=['Normal', 'Fraud'],\n",
    "                              digits=3))\n",
    "\n",
    "# Error analysis\n",
    "print(f\"\\nüîç ERROR ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get the best performing model for detailed analysis\n",
    "best_model_name = max(results.keys(), key=lambda x: results[x]['f1_score'])\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "print(f\"Analyzing errors for: {best_model_name}\")\n",
    "print(f\"Best F1-Score: {best_result['f1_score']:.3f}\")\n",
    "\n",
    "# Get test data for error analysis\n",
    "X_test_indices = y_test.index if hasattr(y_test, 'index') else range(len(y_test))\n",
    "\n",
    "# Find misclassified samples\n",
    "y_pred_best = best_result['y_pred']\n",
    "misclassified_mask = y_test != y_pred_best\n",
    "misclassified_indices = [i for i, mask in enumerate(misclassified_mask) if mask]\n",
    "\n",
    "print(f\"\\nTotal misclassified samples: {sum(misclassified_mask)}\")\n",
    "\n",
    "# Analyze false positives and false negatives\n",
    "false_positives = [(i, y_test.iloc[i] if hasattr(y_test, 'iloc') else y_test[i], y_pred_best[i]) \n",
    "                   for i in misclassified_indices \n",
    "                   if (y_test.iloc[i] if hasattr(y_test, 'iloc') else y_test[i]) == 0 and y_pred_best[i] == 1]\n",
    "\n",
    "false_negatives = [(i, y_test.iloc[i] if hasattr(y_test, 'iloc') else y_test[i], y_pred_best[i]) \n",
    "                   for i in misclassified_indices \n",
    "                   if (y_test.iloc[i] if hasattr(y_test, 'iloc') else y_test[i]) == 1 and y_pred_best[i] == 0]\n",
    "\n",
    "print(f\"False Positives (Normal predicted as Fraud): {len(false_positives)}\")\n",
    "print(f\"False Negatives (Fraud predicted as Normal): {len(false_negatives)}\")\n",
    "\n",
    "# Show examples of misclassified samples\n",
    "if len(false_positives) > 0:\n",
    "    print(f\"\\n‚ùå FALSE POSITIVE EXAMPLES:\")\n",
    "    print(\"-\" * 30)\n",
    "    # Get original test data indices\n",
    "    df_test = df.iloc[y_test.index] if hasattr(y_test, 'index') else df.iloc[-len(y_test):]\n",
    "    \n",
    "    for i, (idx, true_label, pred_label) in enumerate(false_positives[:3]):\n",
    "        original_idx = df_test.index[idx] if hasattr(df_test, 'index') else idx\n",
    "        message = df.loc[original_idx, 'message'] if original_idx in df.index else \"Message not found\"\n",
    "        print(f\"{i+1}. {message[:100]}...\")\n",
    "        print(f\"   True: Normal, Predicted: Fraud\")\n",
    "        print()\n",
    "\n",
    "if len(false_negatives) > 0:\n",
    "    print(f\"\\n‚ùå FALSE NEGATIVE EXAMPLES:\")\n",
    "    print(\"-\" * 30)\n",
    "    df_test = df.iloc[y_test.index] if hasattr(y_test, 'index') else df.iloc[-len(y_test):]\n",
    "    \n",
    "    for i, (idx, true_label, pred_label) in enumerate(false_negatives[:3]):\n",
    "        original_idx = df_test.index[idx] if hasattr(df_test, 'index') else idx\n",
    "        message = df.loc[original_idx, 'message'] if original_idx in df.index else \"Message not found\"\n",
    "        print(f\"{i+1}. {message[:100]}...\")\n",
    "        print(f\"   True: Fraud, Predicted: Normal\")\n",
    "        print()\n",
    "\n",
    "# Cost analysis for fraud detection\n",
    "print(f\"\\nüí∞ COST ANALYSIS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Assuming costs: FN (missed fraud) = $1000, FP (false alarm) = $10\n",
    "cost_fn = 1000  # Cost of missing a fraud\n",
    "cost_fp = 10    # Cost of false alarm\n",
    "\n",
    "for name, result in results.items():\n",
    "    cm = confusion_matrix(y_test, result['y_pred'])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    total_cost = fn * cost_fn + fp * cost_fp\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  False Negatives: {fn} x ${cost_fn} = ${fn * cost_fn}\")\n",
    "    print(f\"  False Positives: {fp} x ${cost_fp} = ${fp * cost_fp}\")\n",
    "    print(f\"  Total Cost: ${total_cost}\")\n",
    "    print(f\"  Cost per sample: ${total_cost / len(y_test):.2f}\")\n",
    "    print()\n",
    "\n",
    "# Performance at different thresholds\n",
    "print(f\"\\nüìà THRESHOLD ANALYSIS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Analyze best model at different thresholds\n",
    "best_model = best_result['model']\n",
    "y_proba = best_result['y_pred_proba']\n",
    "\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"{'Threshold':<10} {'Precision':<10} {'Recall':<10} {'F1':<10} {'Cost':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_proba >= threshold).astype(int)\n",
    "    \n",
    "    precision_thresh = precision_score(y_test, y_pred_thresh)\n",
    "    recall_thresh = recall_score(y_test, y_pred_thresh)\n",
    "    f1_thresh = f1_score(y_test, y_pred_thresh)\n",
    "    \n",
    "    # Calculate cost\n",
    "    cm_thresh = confusion_matrix(y_test, y_pred_thresh)\n",
    "    tn, fp, fn, tp = cm_thresh.ravel()\n",
    "    cost_thresh = fn * cost_fn + fp * cost_fp\n",
    "    \n",
    "    print(f\"{threshold:<10.1f} {precision_thresh:<10.3f} {recall_thresh:<10.3f} \"\n",
    "          f\"{f1_thresh:<10.3f} ${cost_thresh:<9.0f}\")\n",
    "\n",
    "# Plot precision-recall curve\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision_curve, recall_curve, thresholds_pr = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "ax1.plot(recall_curve, precision_curve, color='blue', linewidth=2)\n",
    "ax1.set_xlabel('Recall')\n",
    "ax1.set_ylabel('Precision')\n",
    "ax1.set_title(f'Precision-Recall Curve\\n{best_model_name}', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot threshold vs metrics\n",
    "ax2.plot(thresholds, [precision_score(y_test, (y_proba >= t).astype(int)) for t in thresholds], \n",
    "         label='Precision', marker='o')\n",
    "ax2.plot(thresholds, [recall_score(y_test, (y_proba >= t).astype(int)) for t in thresholds], \n",
    "         label='Recall', marker='s')\n",
    "ax2.plot(thresholds, [f1_score(y_test, (y_proba >= t).astype(int)) for t in thresholds], \n",
    "         label='F1-Score', marker='^')\n",
    "\n",
    "ax2.set_xlabel('Threshold')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Metrics vs Threshold', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14801b32",
   "metadata": {},
   "source": [
    "## 9. Model Prediction on New Text Samples\n",
    "\n",
    "Now let's test our trained models on new, unseen text samples to see how they perform in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c3362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDetectionPredictor:\n",
    "    \"\"\"\n",
    "    Wrapper class for making predictions on new text samples\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, vectorizer, preprocessor, scaler, feature_cols):\n",
    "        self.model = model\n",
    "        self.vectorizer = vectorizer\n",
    "        self.preprocessor = preprocessor\n",
    "        self.scaler = scaler\n",
    "        self.feature_cols = feature_cols\n",
    "        \n",
    "    def predict_message(self, message):\n",
    "        \"\"\"\n",
    "        Predict if a message is fraud or normal\n",
    "        \"\"\"\n",
    "        # Preprocess the message\n",
    "        cleaned_message = self.preprocessor.preprocess(message)\n",
    "        \n",
    "        # Extract TF-IDF features\n",
    "        tfidf_features = self.vectorizer.transform([cleaned_message])\n",
    "        \n",
    "        # Extract additional features\n",
    "        additional_features_dict = self.preprocessor.extract_features(message)\n",
    "        additional_features = np.array([[additional_features_dict[col] for col in self.feature_cols]])\n",
    "        additional_features_scaled = self.scaler.transform(additional_features)\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = np.hstack([tfidf_features.toarray(), additional_features_scaled])\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = self.model.predict(combined_features)[0]\n",
    "        probability = self.model.predict_proba(combined_features)[0]\n",
    "        \n",
    "        return {\n",
    "            'message': message,\n",
    "            'prediction': 'Fraud' if prediction == 1 else 'Normal',\n",
    "            'confidence': max(probability),\n",
    "            'fraud_probability': probability[1],\n",
    "            'normal_probability': probability[0]\n",
    "        }\n",
    "\n",
    "# Create predictor with the best model\n",
    "best_model = results[best_model_name]['model']\n",
    "predictor = FraudDetectionPredictor(\n",
    "    model=best_model,\n",
    "    vectorizer=tfidf_vectorizer,\n",
    "    preprocessor=preprocessor,\n",
    "    scaler=scaler,\n",
    "    feature_cols=additional_features\n",
    ")\n",
    "\n",
    "# Test on new sample messages\n",
    "test_messages = [\n",
    "    # Clear fraud examples\n",
    "    \"URGENT: Your account is suspended! Click here immediately to restore access or lose your money forever!\",\n",
    "    \"Congratulations! You've won $1,000,000 in our lottery! Send $500 processing fee to claim your prize now!\",\n",
    "    \"FINAL NOTICE: Pay $2000 immediately or we will take legal action against you today!\",\n",
    "    \"Your bank account has been compromised. Verify your details at suspicious-bank-link.com right now!\",\n",
    "    \n",
    "    # Clear normal examples  \n",
    "    \"Hey, thanks for helping me with the project yesterday. The presentation went really well!\",\n",
    "    \"Reminder: Your doctor's appointment is scheduled for tomorrow at 2 PM\",\n",
    "    \"The team meeting has been moved to Friday at 10 AM in conference room B\",\n",
    "    \"Happy birthday! Hope you have a wonderful celebration with your family\",\n",
    "    \n",
    "    # Ambiguous/edge cases\n",
    "    \"Limited time offer: 50% off all items. Sale ends soon!\",\n",
    "    \"Your order has been delayed due to shipping issues. Additional fees may apply\",\n",
    "    \"Security alert: We detected unusual activity on your account. Please review\",\n",
    "    \"Investment opportunity: High returns guaranteed with our new fund\"\n",
    "]\n",
    "\n",
    "print(\"üîÆ TESTING ON NEW MESSAGES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Predict for all test messages\n",
    "predictions = []\n",
    "for i, message in enumerate(test_messages, 1):\n",
    "    result = predictor.predict_message(message)\n",
    "    predictions.append(result)\n",
    "    \n",
    "    # Determine emoji and color based on prediction\n",
    "    emoji = \"üö®\" if result['prediction'] == 'Fraud' else \"‚úÖ\"\n",
    "    confidence_color = \"HIGH\" if result['confidence'] > 0.8 else \"MEDIUM\" if result['confidence'] > 0.6 else \"LOW\"\n",
    "    \n",
    "    print(f\"\\n{emoji} Test Message {i}:\")\n",
    "    print(f\"Message: {message}\")\n",
    "    print(f\"Prediction: {result['prediction']} (Confidence: {confidence_color} - {result['confidence']:.3f})\")\n",
    "    print(f\"Fraud Probability: {result['fraud_probability']:.3f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Create visualization of predictions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# 1. Prediction distribution\n",
    "pred_counts = pd.Series([p['prediction'] for p in predictions]).value_counts()\n",
    "colors = ['green' if label == 'Normal' else 'red' for label in pred_counts.index]\n",
    "ax1.pie(pred_counts.values, labels=pred_counts.index, autopct='%1.1f%%', colors=colors, alpha=0.7)\n",
    "ax1.set_title('Prediction Distribution on Test Messages', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Confidence distribution\n",
    "confidences = [p['confidence'] for p in predictions]\n",
    "fraud_confidences = [p['confidence'] for p in predictions if p['prediction'] == 'Fraud']\n",
    "normal_confidences = [p['confidence'] for p in predictions if p['prediction'] == 'Normal']\n",
    "\n",
    "ax2.hist(fraud_confidences, alpha=0.7, label='Fraud Predictions', color='red', bins=5)\n",
    "ax2.hist(normal_confidences, alpha=0.7, label='Normal Predictions', color='green', bins=5)\n",
    "ax2.set_xlabel('Confidence Score')\n",
    "ax2.set_ylabel('Number of Predictions')\n",
    "ax2.set_title('Confidence Distribution by Prediction', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interactive prediction function\n",
    "def interactive_prediction():\n",
    "    \"\"\"\n",
    "    Interactive function for testing custom messages\n",
    "    \"\"\"\n",
    "    print(\"\\nüéØ INTERACTIVE FRAUD DETECTION\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Enter a message to test (or 'quit' to exit):\")\n",
    "    \n",
    "    while True:\n",
    "        user_message = input(\"\\nMessage: \").strip()\n",
    "        \n",
    "        if user_message.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"üëã Thanks for testing!\")\n",
    "            break\n",
    "            \n",
    "        if not user_message:\n",
    "            print(\"‚ö†Ô∏è Please enter a message.\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            result = predictor.predict_message(user_message)\n",
    "            \n",
    "            emoji = \"üö®\" if result['prediction'] == 'Fraud' else \"‚úÖ\"\n",
    "            print(f\"\\n{emoji} Prediction: {result['prediction']}\")\n",
    "            print(f\"   Confidence: {result['confidence']:.3f}\")\n",
    "            print(f\"   Fraud Probability: {result['fraud_probability']:.3f}\")\n",
    "            \n",
    "            if result['prediction'] == 'Fraud':\n",
    "                print(\"   ‚ö†Ô∏è This message appears to be fraudulent!\")\n",
    "            else:\n",
    "                print(\"   ‚úÖ This message appears to be legitimate.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing message: {e}\")\n",
    "\n",
    "# Uncomment the line below to run interactive prediction\n",
    "# interactive_prediction()\n",
    "\n",
    "# Save the best model for future use\n",
    "print(f\"\\nüíæ MODEL PERSISTENCE\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save the complete pipeline\n",
    "model_pipeline = {\n",
    "    'model': best_model,\n",
    "    'vectorizer': tfidf_vectorizer,\n",
    "    'preprocessor': preprocessor,\n",
    "    'scaler': scaler,\n",
    "    'feature_columns': additional_features,\n",
    "    'model_name': best_model_name,\n",
    "    'performance_metrics': {\n",
    "        'accuracy': results[best_model_name]['accuracy'],\n",
    "        'precision': results[best_model_name]['precision'],\n",
    "        'recall': results[best_model_name]['recall'],\n",
    "        'f1_score': results[best_model_name]['f1_score'],\n",
    "        'auc_score': results[best_model_name]['auc_score']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "# joblib.dump(model_pipeline, 'fraud_detection_pipeline.pkl')\n",
    "print(\"Model pipeline ready for saving with joblib.dump()\")\n",
    "\n",
    "print(f\"\\nTo load the model later:\")\n",
    "print(\"model_pipeline = joblib.load('fraud_detection_pipeline.pkl')\")\n",
    "print(\"predictor = FraudDetectionPredictor(**model_pipeline)\")\n",
    "\n",
    "# Summary of best practices\n",
    "print(f\"\\nüìã IMPLEMENTATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üèÜ Best Model: {best_model_name}\")\n",
    "print(f\"üìä Performance: F1-Score = {results[best_model_name]['f1_score']:.3f}\")\n",
    "print(f\"üîß Key Features:\")\n",
    "print(f\"   - TF-IDF vectorization with {X_tfidf.shape[1]} features\")\n",
    "print(f\"   - Additional engineered features: {len(additional_features)}\")\n",
    "print(f\"   - Cross-validation for robust evaluation\")\n",
    "print(f\"   - Hyperparameter tuning\")\n",
    "print(f\"   - Cost-sensitive analysis\")\n",
    "print(f\"\\n‚ú® Next Steps:\")\n",
    "print(f\"   1. Deploy as web service (Flask/FastAPI)\")\n",
    "print(f\"   2. Implement real-time monitoring\")\n",
    "print(f\"   3. Add more sophisticated features\")\n",
    "print(f\"   4. Experiment with BERT/transformer models\")\n",
    "print(f\"   5. Collect and retrain on real-world data\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
