{"metadata":{"kernelspec":{"name":"","display_name":""},"language_info":{"name":"python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13035798,"sourceType":"datasetVersion","datasetId":8254194}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c666a435","cell_type":"markdown","source":"# üöÄ Multiclass Fraud Detection Training on Kaggle\n\nThis notebook is optimized for Kaggle's environment with GPU acceleration and **multiclass classification**.\n\n## üìä Dataset\n- Upload your `final_fraud_detection_dataset.csv`\n- **NEW: Supports 10-class classification** (9 scam types + legitimate)\n- Classes: `legitimate`, `phishing`, `popup_scam`, `sms_spam`, `reward_scam`, `tech_support_scam`, `refund_scam`, `ssn_scam`, `job_scam`\n\n## üéØ Models\n- Traditional ML: TF-IDF + Logistic Regression/SVM (multiclass)\n- Deep Learning: BERT-based classifier (10 classes)\n\n## ‚ö° Kaggle Advantages\n- Free GPU access (Tesla P100)\n- Pre-installed ML libraries\n- Easy dataset upload\n- Community sharing\n\n## üé™ Multiclass Benefits\n- **Granular fraud detection**: Identify specific scam types\n- **Better actionable insights**: Know which type of fraud to defend against\n- **Improved model interpretability**: Understand fraud patterns by category","metadata":{}},{"id":"0ccbf1b4","cell_type":"code","source":"# Install additional packages if needed\n!pip install transformers torch --quiet\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport torch\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"‚úÖ Environment ready!\")\nprint(f\"GPU Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T09:12:33.373627Z","iopub.execute_input":"2025-09-12T09:12:33.373917Z","iopub.status.idle":"2025-09-12T09:12:36.636753Z","shell.execute_reply.started":"2025-09-12T09:12:33.373895Z","shell.execute_reply":"2025-09-12T09:12:36.635699Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Environment ready!\nGPU Available: True\nGPU: Tesla T4\n","output_type":"stream"}],"execution_count":19},{"id":"2fa379cb","cell_type":"code","source":"# Load your dataset\ntry:\n    df = pd.read_csv('/kaggle/input/fraud-detection-dataset/final_fraud_detection_dataset.csv')\n    print(f\"‚úÖ Dataset loaded: {len(df)} samples\")\n    print(f\"Columns: {df.columns.tolist()}\")\n    print(f\"Label distribution: {df['binary_label'].value_counts()}\")\nexcept FileNotFoundError:\n    print(\"‚ùå Dataset not found. Please upload your CSV file.\")\n    # Create sample data for demonstration\n    print(\"üìù Using sample data instead...\")\n    # [Sample data creation code here]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T09:12:39.458650Z","iopub.execute_input":"2025-09-12T09:12:39.458937Z","iopub.status.idle":"2025-09-12T09:12:41.476038Z","shell.execute_reply.started":"2025-09-12T09:12:39.458912Z","shell.execute_reply":"2025-09-12T09:12:41.475204Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Dataset loaded: 194913 samples\nColumns: ['text', 'binary_label', 'detailed_category', 'data_type']\nLabel distribution: binary_label\n0    101717\n1     93196\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":20},{"id":"537538ff","cell_type":"code","source":"# Data preprocessing for MULTICLASS classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Use detailed_category for multiclass classification (10 classes)\nprint(\"üìä Dataset Overview:\")\nprint(f\"Total samples: {len(df)}\")\nprint(f\"Classes available: {df['detailed_category'].unique()}\")\nprint(f\"Class distribution:\\n{df['detailed_category'].value_counts()}\")\n\n# Split data using detailed_category for multiclass\nX_train, X_test, y_train, y_test = train_test_split(\n    df['text'], df['detailed_category'],\n    test_size=0.2,\n    random_state=42,\n    stratify=df['detailed_category']\n)\n\nprint(f\"\\nüîÑ Data Split:\")\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Testing samples: {len(X_test)}\")\nprint(f\"Training class distribution:\\n{y_train.value_counts()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T09:12:44.188953Z","iopub.execute_input":"2025-09-12T09:12:44.189772Z","iopub.status.idle":"2025-09-12T09:12:44.414358Z","shell.execute_reply.started":"2025-09-12T09:12:44.189744Z","shell.execute_reply":"2025-09-12T09:12:44.413419Z"}},"outputs":[{"name":"stdout","text":"üìä Dataset Overview:\nTotal samples: 194913\nClasses available: ['job_scam' 'legitimate' 'phishing' 'popup_scam' 'refund_scam'\n 'reward_scam' 'sms_spam' 'ssn_scam' 'tech_support_scam']\nClass distribution:\ndetailed_category\nlegitimate           101717\nphishing              71857\npopup_scam            11333\nsms_spam               6988\nreward_scam             606\ntech_support_scam       605\nrefund_scam             604\nssn_scam                604\njob_scam                599\nName: count, dtype: int64\n\nüîÑ Data Split:\nTraining samples: 155930\nTesting samples: 38983\nTraining class distribution:\ndetailed_category\nlegitimate           81374\nphishing             57486\npopup_scam            9066\nsms_spam              5590\nreward_scam            485\ntech_support_scam      484\nrefund_scam            483\nssn_scam               483\njob_scam               479\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":21},{"id":"d6f7c9e0","cell_type":"code","source":"# TF-IDF Vectorization\ntfidf = TfidfVectorizer(max_features=5000, stop_words='english')\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)\n\nprint(f\"TF-IDF features: {X_train_tfidf.shape[1]}\")\nprint(\"‚úÖ Text vectorization complete!\")","metadata":{},"outputs":[],"execution_count":null},{"id":"143abe9e","cell_type":"code","source":"# Train traditional ML models for MULTICLASS classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Encode labels for multiclass (10 classes)\nle = LabelEncoder()\ny_train_encoded = le.fit_transform(y_train)\ny_test_encoded = le.transform(y_test)\n\nprint(f\"üìã Label encoding:\")\nprint(f\"Number of classes: {len(le.classes_)}\")\nprint(f\"Classes: {le.classes_}\")\n\n# Logistic Regression for multiclass\nlr_model = LogisticRegression(\n    random_state=42, \n    max_iter=1000,  # Increased iterations for multiclass\n    multi_class='ovr'  # One-vs-Rest for multiclass\n)\nlr_model.fit(X_train_tfidf, y_train_encoded)\n\n# SVM for multiclass\nsvm_model = SVC(\n    kernel='linear', \n    probability=True, \n    random_state=42,\n    decision_function_shape='ovr'  # One-vs-Rest for multiclass\n)\nsvm_model.fit(X_train_tfidf, y_train_encoded)\n\nprint(\"‚úÖ Multiclass models trained!\")","metadata":{},"outputs":[],"execution_count":null},{"id":"329eb2b0","cell_type":"code","source":"# Evaluate MULTICLASS models\nmodels = {'Logistic Regression': lr_model, 'SVM': svm_model}\n\nfor name, model in models.items():\n    y_pred = model.predict(X_test_tfidf)\n    print(f\"\\nüîç {name} Results (Multiclass):\")\n    print(classification_report(y_test_encoded, y_pred, target_names=le.classes_))\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_test_encoded, y_pred)\n    print(f\"\\nConfusion Matrix shape: {cm.shape}\")\n    print(\"Note: Full confusion matrix too large to display completely\")\n    \n    # Show accuracy for each class\n    from sklearn.metrics import accuracy_score, f1_score\n    accuracy = accuracy_score(y_test_encoded, y_pred)\n    f1_macro = f1_score(y_test_encoded, y_pred, average='macro')\n    f1_weighted = f1_score(y_test_encoded, y_pred, average='weighted')\n    \n    print(f\"üìä Overall Metrics:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"F1-Score (Macro): {f1_macro:.4f}\")\n    print(f\"F1-Score (Weighted): {f1_weighted:.4f}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"215f8bc5","cell_type":"code","source":"# BERT Training for MULTICLASS classification (GPU accelerated)\nimport torch\nfrom transformers import (\n    BertTokenizer, BertForSequenceClassification,\n    get_linear_schedule_with_warmup\n)\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader\n\nclass FraudDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        # Handle both pandas Series and numpy arrays\n        if hasattr(self.texts, 'iloc'):\n            text = str(self.texts.iloc[idx])\n        else:\n            text = str(self.texts[idx])\n            \n        if hasattr(self.labels, 'iloc'):\n            label = self.labels.iloc[idx]\n        else:\n            label = self.labels[idx]\n        \n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\nprint(\"üöÄ Initializing BERT for MULTICLASS classification...\")\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# IMPORTANT: Change num_labels to 10 for multiclass (9 scam types + 1 legitimate)\nnum_classes = len(le.classes_)\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased', \n    num_labels=num_classes\n)\n\n# Move to GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nprint(f\"Using device: {device}\")\nprint(f\"üéØ Multiclass setup: {num_classes} classes\")\nprint(f\"Classes: {', '.join(le.classes_)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T09:12:59.524068Z","iopub.execute_input":"2025-09-12T09:12:59.524366Z","iopub.status.idle":"2025-09-12T09:13:00.272749Z","shell.execute_reply.started":"2025-09-12T09:12:59.524346Z","shell.execute_reply":"2025-09-12T09:13:00.272136Z"}},"outputs":[{"name":"stdout","text":"üöÄ Initializing BERT for MULTICLASS classification...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nüéØ Multiclass setup: 9 classes\nClasses: job_scam, legitimate, phishing, popup_scam, refund_scam, reward_scam, sms_spam, ssn_scam, tech_support_scam\n","output_type":"stream"}],"execution_count":22},{"id":"3dcfc7a5","cell_type":"code","source":"# Prepare BERT datasets\ntrain_dataset = FraudDataset(X_train, y_train_encoded, tokenizer)\ntest_dataset = FraudDataset(X_test, y_test_encoded, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\nprint(f\"Training batches: {len(train_loader)}\")\nprint(f\"Testing batches: {len(test_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T09:13:20.820996Z","iopub.execute_input":"2025-09-12T09:13:20.821706Z","iopub.status.idle":"2025-09-12T09:13:20.826983Z","shell.execute_reply.started":"2025-09-12T09:13:20.821678Z","shell.execute_reply":"2025-09-12T09:13:20.826331Z"}},"outputs":[{"name":"stdout","text":"Training batches: 9746\nTesting batches: 2437\n","output_type":"stream"}],"execution_count":23},{"id":"467170ba","cell_type":"code","source":"# Training loop for MULTICLASS BERT\noptimizer = AdamW(model.parameters(), lr=2e-5)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=len(train_loader) * 3\n)\n\nmodel.train()\nfor epoch in range(3):\n    print(f\"\\nüöÄ Epoch {epoch + 1}/3\")\n    total_loss = 0\n    \n    for batch in train_loader:\n        optimizer.zero_grad()\n        \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        \n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        \n        # CrossEntropyLoss handles multiclass automatically\n        loss = outputs.loss\n        total_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n    \n    avg_loss = total_loss / len(train_loader)\n    print(f\"Average loss: {avg_loss:.4f}\")\n\nprint(\"‚úÖ BERT multiclass training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T09:13:23.609296Z","iopub.execute_input":"2025-09-12T09:13:23.609629Z","iopub.status.idle":"2025-09-12T12:27:04.034037Z","shell.execute_reply.started":"2025-09-12T09:13:23.609604Z","shell.execute_reply":"2025-09-12T12:27:04.033156Z"}},"outputs":[{"name":"stdout","text":"\nüöÄ Epoch 1/3\nAverage loss: 0.1282\n\nüöÄ Epoch 2/3\nAverage loss: 0.0393\n\nüöÄ Epoch 3/3\nAverage loss: 0.0130\n‚úÖ BERT multiclass training complete!\n","output_type":"stream"}],"execution_count":24},{"id":"ee94c1f8","cell_type":"code","source":"# Evaluate BERT MULTICLASS model\nmodel.eval()\npredictions = []\ntrue_labels = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label']\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        \n        predictions.extend(preds.cpu().numpy())\n        true_labels.extend(labels.numpy())\n\nprint(\"\\nüéØ BERT Multiclass Evaluation Results:\")\nprint(classification_report(true_labels, predictions, target_names=le.classes_))\n\n# Confusion Matrix\ncm = confusion_matrix(true_labels, predictions)\nprint(f\"\\nConfusion Matrix shape: {cm.shape}\")\n\n# Overall metrics\nfrom sklearn.metrics import accuracy_score, f1_score\naccuracy = accuracy_score(true_labels, predictions)\nf1_macro = f1_score(true_labels, predictions, average='macro')\nf1_weighted = f1_score(true_labels, predictions, average='weighted')\n\nprint(f\"\\nüìä BERT Overall Metrics:\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1-Score (Macro): {f1_macro:.4f}\")\nprint(f\"F1-Score (Weighted): {f1_weighted:.4f}\")\n\n# Show per-class performance\nprint(f\"\\nüè∑Ô∏è Per-Class F1 Scores:\")\nf1_per_class = f1_score(true_labels, predictions, average=None)\nfor i, class_name in enumerate(le.classes_):\n    print(f\"{class_name}: {f1_per_class[i]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T12:27:22.508733Z","iopub.execute_input":"2025-09-12T12:27:22.509000Z","iopub.status.idle":"2025-09-12T12:33:58.024526Z","shell.execute_reply.started":"2025-09-12T12:27:22.508983Z","shell.execute_reply":"2025-09-12T12:33:58.023845Z"}},"outputs":[{"name":"stdout","text":"\nüéØ BERT Multiclass Evaluation Results:\n                   precision    recall  f1-score   support\n\n         job_scam       0.67      0.58      0.62       120\n       legitimate       0.98      0.98      0.98     20343\n         phishing       0.98      0.97      0.98     14371\n       popup_scam       1.00      1.00      1.00      2267\n      refund_scam       0.99      1.00      1.00       121\n      reward_scam       1.00      1.00      1.00       121\n         sms_spam       0.99      0.99      0.99      1398\n         ssn_scam       1.00      1.00      1.00       121\ntech_support_scam       1.00      0.99      1.00       121\n\n         accuracy                           0.98     38983\n        macro avg       0.96      0.95      0.95     38983\n     weighted avg       0.98      0.98      0.98     38983\n\n\nConfusion Matrix shape: (9, 9)\n\nüìä BERT Overall Metrics:\nAccuracy: 0.9804\nF1-Score (Macro): 0.9514\nF1-Score (Weighted): 0.9803\n\nüè∑Ô∏è Per-Class F1 Scores:\njob_scam: 0.6222\nlegitimate: 0.9817\nphishing: 0.9767\npopup_scam: 1.0000\nrefund_scam: 0.9959\nreward_scam: 1.0000\nsms_spam: 0.9900\nssn_scam: 1.0000\ntech_support_scam: 0.9959\n","output_type":"stream"}],"execution_count":25},{"id":"1429ae9c","cell_type":"code","source":"# Save models for download\nimport joblib\nimport os\n\n# Create output directory\nos.makedirs('/kaggle/working/models', exist_ok=True)\n\n# Save traditional ML models\n# joblib.dump(lr_model, '/kaggle/working/models/logistic_regression.pkl')\n# joblib.dump(svm_model, '/kaggle/working/models/svm.pkl')\n# joblib.dump(tfidf, '/kaggle/working/models/tfidf_vectorizer.pkl')\n# joblib.dump(le, '/kaggle/working/models/label_encoder.pkl')\n\n# Save BERT model\nmodel.save_pretrained('/kaggle/working/models/bert_model')\ntokenizer.save_pretrained('/kaggle/working/models/bert_tokenizer')\n\nprint(\"üíæ Models saved to /kaggle/working/models/\")\nprint(\"Download them from the Output tab!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T12:34:16.389751Z","iopub.execute_input":"2025-09-12T12:34:16.390037Z","iopub.status.idle":"2025-09-12T12:34:17.360801Z","shell.execute_reply.started":"2025-09-12T12:34:16.390014Z","shell.execute_reply":"2025-09-12T12:34:17.360047Z"}},"outputs":[{"name":"stdout","text":"üíæ Models saved to /kaggle/working/models/\nDownload them from the Output tab!\n","output_type":"stream"}],"execution_count":26},{"id":"eb693577","cell_type":"markdown","source":"# üìä Results Summary\n\n## üéØ Performance Comparison\n- Compare all models' F1-scores, precision, and recall\n- BERT typically performs best but requires more resources\n\n## üí° Next Steps\n1. **Download Models**: Get your trained models from the Output tab\n2. **Deploy**: Use the saved models in production\n3. **Experiment**: Try different hyperparameters\n4. **Share**: Publish your notebook to Kaggle community\n\n## ‚ö° Kaggle Tips\n- Use GPU accelerator for faster training\n- Save models regularly to avoid losing progress\n- Monitor memory usage with large datasets\n- Use the Discussion forum for questions","metadata":{}}]}